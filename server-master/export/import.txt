From neale@sinenomine.net Wed Aug 20 00:45:17 2014
Received: from int-mx14.intmail.prod.int.phx2.redhat.com
	(int-mx14.intmail.prod.int.phx2.redhat.com [10.5.11.27])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7K4jHeR015884 for <linux-cluster@listman.util.phx.redhat.com>;
	Wed, 20 Aug 2014 00:45:17 -0400
Received: from mx1.redhat.com (ext-mx15.extmail.prod.ext.phx2.redhat.com
	[10.5.110.20])
	by int-mx14.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7K4jGcC021921
	for <linux-cluster@redhat.com>; Wed, 20 Aug 2014 00:45:17 -0400
Received: from smtp81.ord1c.emailsrvr.com (smtp81.ord1c.emailsrvr.com
	[108.166.43.81])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7K4jEZw019452
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=NO)
	for <linux-cluster@redhat.com>; Wed, 20 Aug 2014 00:45:15 -0400
Received: from smtp11.relay.ord1c.emailsrvr.com (localhost.localdomain
	[127.0.0.1])
	by smtp11.relay.ord1c.emailsrvr.com (SMTP Server) with ESMTP id
	A85261806F2
	for <linux-cluster@redhat.com>; Wed, 20 Aug 2014 00:45:13 -0400 (EDT)
X-SMTPDoctor-Processed: csmtpprox 2.7.1
Received: from localhost (localhost.localdomain [127.0.0.1])
	by smtp11.relay.ord1c.emailsrvr.com (SMTP Server) with ESMTP id
	A59A7180708
	for <linux-cluster@redhat.com>; Wed, 20 Aug 2014 00:45:13 -0400 (EDT)
X-Virus-Scanned: OK
Received: from smtp192.mex05.mlsrvr.com (unknown [184.106.31.85])
	by smtp11.relay.ord1c.emailsrvr.com (SMTP Server) with ESMTPS id
	9567D1806F2
	for <linux-cluster@redhat.com>; Wed, 20 Aug 2014 00:45:13 -0400 (EDT)
Received: from ORD2MBX02F.mex05.mlsrvr.com ([fe80::92e2:baff:fe11:e744]) by
	ORD2HUB34.mex05.mlsrvr.com ([::1]) with mapi id 14.03.0169.001;
	Tue, 19 Aug 2014 23:45:13 -0500
From: Neale Ferguson <neale@sinenomine.net>
To: linux clustering <linux-cluster@redhat.com>
Thread-Topic: clvmd not terminating
Thread-Index: AQHPvDGHbMQA89shcEK5X3Oh+KJ7xQ==
Date: Wed, 20 Aug 2014 04:45:12 +0000
Message-ID: <6A0A9B9D-E287-468C-B784-3F0A8DAB4E23@sinenomine.net>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
x-originating-ip: [96.247.193.74]
Content-Type: text/plain; charset="us-ascii"
Content-ID: <61CB4A60538E474990DF067418E63097@mex05.mlsrvr.com>
MIME-Version: 1.0
X-RedHat-Spam-Score: -2.311  (BAYES_00, DCC_REPUT_00_12, RCVD_IN_DNSWL_NONE,
	SPF_PASS)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.27
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.20
Content-Transfer-Encoding: 8bit
X-MIME-Autoconverted: from quoted-printable to 8bit by
	lists01.pubmisc.prod.ext.phx2.redhat.com id s7K4jHeR015884
X-loop: linux-cluster@redhat.com
Subject: [Linux-cluster] clvmd not terminating
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Wed, 20 Aug 2014 04:45:17 -0000

We have a sporadic situation where we are attempting to shutdown/restart both nodes of a two node cluster. One shutdowns completely but one sometimes hangs with:

[root@aude2mq036nabzi ~]# service cman stop
Stopping cluster:
   Leaving fence domain... found dlm lockspace /sys/kernel/dlm/clvmd
fence_tool: cannot leave due to active systems
[FAILED]

When the other node is brought back up it has problems with clvmd:

># pvscan
  connect() failed on local socket: Connection refused
  Internal cluster locking initialisation failed.
  WARNING: Falling back to local file-based locking.
  Volume Groups with the clustered attribute will be inaccessible.

Sometimes it works fine but very occasionally we get the above situation. I've encountered the fence message before, usually when the fence devices were incorrectly configured but it would always fail because of this. Before I get too far into investigation mode I wondered if the above symptoms ring any bells for anyone.

Neale

From prvs=53095580F9=ricks@alldigital.com Wed Aug 20 01:05:02 2014
Received: from int-mx10.intmail.prod.int.phx2.redhat.com
	(int-mx10.intmail.prod.int.phx2.redhat.com [10.5.11.23])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7K551TR019047 for <linux-cluster@listman.util.phx.redhat.com>;
	Wed, 20 Aug 2014 01:05:01 -0400
Received: from mx1.redhat.com (ext-mx13.extmail.prod.ext.phx2.redhat.com
	[10.5.110.18])
	by int-mx10.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7K551a9015332
	for <linux-cluster@redhat.com>; Wed, 20 Aug 2014 01:05:01 -0400
Received: from corp.alldigital.com (mex2-r1.alldigital.com [70.183.30.217])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7K54wNX017148
	(version=TLSv1/SSLv3 cipher=AES128-SHA bits=128 verify=FAIL)
	for <linux-cluster@redhat.com>; Wed, 20 Aug 2014 01:04:59 -0400
Received: from [192.168.0.205] (98.154.220.140) by mex2-r1.corp.alldigital.com
	(192.168.4.180) with Microsoft SMTP Server (TLS) id 14.1.438.0;
	Tue, 19 Aug 2014 22:04:56 -0700
Date: Tue, 19 Aug 2014 22:04:56 -0700
Message-ID: <r7ytrr41hwxiir22jyobpqmr.1408511096413@email.android.com>
Importance: normal
From: ricks <ricks@alldigital.com>
To: Neale Ferguson <neale@sinenomine.net>, linux clustering
	<linux-cluster@redhat.com>
MIME-Version: 1.0
Content-Type: multipart/alternative;
	boundary="--_com.android.email_2654625065860350"
X-Originating-IP: [98.154.220.140]
X-RedHat-Spam-Score: -2.569  (BAYES_00, HTML_MESSAGE, RP_MATCHES_RCVD,
	SPF_HELO_PASS, SPF_PASS)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.23
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.18
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] clvmd not terminating
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Wed, 20 Aug 2014 05:05:02 -0000

----_com.android.email_2654625065860350
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
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=

----_com.android.email_2654625065860350
Content-Type: text/html; charset="utf-8"
Content-Transfer-Encoding: base64
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----_com.android.email_2654625065860350--

From prvs=6309501FB3=ricks@alldigital.com Wed Aug 20 01:07:19 2014
Received: from int-mx11.intmail.prod.int.phx2.redhat.com
	(int-mx11.intmail.prod.int.phx2.redhat.com [10.5.11.24])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7K57JLY003953 for <linux-cluster@listman.util.phx.redhat.com>;
	Wed, 20 Aug 2014 01:07:19 -0400
Received: from mx1.redhat.com (ext-mx14.extmail.prod.ext.phx2.redhat.com
	[10.5.110.19])
	by int-mx11.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7K57JnP004107
	for <linux-cluster@redhat.com>; Wed, 20 Aug 2014 01:07:19 -0400
Received: from corp.alldigital.com (mex2-r1.alldigital.com [70.183.30.217])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7K57H15014817
	(version=TLSv1/SSLv3 cipher=AES128-SHA bits=128 verify=FAIL)
	for <linux-cluster@redhat.com>; Wed, 20 Aug 2014 01:07:17 -0400
Received: from [192.168.0.205] (98.154.220.140) by mex2-r1.corp.alldigital.com
	(192.168.4.180) with Microsoft SMTP Server (TLS) id 14.1.438.0;
	Tue, 19 Aug 2014 22:07:16 -0700
Date: Tue, 19 Aug 2014 22:07:16 -0700
Message-ID: <guopjf57t8r0ygaycyb82dbr.1408511236324@email.android.com>
Importance: normal
From: ricks <ricks@alldigital.com>
To: Neale Ferguson <neale@sinenomine.net>, linux clustering
	<linux-cluster@redhat.com>
MIME-Version: 1.0
Content-Type: multipart/alternative;
	boundary="--_com.android.email_2656025805880140"
X-Originating-IP: [98.154.220.140]
X-RedHat-Spam-Score: -2.569  (BAYES_00, HTML_MESSAGE, RP_MATCHES_RCVD,
	SPF_HELO_PASS, SPF_PASS)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.24
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.19
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] clvmd not terminating
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Wed, 20 Aug 2014 05:07:19 -0000

----_com.android.email_2656025805880140
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
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----_com.android.email_2656025805880140
Content-Type: text/html; charset="utf-8"
Content-Transfer-Encoding: base64
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----_com.android.email_2656025805880140--

From wferi@niif.hu Thu Aug 21 20:37:55 2014
Received: from int-mx13.intmail.prod.int.phx2.redhat.com
	(int-mx13.intmail.prod.int.phx2.redhat.com [10.5.11.26])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7M0btnx024641 for <linux-cluster@listman.util.phx.redhat.com>;
	Thu, 21 Aug 2014 20:37:55 -0400
Received: from mx1.redhat.com (ext-mx15.extmail.prod.ext.phx2.redhat.com
	[10.5.110.20])
	by int-mx13.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7M0bt4m022567
	for <linux-cluster@redhat.com>; Thu, 21 Aug 2014 20:37:55 -0400
Received: from listserv2.niif.hu (listserv2.niif.hu [193.225.14.155])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7M0bpsO007144
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES128-SHA bits=128 verify=NO)
	for <linux-cluster@redhat.com>; Thu, 21 Aug 2014 20:37:52 -0400
Received: from business-188-142-225-206.business.broadband.hu
	([188.142.225.206] helo=lant.ki.iif.hu)
	by listserv2.niif.hu with esmtpsa (TLS1.2:DHE_RSA_AES_128_CBC_SHA1:128)
	(Exim 4.80) (envelope-from <wferi@niif.hu>) id 1XKcrV-0006n6-SR
	for linux-cluster@redhat.com; Fri, 22 Aug 2014 02:37:49 +0200
Received: from wferi by lant.ki.iif.hu with local (Exim 4.80)
	(envelope-from <wferi@lant.ki.iif.hu>) id 1XKcrQ-0004CM-6t
	for linux-cluster@redhat.com; Fri, 22 Aug 2014 02:37:44 +0200
From: Ferenc Wagner <wferi@niif.hu>
To: linux clustering <linux-cluster@redhat.com>
Date: Fri, 22 Aug 2014 02:37:44 +0200
Message-ID: <8761hlickn.fsf@lant.ki.iif.hu>
User-Agent: Gnus/5.13 (Gnus v5.13) Emacs/23.4 (gnu/linux)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
X-RedHat-Spam-Score: -4.868  (BAYES_00,RCVD_IN_DNSWL_MED,RP_MATCHES_RCVD)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.26
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.20
X-loop: linux-cluster@redhat.com
Subject: [Linux-cluster] on exiting maintenance mode
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Fri, 22 Aug 2014 00:37:55 -0000

Hi,

While my Pacemaker cluster was in maintenance mode, resources were moved
(by hand) between the nodes as I rebooted each node in turn.  In the end
the crm status output became perfectly empty, as the reboot of a given
node removed from the output the resources which were located on the
rebooted node at the time of entering maintenance mode.  I expected full
resource discovery on exiting maintenance mode, but it probably did not
happen, as the cluster started up resources already running on other
nodes, which is generally forbidden.  Given that all resources were
running (though possibly migrated during the maintenance), what would
have been the correct way of bringing the cluster out of maintenance
mode?  This should have required no resource actions at all.  Would
cleanup of all resources have helped?  Or is there a better way?
-- 
Thanks,
Feri.

From vasil.val@gmail.com Tue Aug 26 02:56:32 2014
Received: from int-mx13.intmail.prod.int.phx2.redhat.com
	(int-mx13.intmail.prod.int.phx2.redhat.com [10.5.11.26])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7Q6uWeV015148 for <linux-cluster@listman.util.phx.redhat.com>;
	Tue, 26 Aug 2014 02:56:32 -0400
Received: from mx1.redhat.com (ext-mx14.extmail.prod.ext.phx2.redhat.com
	[10.5.110.19])
	by int-mx13.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7Q6uWfS024998
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 02:56:32 -0400
Received: from mail-la0-f54.google.com (mail-la0-f54.google.com
	[209.85.215.54])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7Q6uTXg020608
	(version=TLSv1/SSLv3 cipher=RC4-SHA bits=128 verify=FAIL)
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 02:56:31 -0400
Received: by mail-la0-f54.google.com with SMTP id hz20so14556792lab.13
	for <linux-cluster@redhat.com>; Mon, 25 Aug 2014 23:56:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=gmail.com; s=20120113;
	h=mime-version:date:message-id:subject:from:to:content-type;
	bh=l41Tr1EZts/OOXBhMx1A8w5H77OBDfyjrP8mUv+RV10=;
	b=jKoJoWe9+hjJFclhLuZD4CkRh9wajv3AOjeKjFbmAS9Je7ZoYPepZy8/bR986/Gnl9
	0JrKeiYZCy4x3lBMwhQcOXOuD3Vkubk/1SQoVsLUSSrC0g4cHhxgI2I2KwTVQ/E7w0o3
	x62LOSLoOxSSWTV1Rv/QlgRBCL5Fvtkj+w3yEBjd06rGlMCJcdHSbe/dBzV4qWnmlV1A
	biZcjuiwI1uPfOcX2vAFOwsbQFfcaz/pQUjqBrN0n+pfLURlhirjyDG2LIOK5JOr6ZLY
	zyxM1uwqWGCfP37kSJm/rLkPolXTZGl6eVCtrT+s5Lnil22P8cYnwveZeYC3pduigf7L
	lf+Q==
MIME-Version: 1.0
X-Received: by 10.152.9.170 with SMTP id a10mr12493889lab.79.1409036189485;
	Mon, 25 Aug 2014 23:56:29 -0700 (PDT)
Received: by 10.25.162.199 with HTTP; Mon, 25 Aug 2014 23:56:29 -0700 (PDT)
Date: Tue, 26 Aug 2014 09:56:29 +0300
Message-ID: <CAFZxf=L12UCn6nEnd_LtRWL6_P=wOALfArR6DxhL9RU5iA2Tnw@mail.gmail.com>
From: Vasil Valchev <vasil.val@gmail.com>
To: linux-cluster@redhat.com
Content-Type: multipart/alternative; boundary=001a1132f49a8cd2d9050182cde2
X-RedHat-Spam-Score: -3.099  (BAYES_00, DCC_REPUT_00_12, DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU, FREEMAIL_FROM, HTML_MESSAGE,
	RCVD_IN_DNSWL_LOW, SPF_PASS)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.26
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.19
X-loop: linux-cluster@redhat.com
Subject: [Linux-cluster] totem token & post_fail_delay question
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Tue, 26 Aug 2014 06:56:32 -0000

--001a1132f49a8cd2d9050182cde2
Content-Type: text/plain; charset=UTF-8

Hello,

I have a cluster that sometimes has intermittent network issues on the
heartbeat network.
Unfortunately improving the network is not an option, so I am looking for a
way to tolerate longer interruptions.

Previously it seemed to me the post_fail_delay option is suitable, but
after some research it might not be what I am looking for.

If I am correct, when a member leaves (due to token timeout) the cluster
will wait the post_fail_delay before fencing. If the member rejoins before
that, it will still be fenced, because it has previous state?
>From a recent fencing on this cluster there is a strange message:

Aug 24 06:20:45 node2 openais[29048]: [MAIN ] Not killing node node1cl
despite it rejoining the cluster with existing state, it has a lower node ID

What does this mean?

And lastly is increasing the totem token timeout the way to go?


Thanks,
Vasil Valchev

--001a1132f49a8cd2d9050182cde2
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hello,<div><br></div><div>I have a cluster that sometimes =
has intermittent network issues on the heartbeat network.</div><div>Unfortu=
nately improving the network is not an option, so I am looking for a way to=
 tolerate longer interruptions.</div>
<div><br></div><div>Previously it seemed to me the post_fail_delay option i=
s suitable, but after some research it might not be what I am looking for.<=
br></div><div><br></div><div>If I am correct, when a member leaves (due to =
token timeout) the cluster will wait the post_fail_delay before fencing. If=
 the member rejoins before that, it will still be fenced, because it has pr=
evious state?</div>
<div>From a recent fencing on this cluster there is a strange message:</div=
><div><br></div><div>Aug 24 06:20:45 node2 openais[29048]: [MAIN ] Not kill=
ing node node1cl despite it rejoining the cluster with existing state, it h=
as a lower node ID<br>
</div><div><br></div><div>What does this mean?</div><div><br></div><div>And=
 lastly is increasing the totem token timeout the way to go?<br></div><div>=
<br></div><div><br></div><div>Thanks,</div><div>Vasil Valchev</div></div>

--001a1132f49a8cd2d9050182cde2--

From andrew@beekhof.net Tue Aug 26 03:40:55 2014
Received: from int-mx09.intmail.prod.int.phx2.redhat.com
	(int-mx09.intmail.prod.int.phx2.redhat.com [10.5.11.22])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7Q7et0l028870 for <linux-cluster@listman.util.phx.redhat.com>;
	Tue, 26 Aug 2014 03:40:55 -0400
Received: from mx1.redhat.com (ext-mx14.extmail.prod.ext.phx2.redhat.com
	[10.5.110.19])
	by int-mx09.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7Q7etCu010141
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 03:40:55 -0400
Received: from out2-smtp.messagingengine.com (out2-smtp.messagingengine.com
	[66.111.4.26])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7Q7esSL002132
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256
	verify=NO)
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 03:40:54 -0400
Received: from compute3.internal (compute3.nyi.internal [10.202.2.43])
	by gateway2.nyi.internal (Postfix) with ESMTP id 9EABE20905
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 03:40:53 -0400 (EDT)
Received: from frontend1 ([10.202.2.160])
	by compute3.internal (MEProxy); Tue, 26 Aug 2014 03:40:53 -0400
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=beekhof.net; h=
	content-type:subject:mime-version:from:in-reply-to:date
	:message-id:references:to; s=mesmtp; bh=lpGMbSjwbDJoOHwzTvPWp19X
	ajc=; b=QBtvL1PfPKKAd9FLNRpJvOYuvY09tzwPUSGFfngP3JK24TCbHWMLATxn
	TghuwVIr2brtFbW5YEtmON0iJKaLlQo5yCWrjUPia1rfYlnkvQKzVFW62OoUVBHj
	Gm9KelvQw6bRSEusNUmpStj+6nW8CLWrcXtosDtXSoF+AJDZERA=
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=
	messagingengine.com; h=content-type:subject:mime-version:from
	:in-reply-to:date:message-id:references:to; s=smtpout; bh=lpGMbS
	jwbDJoOHwzTvPWp19Xajc=; b=brfQei8XnfgyFj8Uo3FBDexp9PU3+8lR7Irzpj
	+xdex9GrEDDmKMkUSha4cIBhwX9vmgVBTJAxcjcfg3y96zCmPXbzhuQlirV4tqr9
	MWO6pr2a6aG3G9ojyrHcl9ZULO6am0rXsQfFUU5nYnQu36fyznH2uvo2CnmGqPHb
	wyku0=
X-Sasl-enc: ezexVGhEMn13dmTLP9aGcGN3i8f8ZETzpWftYyUVNP7U 1409038853
Received: from [172.16.1.5] (unknown [120.147.36.73])
	by mail.messagingengine.com (Postfix) with ESMTPA id AF7FEC008FC
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 03:40:52 -0400 (EDT)
Content-Type: multipart/signed;
	boundary="Apple-Mail=_2F0ED310-AFCD-4A8C-8864-BDE9C09FD9B4";
	protocol="application/pgp-signature"; micalg=pgp-sha512
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
From: Andrew Beekhof <andrew@beekhof.net>
In-Reply-To: <8761hlickn.fsf@lant.ki.iif.hu>
Date: Tue, 26 Aug 2014 17:40:50 +1000
X-Mao-Original-Outgoing-Id: 430731649.193825-613619ee84c2461a7b15b12a65503e51
Message-Id: <67506B71-8594-4C16-82C1-F94779F59826@beekhof.net>
References: <8761hlickn.fsf@lant.ki.iif.hu>
To: linux clustering <linux-cluster@redhat.com>
X-RedHat-Spam-Score: -1.201  (BAYES_20, DCC_REPUT_00_12, DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU, RCVD_IN_DNSWL_LOW)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.22
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.19
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] on exiting maintenance mode
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Tue, 26 Aug 2014 07:40:55 -0000


--Apple-Mail=_2F0ED310-AFCD-4A8C-8864-BDE9C09FD9B4
Content-Transfer-Encoding: 7bit
Content-Type: text/plain;
	charset=us-ascii


On 22 Aug 2014, at 10:37 am, Ferenc Wagner <wferi@niif.hu> wrote:

> Hi,
> 
> While my Pacemaker cluster was in maintenance mode, resources were moved
> (by hand) between the nodes as I rebooted each node in turn.  In the end
> the crm status output became perfectly empty, as the reboot of a given
> node removed from the output the resources which were located on the
> rebooted node at the time of entering maintenance mode.  I expected full
> resource discovery on exiting maintenance mode,

Version and logs?

The discovery usually happens at the point the cluster is started on a node.
Maintenance mode just prevents the cluster from doing anything about it.

> but it probably did not
> happen, as the cluster started up resources already running on other
> nodes, which is generally forbidden.  Given that all resources were
> running (though possibly migrated during the maintenance), what would
> have been the correct way of bringing the cluster out of maintenance
> mode?  This should have required no resource actions at all.  Would
> cleanup of all resources have helped?  Or is there a better way?
> -- 
> Thanks,
> Feri.
> 
> -- 
> Linux-cluster mailing list
> Linux-cluster@redhat.com
> https://www.redhat.com/mailman/listinfo/linux-cluster


--Apple-Mail=_2F0ED310-AFCD-4A8C-8864-BDE9C09FD9B4
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment;
	filename=signature.asc
Content-Type: application/pgp-signature;
	name=signature.asc
Content-Description: Message signed with OpenPGP using GPGMail

-----BEGIN PGP SIGNATURE-----
Comment: GPGTools - http://gpgtools.org

iQIcBAEBCgAGBQJT/DoBAAoJEBTzwpg4iwmNP6UP/RD/otuXqJXlcQBTph2M3ija
1iCvUUe4LV7VrpUUz7g7gZFFWadHHBqWi+F3rE4eeARA0qtasBvnEM+35AKmMB8R
PELKXy3jIyvBhZcsojUbiheGKJqsqmeWbiGtUkwFJ9KQMfTgdzHAVuiVDddDKS00
lwSBIRzLvzJbszlLE3IxduciYFuM2jH9rUv5sCsnXWTE2xZaJBkyjUGob3rkK13J
UzJOKUT2cKORNMRrctqOC15f0IyDB1Wdg0XmRv3jqH93l1XzQe3vDAl9LJZkCwd0
/qdSfdKbOWPf/Eb6EB4j9VX/qtbQoxqOlXgSpzTxh0bNUAwci1DeWPb+LcaH3pmE
dtp+nmm808jQAEJ8vpm3xfeyRSPofjThMNkpoKYtkRvfiP5c8G4T3sfEU22BDkV8
bJJ0bgyVhwnfea8LqYBy/b1zLmTzHlx2JXNptHc1G9iCdcDI19xt7rG86Zaxs/GR
OgbWHg4cI/DdUYxcXBem0T+Kxi9J91ChF4rmXwWlUXajP6FOMMx1XNWeMNVqb58I
F/jJa5uAnlwt9SJnk65wwSux1Ly0X68KXrcKJIrxQKMl/jKgYm5Ef4p+Bdh3deZL
D2oiMdj78Qh75u2Wu5JmHT1j2QzrFrQc3HaLgslb3ZofViPoj190gDpnFo0pK2Rn
fjxbxxuKfTDJYBVm4s2M
=IrHk
-----END PGP SIGNATURE-----

--Apple-Mail=_2F0ED310-AFCD-4A8C-8864-BDE9C09FD9B4--

From emi2fast@gmail.com Tue Aug 26 04:18:04 2014
Received: from int-mx14.intmail.prod.int.phx2.redhat.com
	(int-mx14.intmail.prod.int.phx2.redhat.com [10.5.11.27])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7Q8I4kE016989 for <linux-cluster@listman.util.phx.redhat.com>;
	Tue, 26 Aug 2014 04:18:04 -0400
Received: from mx1.redhat.com (ext-mx13.extmail.prod.ext.phx2.redhat.com
	[10.5.110.18])
	by int-mx14.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7Q8I36n015692
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 04:18:03 -0400
Received: from mail-ob0-f177.google.com (mail-ob0-f177.google.com
	[209.85.214.177])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7Q8I1Po006206
	(version=TLSv1/SSLv3 cipher=RC4-SHA bits=128 verify=FAIL)
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 04:18:02 -0400
Received: by mail-ob0-f177.google.com with SMTP id wp18so11140761obc.22
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 01:18:01 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=gmail.com; s=20120113;
	h=mime-version:in-reply-to:references:date:message-id:subject:from:to
	:content-type; bh=Dsw7Kibdmsb5py0hlWCOL0lMjSuTamrY0T0rDJtHuhM=;
	b=0Tx1yyfiO0r3FdJJFW1U5LtoezZdRDXaGdEuD2T18nPRbx7iDATULubq1pFfxWWhoX
	vI0INiBCVXG6eVj48/z7PkqsnYktVC3+MDE5+b7AJmoIEOiRdQF59O3dhstGSAdXPrey
	EmKkhauYmvnq7DwNaqtWBlevBEGaZ8gLeNMgtiAQUyGrCzXb+MbQYDlo/Oji6dWfdosC
	cMsB/TvK2htYipY0knkFEBsdpWEMMOLSts0gzb6rZi5gtt4fYhaf575A2QlY/J6XLmcV
	1NP4/KWn8Ein6VbNi6d9f1qhb1Z/3qhW5ko0xPQRymt/2RNqKIcqxlp/T4zVE7Zxr2to
	MS/Q==
MIME-Version: 1.0
X-Received: by 10.182.191.39 with SMTP id gv7mr5719429obc.14.1409040699953;
	Tue, 26 Aug 2014 01:11:39 -0700 (PDT)
Received: by 10.76.156.71 with HTTP; Tue, 26 Aug 2014 01:11:39 -0700 (PDT)
In-Reply-To: <CAFZxf=L12UCn6nEnd_LtRWL6_P=wOALfArR6DxhL9RU5iA2Tnw@mail.gmail.com>
References: <CAFZxf=L12UCn6nEnd_LtRWL6_P=wOALfArR6DxhL9RU5iA2Tnw@mail.gmail.com>
Date: Tue, 26 Aug 2014 10:11:39 +0200
Message-ID: <CAE7pJ3BsQtgeO9pHquqtA_STd_HCTh=RXTrNEMFNQzRSM2A0Tg@mail.gmail.com>
From: emmanuel segura <emi2fast@gmail.com>
To: linux clustering <linux-cluster@redhat.com>
Content-Type: text/plain; charset=UTF-8
X-RedHat-Spam-Score: -0.801  (BAYES_20, DKIM_SIGNED, DKIM_VALID, DKIM_VALID_AU,
	FREEMAIL_FROM, RCVD_IN_DNSWL_LOW, SPF_PASS)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.27
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.18
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] totem token & post_fail_delay question
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Tue, 26 Aug 2014 08:18:04 -0000

from man fenced

Post-fail delay is the number of seconds the daemon will wait before
fencing any victims after a domain member fails.

It's used for delay the fence action.

2014-08-26 8:56 GMT+02:00 Vasil Valchev <vasil.val@gmail.com>:
> Hello,
>
> I have a cluster that sometimes has intermittent network issues on the
> heartbeat network.
> Unfortunately improving the network is not an option, so I am looking for a
> way to tolerate longer interruptions.
>
> Previously it seemed to me the post_fail_delay option is suitable, but after
> some research it might not be what I am looking for.
>
> If I am correct, when a member leaves (due to token timeout) the cluster
> will wait the post_fail_delay before fencing. If the member rejoins before
> that, it will still be fenced, because it has previous state?
> From a recent fencing on this cluster there is a strange message:
>
> Aug 24 06:20:45 node2 openais[29048]: [MAIN ] Not killing node node1cl
> despite it rejoining the cluster with existing state, it has a lower node ID
>
> What does this mean?
>
> And lastly is increasing the totem token timeout the way to go?
>
>
> Thanks,
> Vasil Valchev
>
> --
> Linux-cluster mailing list
> Linux-cluster@redhat.com
> https://www.redhat.com/mailman/listinfo/linux-cluster



-- 
esta es mi vida e me la vivo hasta que dios quiera

From ccaulfie@redhat.com Tue Aug 26 04:23:16 2014
Received: from int-mx09.intmail.prod.int.phx2.redhat.com
	(int-mx09.intmail.prod.int.phx2.redhat.com [10.5.11.22])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7Q8NGVS018071 for <linux-cluster@listman.util.phx.redhat.com>;
	Tue, 26 Aug 2014 04:23:16 -0400
Received: from rhdesktop.chrissie.net (vpn1-5-107.ams2.redhat.com
	[10.36.5.107])
	by int-mx09.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7Q8NEoH030103
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES128-SHA bits=128 verify=NO)
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 04:23:16 -0400
Message-ID: <53FC43F2.2010003@redhat.com>
Date: Tue, 26 Aug 2014 09:23:14 +0100
From: Christine Caulfield <ccaulfie@redhat.com>
User-Agent: Mozilla/5.0 (X11; Linux x86_64;
	rv:24.0) Gecko/20100101 Thunderbird/24.7.0
MIME-Version: 1.0
To: linux-cluster@redhat.com
References: <CAFZxf=L12UCn6nEnd_LtRWL6_P=wOALfArR6DxhL9RU5iA2Tnw@mail.gmail.com>
In-Reply-To: <CAFZxf=L12UCn6nEnd_LtRWL6_P=wOALfArR6DxhL9RU5iA2Tnw@mail.gmail.com>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed
Content-Transfer-Encoding: 7bit
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.22
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] totem token & post_fail_delay question
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Tue, 26 Aug 2014 08:23:16 -0000

On 26/08/14 07:56, Vasil Valchev wrote:
> Hello,
>
> I have a cluster that sometimes has intermittent network issues on the
> heartbeat network.
> Unfortunately improving the network is not an option, so I am looking
> for a way to tolerate longer interruptions.
>
> Previously it seemed to me the post_fail_delay option is suitable, but
> after some research it might not be what I am looking for.
>
> If I am correct, when a member leaves (due to token timeout) the cluster
> will wait the post_fail_delay before fencing. If the member rejoins
> before that, it will still be fenced, because it has previous state?
>  From a recent fencing on this cluster there is a strange message:
>
> Aug 24 06:20:45 node2 openais[29048]: [MAIN ] Not killing node node1cl
> despite it rejoining the cluster with existing state, it has a lower node ID
>
> What does this mean?
>

It's an attempt by cman to sort out which node to kill in the situation 
where a node rejoins too quickly. If both nodes try to send a 'kill' 
message then then both nodes would leave the cluster leaving you with no 
active nodes. So cman (and fencing) prioritise the node with the lowest 
nodeID in an attempt at a tie-break. you should see a corresponding 
message on the other node:
"Killing node %s because it has rejoined the cluster with existing state 
and has higher node ID"


> And lastly is increasing the totem token timeout the way to go?
>

if there is no option for improving the network situation then, yes, 
increasing token timeout is probably your best option.

Chrissie

From emi2fast@gmail.com Tue Aug 26 06:08:51 2014
Received: from int-mx14.intmail.prod.int.phx2.redhat.com
	(int-mx14.intmail.prod.int.phx2.redhat.com [10.5.11.27])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7QA8pdR010966 for <linux-cluster@listman.util.phx.redhat.com>;
	Tue, 26 Aug 2014 06:08:51 -0400
Received: from mx1.redhat.com (ext-mx13.extmail.prod.ext.phx2.redhat.com
	[10.5.110.18])
	by int-mx14.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7QA8pgj008148
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 06:08:51 -0400
Received: from mail-oa0-f42.google.com (mail-oa0-f42.google.com
	[209.85.219.42])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7QA8mfV011558
	(version=TLSv1/SSLv3 cipher=RC4-SHA bits=128 verify=FAIL)
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 06:08:49 -0400
Received: by mail-oa0-f42.google.com with SMTP id n16so11838661oag.1
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 03:08:48 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=gmail.com; s=20120113;
	h=mime-version:in-reply-to:references:date:message-id:subject:from:to
	:content-type; bh=5PyVKw5n4J+ACT7HhUVclBOeNPGqV3NCneINBh14tj8=;
	b=kVuhDofYENzbZwd+5iMAmmaiHJxp3FBLG6GHkuV92XQW4Mhj3/wn9Fgblyhl8BqaXu
	KLcTCYBIid94pYkNfTMYaMDn7KMTDcHB7tsuwiBVzWzFl2GuUgT8OLwM0TQnjjtM8tRy
	NjM/LQ4/2uxziB6aESAcBMaaApsuv/13n9PvduBu/rb1nEXPRw4c4QE+qr0NRr5ovCbq
	Yns4hfkL19NTMOXBNTd3t+jg/hJrph50cQxV48XQBHDdNvR4eb9LZmOLtrFXtGrou377
	NWnZXNzuuU+6M1ngG9W5gnDaEpWTbAhL399flBEeooweKzZ/ewO8hGon6trLEZdBF/Bp
	mnYg==
MIME-Version: 1.0
X-Received: by 10.182.191.39 with SMTP id gv7mr6196997obc.14.1409047728198;
	Tue, 26 Aug 2014 03:08:48 -0700 (PDT)
Received: by 10.76.156.71 with HTTP; Tue, 26 Aug 2014 03:08:48 -0700 (PDT)
In-Reply-To: <53FC43F2.2010003@redhat.com>
References: <CAFZxf=L12UCn6nEnd_LtRWL6_P=wOALfArR6DxhL9RU5iA2Tnw@mail.gmail.com>
	<53FC43F2.2010003@redhat.com>
Date: Tue, 26 Aug 2014 12:08:48 +0200
Message-ID: <CAE7pJ3Ae29bUz4CRo41Lv+8CSUD9cQC35zr2mP0A__Bvu40eZQ@mail.gmail.com>
From: emmanuel segura <emi2fast@gmail.com>
To: linux clustering <linux-cluster@redhat.com>
Content-Type: text/plain; charset=UTF-8
X-RedHat-Spam-Score: -2.7  (BAYES_00, DKIM_SIGNED, DKIM_VALID, DKIM_VALID_AU,
	FREEMAIL_FROM, RCVD_IN_DNSWL_LOW, SPF_PASS)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.27
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.18
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] totem token & post_fail_delay question
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Tue, 26 Aug 2014 10:08:51 -0000

i think, you are talking about:

 Post-join delay is the number of seconds the daemon will wait before
fencing any victims after a node joins the domain.



2014-08-26 10:23 GMT+02:00 Christine Caulfield <ccaulfie@redhat.com>:
> On 26/08/14 07:56, Vasil Valchev wrote:
>>
>> Hello,
>>
>> I have a cluster that sometimes has intermittent network issues on the
>> heartbeat network.
>> Unfortunately improving the network is not an option, so I am looking
>> for a way to tolerate longer interruptions.
>>
>> Previously it seemed to me the post_fail_delay option is suitable, but
>> after some research it might not be what I am looking for.
>>
>> If I am correct, when a member leaves (due to token timeout) the cluster
>> will wait the post_fail_delay before fencing. If the member rejoins
>> before that, it will still be fenced, because it has previous state?
>>  From a recent fencing on this cluster there is a strange message:
>>
>> Aug 24 06:20:45 node2 openais[29048]: [MAIN ] Not killing node node1cl
>> despite it rejoining the cluster with existing state, it has a lower node
>> ID
>>
>> What does this mean?
>>
>
> It's an attempt by cman to sort out which node to kill in the situation
> where a node rejoins too quickly. If both nodes try to send a 'kill' message
> then then both nodes would leave the cluster leaving you with no active
> nodes. So cman (and fencing) prioritise the node with the lowest nodeID in
> an attempt at a tie-break. you should see a corresponding message on the
> other node:
> "Killing node %s because it has rejoined the cluster with existing state and
> has higher node ID"
>
>
>
>> And lastly is increasing the totem token timeout the way to go?
>>
>
> if there is no option for improving the network situation then, yes,
> increasing token timeout is probably your best option.
>
> Chrissie
>
>
> --
> Linux-cluster mailing list
> Linux-cluster@redhat.com
> https://www.redhat.com/mailman/listinfo/linux-cluster



-- 
esta es mi vida e me la vivo hasta que dios quiera

From wferi@niif.hu Tue Aug 26 13:40:31 2014
Received: from int-mx10.intmail.prod.int.phx2.redhat.com
	(int-mx10.intmail.prod.int.phx2.redhat.com [10.5.11.23])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7QHeVrm005010 for <linux-cluster@listman.util.phx.redhat.com>;
	Tue, 26 Aug 2014 13:40:31 -0400
Received: from mx1.redhat.com (ext-mx16.extmail.prod.ext.phx2.redhat.com
	[10.5.110.21])
	by int-mx10.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7QHeVAx005837
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 13:40:31 -0400
Received: from listserv2.niif.hu (listserv2.niif.hu [193.225.14.155])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7QHePPt000593
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES128-SHA bits=128 verify=NO)
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 13:40:27 -0400
Received: from business-188-142-225-206.business.broadband.hu
	([188.142.225.206] helo=lant.ki.iif.hu)
	by listserv2.niif.hu with esmtpsa (TLS1.2:DHE_RSA_AES_128_CBC_SHA1:128)
	(Exim 4.80) (envelope-from <wferi@niif.hu>) id 1XMKjI-0001Ob-UQ
	for linux-cluster@redhat.com; Tue, 26 Aug 2014 19:40:24 +0200
Received: from wferi by lant.ki.iif.hu with local (Exim 4.80)
	(envelope-from <wferi@lant.ki.iif.hu>) id 1XMKjD-0002SY-Bs
	for linux-cluster@redhat.com; Tue, 26 Aug 2014 19:40:19 +0200
From: Ferenc Wagner <wferi@niif.hu>
To: linux clustering <linux-cluster@redhat.com>
References: <8761hlickn.fsf@lant.ki.iif.hu>
	<67506B71-8594-4C16-82C1-F94779F59826@beekhof.net>
Date: Tue, 26 Aug 2014 19:40:19 +0200
In-Reply-To: <67506B71-8594-4C16-82C1-F94779F59826@beekhof.net> (Andrew
	Beekhof's message of "Tue, 26 Aug 2014 17:40:50 +1000")
Message-ID: <87iolf9mkc.fsf@lant.ki.iif.hu>
User-Agent: Gnus/5.13 (Gnus v5.13) Emacs/23.4 (gnu/linux)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
X-RedHat-Spam-Score: -4.199  (BAYES_00,RCVD_IN_DNSWL_MED,RP_MATCHES_RCVD)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.23
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.21
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] on exiting maintenance mode
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Tue, 26 Aug 2014 17:40:31 -0000

Andrew Beekhof <andrew@beekhof.net> writes:

> On 22 Aug 2014, at 10:37 am, Ferenc Wagner <wferi@niif.hu> wrote:
>
>> While my Pacemaker cluster was in maintenance mode, resources were moved
>> (by hand) between the nodes as I rebooted each node in turn.  In the end
>> the crm status output became perfectly empty, as the reboot of a given
>> node removed from the output the resources which were located on the
>> rebooted node at the time of entering maintenance mode.  I expected full
>> resource discovery on exiting maintenance mode,
>
> Version and logs?

(The more interesting part comes later, please skip to the theoretical
part if you're short on time. :)

I left those out, as I don't expect the actual behavior to be a bug.
But I experienced this with Pacemaker version 1.1.7.  I know it's old
and it suffers from crmd segfault on entering maintenance mode (cf.
http://thread.gmane.org/gmane.linux.highavailability.user/39121), but
works well generally so I did not get to upgrade it yet.  Now that I
mentioned the crmd segfault: I noted that it died on the DC when I
entered maintenance mode:

crmd: [7452]: info: te_rsc_command: Initiating action 64: cancel vm-tmvp_monitor_60000 on n01 (local)
crmd: [7452]: ERROR: lrm_get_rsc(666): failed to send a getrsc message to lrmd via ch_cmd channel.
crmd: [7452]: ERROR: get_lrm_resource: Could not add resource vm-tmvp to LRM
crmd: [7452]: ERROR: do_lrm_invoke: Invalid resource definition
crmd: [7452]: WARN: do_lrm_invoke: bad input <create_request_adv origin="te_rsc_command" t="crmd" version="3.0.6" subt="request" reference="lrm_invoke-tengine-1408517719-30820" crm_task="lrm_invoke" crm_sys_to="lrmd" crm_sys_from="tengine" crm_host_to="n01" >
crmd: [7452]: WARN: do_lrm_invoke: bad input   <crm_xml >
crmd: [7452]: WARN: do_lrm_invoke: bad input     <rsc_op id="64" operation="cancel" operation_key="vm-tmvp_monitor_60000" on_node="n01" on_node_uuid="n01" transition-key="64:20579:0:1b0a6e79-af5a-41e4-8ced-299371e7922c" >
crmd: [7452]: WARN: do_lrm_invoke: bad input       <primitive id="vm-tmvp" long-id="vm-tmvp" class="ocf" provider="niif" type="TransientDomain" />
crmd: [7452]: info: te_rsc_command: Initiating action 86: cancel vm-wfweb_monitor_60000 on n01 (local)
crmd: [7452]: ERROR: lrm_add_rsc(870): failed to send a addrsc message to lrmd via ch_cmd channel.
crmd: [7452]: ERROR: lrm_get_rsc(666): failed to send a getrsc message to lrmd via ch_cmd channel.
corosync[6966]:   [pcmk  ] info: pcmk_ipc_exit: Client crmd (conn=0x1dc6ea0, async-conn=0x1dc6ea0) left
pacemakerd: [7443]: WARN: Managed crmd process 7452 killed by signal 11 [SIGSEGV - Segmentation violation].
pacemakerd: [7443]: notice: pcmk_child_exit: Child process crmd terminated with signal 11 (pid=7452, rc=0)

However, it got restarted seamlessly, without the node being fenced, so
I did not even notice this until now.  Should this have resulted in the
node being fenced?

But back to the issue at hand.  The Pacemaker shutdown seemed normal,
apart from the bunch of messages like:

crmd: [13794]: ERROR: verify_stopped: Resource vm-web5 was active at shutdown.  You may ignore this error if it is unmanaged.

appearing twice and warnings like:

cib: [7447]: WARN: send_ipc_message: IPC Channel to 13794 is not connected
cib: [7447]: WARN: send_via_callback_channel: Delivery of reply to client 13794/bf6f43a2-70db-40ac-a902-eabc3c12e20d failed
cib: [7447]: WARN: do_local_notify: A-Sync reply to crmd failed: reply failed
corosync[6966]:   [pcmk  ] WARN: route_ais_message: Sending message to local.crmd failed: ipc delivery failed (rc=-2)

On reboot, corosync complained until the some Pacemaker components
started:

corosync[8461]:   [pcmk  ] WARN: route_ais_message: Sending message to local.cib failed: ipc delivery failed (rc=-2)
corosync[8461]:   [pcmk  ] WARN: route_ais_message: Sending message to local.crmd failed: ipc delivery failed (rc=-2)

Pacemaker then probed the resources on the local node (all was inactive):

lrmd: [8946]: info: rsc:stonith-n01 probe[5] (pid 9081)
lrmd: [8946]: info: rsc:dlm:0 probe[6] (pid 9082)
[...]
lrmd: [8946]: info: operation monitor[112] on vm-fir for client 8949: pid 12015 exited with return code 7
crmd: [8949]: info: process_lrm_event: LRM operation vm-fir_monitor_0 (call=112, rc=7, cib-update=130, confirmed=true) not running
attrd: [8947]: notice: attrd_trigger_update: Sending flush op to all hosts for: probe_complete (true)
attrd: [8947]: notice: attrd_perform_update: Sent update 4: probe_complete=true

Then I cleaned up some resources running on other nodes, which resulted
in those showing up in the crm status output providing log lines like eg.:

crmd: [8949]: WARN: status_from_rc: Action 4 (vm-web5_monitor_0) on n02 failed (target: 7 vs. rc: 0): Error

Finally, I exited maintenance mode, and Pacemaker started every resource
I did not clean up beforehand, concurrently with their already running
instances:

pengine: [8948]: notice: LogActions: Start   vm-web9#011(n03)

I can provide more logs if this behavior is indeed unexpected, but it
looks more like I miss the exact concept of maintenance mode.

> The discovery usually happens at the point the cluster is started on a node.

A local discovery did happen, but it could not find anything, as the
cluster was started by the init scripts, well before any resource could
have been moved to the freshly rebooted node (manually, to free the next
node for rebooting).

> Maintenance mode just prevents the cluster from doing anything about it.

Fine.  So I should have restarted Pacemaker on each node before leaving
maintenance mode, right?  Or is there a better way?  (Unfortunately, I
could not manage the rolling reboot through Pacemaker, as some DLM/cLVM
freeze made the cluster inoperable in its normal way.)

>> but it probably did not happen, as the cluster started up resources
>> already running on other nodes, which is generally forbidden.  Given
>> that all resources were running (though possibly migrated during the
>> maintenance), what would have been the correct way of bringing the
>> cluster out of maintenance mode?  This should have required no
>> resource actions at all.  Would cleanup of all resources have helped?
>> Or is there a better way?

You say in the above thread that resource definitions can be changed:
http://thread.gmane.org/gmane.linux.highavailability.user/39121/focus=39437
Let me quote from there (starting with the words of Ulrich Windl):

>>>> I think it's a common misconception that you can modify cluster
>>>> resources while in maintenance mode:
>>> 
>>> No, you _should_ be able to.  If that's not the case, its a bug.
>> 
>> So the end of maintenance mode starts with a "re-probe"?
>
> No, but it doesn't need to.  
> The policy engine already knows if the resource definitions changed
> and the recurring monitor ops will find out if any are not running.

My experiences show that you may not *move around* resources while in
maintenance mode.  That would indeed require a cluster-wide re-probe,
which does not seem to happen (unless forced some way).  Probably there
was some misunderstanding in the above discussion, I guess Ulrich meant
moving resources when he wrote "modifying cluster resources".  Does this
make sense?
-- 
Thanks,
Feri.

From wferi@niif.hu Tue Aug 26 16:42:16 2014
Received: from int-mx14.intmail.prod.int.phx2.redhat.com
	(int-mx14.intmail.prod.int.phx2.redhat.com [10.5.11.27])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7QKgGg2026596 for <linux-cluster@listman.util.phx.redhat.com>;
	Tue, 26 Aug 2014 16:42:16 -0400
Received: from mx1.redhat.com (ext-mx11.extmail.prod.ext.phx2.redhat.com
	[10.5.110.16])
	by int-mx14.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7QKgGtS017221
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 16:42:16 -0400
Received: from listserv2.niif.hu (listserv2.niif.hu [193.225.14.155])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7QKgDXq004275
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES128-SHA bits=128 verify=NO)
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 16:42:14 -0400
Received: from business-188-142-225-206.business.broadband.hu
	([188.142.225.206] helo=lant.ki.iif.hu)
	by listserv2.niif.hu with esmtpsa (TLS1.2:DHE_RSA_AES_128_CBC_SHA1:128)
	(Exim 4.80) (envelope-from <wferi@niif.hu>) id 1XMNZE-00060m-Pt
	for linux-cluster@redhat.com; Tue, 26 Aug 2014 22:42:12 +0200
Received: from wferi by lant.ki.iif.hu with local (Exim 4.80)
	(envelope-from <wferi@lant.ki.iif.hu>) id 1XMNZ9-00034y-4Z
	for linux-cluster@redhat.com; Tue, 26 Aug 2014 22:42:07 +0200
From: Ferenc Wagner <wferi@niif.hu>
To: linux clustering <linux-cluster@redhat.com>
Date: Tue, 26 Aug 2014 22:42:07 +0200
Message-ID: <871ts39e5c.fsf@lant.ki.iif.hu>
User-Agent: Gnus/5.13 (Gnus v5.13) Emacs/23.4 (gnu/linux)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
X-RedHat-Spam-Score: -4.199  (BAYES_00,RCVD_IN_DNSWL_MED,RP_MATCHES_RCVD)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.27
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.16
X-loop: linux-cluster@redhat.com
Subject: [Linux-cluster] locating a starting resource
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Tue, 26 Aug 2014 20:42:16 -0000

Hi,

crm_resource --locate finds the hosting node of a running (successfully
started) resource just fine.  Is there a way to similarly find out the
location of a resource *being* started, ie. whose resource agent is
already running the start action, but that action is not finished yet?
-- 
Thanks,
Feri.

From andrew@beekhof.net Tue Aug 26 19:46:25 2014
Received: from int-mx10.intmail.prod.int.phx2.redhat.com
	(int-mx10.intmail.prod.int.phx2.redhat.com [10.5.11.23])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7QNkPMw006678 for <linux-cluster@listman.util.phx.redhat.com>;
	Tue, 26 Aug 2014 19:46:25 -0400
Received: from mx1.redhat.com (ext-mx16.extmail.prod.ext.phx2.redhat.com
	[10.5.110.21])
	by int-mx10.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7QNkPxd012218
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 19:46:25 -0400
Received: from out2-smtp.messagingengine.com (out2-smtp.messagingengine.com
	[66.111.4.26])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7QNkNVC018076
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256
	verify=NO)
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 19:46:24 -0400
Received: from compute3.internal (compute3.nyi.internal [10.202.2.43])
	by gateway2.nyi.internal (Postfix) with ESMTP id 1D29920873
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 19:46:23 -0400 (EDT)
Received: from frontend2 ([10.202.2.161])
	by compute3.internal (MEProxy); Tue, 26 Aug 2014 19:46:23 -0400
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=beekhof.net; h=
	content-type:subject:mime-version:from:in-reply-to:date
	:message-id:references:to; s=mesmtp; bh=7cFv7ahPDzav8AG5LM6MRTtD
	eI4=; b=fYo8Hu2dF1EU1pWZPqqXroNvpjdi1tmiMoVKDcJIXL6UGt2giXtv4xp8
	xkR5DkrpCzosxiLuUK7pq61DFzJrOlqz1p3uxl9x8aHjyuoQgHSnpPqjBtWIBqkP
	etJJRx4keGveZ5SpQA9Rb76C1VmaeblagGmfpDFWGOztBeSh5+8=
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=
	messagingengine.com; h=content-type:subject:mime-version:from
	:in-reply-to:date:message-id:references:to; s=smtpout; bh=7cFv7a
	hPDzav8AG5LM6MRTtDeI4=; b=A/YB4kpRY7LdiONhO13J/eKttHE5GzSjR/tkWw
	mGAOoT+QYKxL6Owna8zAwbpNXc6c7BWmI+A8k83hnLc1ouCNy0E6qxAEkgjc1M4M
	N3r6HChv+dcDOFMF0IxftK9xqVstpY7FBFdvQVZF9llDlL6VH89GWyD12cClZvZb
	TBP6A=
X-Sasl-enc: rWzPpp7FFgxl18UZyBaVvXAJ5yzSd0oqGZQW0AzlvDuM 1409096782
Received: from [172.16.1.5] (unknown [120.147.36.73])
	by mail.messagingengine.com (Postfix) with ESMTPA id 119DF68025E
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 19:46:21 -0400 (EDT)
Content-Type: multipart/signed;
	boundary="Apple-Mail=_DFE6F4EF-BA08-470A-8141-D2A338C34BBD";
	protocol="application/pgp-signature"; micalg=pgp-sha512
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
From: Andrew Beekhof <andrew@beekhof.net>
In-Reply-To: <871ts39e5c.fsf@lant.ki.iif.hu>
Date: Wed, 27 Aug 2014 09:46:18 +1000
X-Mao-Original-Outgoing-Id: 430789577.368869-4039416e340ef1d4f8e3718d80c9a0fd
Message-Id: <EBFD09CA-65DD-4759-B7F3-B73039FC6B3F@beekhof.net>
References: <871ts39e5c.fsf@lant.ki.iif.hu>
To: linux clustering <linux-cluster@redhat.com>
X-RedHat-Spam-Score: -3.1  (BAYES_00, DCC_REPUT_00_12, DKIM_SIGNED, DKIM_VALID,
	DKIM_VALID_AU, RCVD_IN_DNSWL_LOW)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.23
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.21
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] locating a starting resource
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Tue, 26 Aug 2014 23:46:25 -0000


--Apple-Mail=_DFE6F4EF-BA08-470A-8141-D2A338C34BBD
Content-Transfer-Encoding: 7bit
Content-Type: text/plain;
	charset=us-ascii


On 27 Aug 2014, at 6:42 am, Ferenc Wagner <wferi@niif.hu> wrote:

> Hi,
> 
> crm_resource --locate finds the hosting node of a running (successfully
> started) resource just fine.  Is there a way to similarly find out the
> location of a resource *being* started, ie. whose resource agent is
> already running the start action, but that action is not finished yet?

You need to set record-pending=true in the op_defaults section.
For some reason this is not yet documented :-/

With this in place, crm_resource will find the correct location


> -- 
> Thanks,
> Feri.
> 
> -- 
> Linux-cluster mailing list
> Linux-cluster@redhat.com
> https://www.redhat.com/mailman/listinfo/linux-cluster


--Apple-Mail=_DFE6F4EF-BA08-470A-8141-D2A338C34BBD
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment;
	filename=signature.asc
Content-Type: application/pgp-signature;
	name=signature.asc
Content-Description: Message signed with OpenPGP using GPGMail

-----BEGIN PGP SIGNATURE-----
Comment: GPGTools - http://gpgtools.org

iQIcBAEBCgAGBQJT/RxJAAoJEBTzwpg4iwmNMhUP/RuR2tgorkHT7OV/fU6QWbJW
aYD1I34rgtnjbl3u3fcQpLEq7zl0onSVwCooV6AxI7Shz+ilfAiMDkmGkrK33ZMy
OKd3rBx9kOCYcN7fi/G+747A55bJSDDxbeWMgutS9l4R0abYBFMiuHLqj0RZPb+3
jl7twExjN/dTdZpQWXTqevXGHfdcWRux8V+kO80DLJ9K4F0izVcWoCjWSeA29G5L
pQWKZheDnNajAeObWT+g4TEbWevZ/JLtMf61TXF7z6EpB5c2NZy95xRxpDqe0v2H
EOTrlGvTffulqOvtun+ePaPTIn8/AEf0sJFV/69ZunSBPXj/CV7Jw3xJZwVzOQ0A
fPV3vr462t7Kk8QhuFtE9bLFvX8ApDJ6WrTpLMppD+ft/QmwyLom/0HwMzV8lL1O
AhiVorPbbeY9nMLc0QDWp7zOwt2fzVjwAgQ1DBLwbGL7mfZZZpH9MYzqg7zDB8T7
sXW/ktZyGVJWc5fiXhyLeTaDHXP6ozWfKSa1Ohr/LpCtx7UnDOsSTo/Rp4l9v0BZ
vd4BvnaP8DsPdMej89XzF/TpyEK4TrpqIt6AcrJdRZRqoMv42rrd3g2BZaaCkrYd
tL2olks72aFFwp23SrU56CzhYyL6Vw0jKAxaHOwTaiLvq28TUiS8CqEFUhlnKJD3
sCM6ynW4g3uwCsQs1aWx
=X4K8
-----END PGP SIGNATURE-----

--Apple-Mail=_DFE6F4EF-BA08-470A-8141-D2A338C34BBD--

From andrew@beekhof.net Wed Aug 27 00:54:45 2014
Received: from int-mx14.intmail.prod.int.phx2.redhat.com
	(int-mx14.intmail.prod.int.phx2.redhat.com [10.5.11.27])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7R4sjjI012629 for <linux-cluster@listman.util.phx.redhat.com>;
	Wed, 27 Aug 2014 00:54:45 -0400
Received: from mx1.redhat.com (ext-mx13.extmail.prod.ext.phx2.redhat.com
	[10.5.110.18])
	by int-mx14.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7R4sjAG011155
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 00:54:45 -0400
Received: from out2-smtp.messagingengine.com (out2-smtp.messagingengine.com
	[66.111.4.26])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7R4sgEr008543
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256
	verify=NO)
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 00:54:43 -0400
Received: from compute2.internal (compute2.nyi.internal [10.202.2.42])
	by gateway2.nyi.internal (Postfix) with ESMTP id 5B3AA20914
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 00:54:42 -0400 (EDT)
Received: from frontend1 ([10.202.2.160])
	by compute2.internal (MEProxy); Wed, 27 Aug 2014 00:54:42 -0400
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=beekhof.net; h=
	content-type:subject:mime-version:from:in-reply-to:date
	:message-id:references:to; s=mesmtp; bh=LVr4oO7C8AzGDM29g7UpaRgm
	mko=; b=OvOZZ+i0o/cJHVaPrAYtgq6yIJCo3uc2utdL5LokNH6MvTo/zzXJPWjm
	fYwjXnHCRFdkCvcHcN4vjNgVZUJMlGbMG/5CqmNW3Ck1Ab3SN5qUFrapAq1sC17Q
	4yWvswgaFvqePWbQcV5xosHmQlwA+bXWDlKV89InD6DTbB7Dr5E=
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=
	messagingengine.com; h=content-type:subject:mime-version:from
	:in-reply-to:date:message-id:references:to; s=smtpout; bh=LVr4oO
	7C8AzGDM29g7UpaRgmmko=; b=X0golLNZzHNXezhgKS3OKrdWYlO04dq3bCfuTM
	1kS1oxAusE7pwqhYfTX62htM71qZ2FR1wWRwG8O97MD34rkMjmRnoIUB8enFgLsi
	S0ms995NubZUah2oQa4Af4SK+27hNGY8hkX/8J1D5KdOoPSFChsREVXsQ5OI4jl8
	dS4xA=
X-Sasl-enc: fYRcX1Dk671vKSEg4zXk7xFbR/OIUz2IFVWXO808zaRK 1409115281
Received: from [172.16.1.5] (unknown [120.147.36.73])
	by mail.messagingengine.com (Postfix) with ESMTPA id 0050DC0091B
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 00:54:40 -0400 (EDT)
Content-Type: multipart/signed;
	boundary="Apple-Mail=_0B6CC477-554F-4EE4-9FEA-2CBB77437EEA";
	protocol="application/pgp-signature"; micalg=pgp-sha512
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
From: Andrew Beekhof <andrew@beekhof.net>
In-Reply-To: <87iolf9mkc.fsf@lant.ki.iif.hu>
Date: Wed, 27 Aug 2014 14:54:33 +1000
X-Mao-Original-Outgoing-Id: 430808071.527142-ed874a6411107a0e216331c9e8a4cd94
Message-Id: <F1686CC9-7920-4B72-981D-D6057EC2B754@beekhof.net>
References: <8761hlickn.fsf@lant.ki.iif.hu>
	<67506B71-8594-4C16-82C1-F94779F59826@beekhof.net>
	<87iolf9mkc.fsf@lant.ki.iif.hu>
To: linux clustering <linux-cluster@redhat.com>
X-RedHat-Spam-Score: -2.7  (BAYES_00, DKIM_SIGNED, DKIM_VALID, DKIM_VALID_AU,
	RCVD_IN_DNSWL_LOW)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.27
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.18
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] on exiting maintenance mode
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Wed, 27 Aug 2014 04:54:45 -0000


--Apple-Mail=_0B6CC477-554F-4EE4-9FEA-2CBB77437EEA
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=us-ascii


On 27 Aug 2014, at 3:40 am, Ferenc Wagner <wferi@niif.hu> wrote:

> Andrew Beekhof <andrew@beekhof.net> writes:
>=20
>> On 22 Aug 2014, at 10:37 am, Ferenc Wagner <wferi@niif.hu> wrote:
>>=20
>>> While my Pacemaker cluster was in maintenance mode, resources were =
moved
>>> (by hand) between the nodes as I rebooted each node in turn.  In the =
end
>>> the crm status output became perfectly empty, as the reboot of a =
given
>>> node removed from the output the resources which were located on the
>>> rebooted node at the time of entering maintenance mode.  I expected =
full
>>> resource discovery on exiting maintenance mode,
>>=20
>> Version and logs?
>=20
> (The more interesting part comes later, please skip to the theoretical
> part if you're short on time. :)
>=20
> I left those out, as I don't expect the actual behavior to be a bug.
> But I experienced this with Pacemaker version 1.1.7.  I know it's old

No kidding :)

> and it suffers from crmd segfault on entering maintenance mode (cf.
> http://thread.gmane.org/gmane.linux.highavailability.user/39121), but
> works well generally so I did not get to upgrade it yet.  Now that I
> mentioned the crmd segfault: I noted that it died on the DC when I
> entered maintenance mode:
>=20
> crmd: [7452]: info: te_rsc_command: Initiating action 64: cancel =
vm-tmvp_monitor_60000 on n01 (local)
> crmd: [7452]: ERROR: lrm_get_rsc(666): failed to send a getrsc message =
to lrmd via ch_cmd channel.

That looks like the lrmd died.

> crmd: [7452]: ERROR: get_lrm_resource: Could not add resource vm-tmvp =
to LRM
> crmd: [7452]: ERROR: do_lrm_invoke: Invalid resource definition
> crmd: [7452]: WARN: do_lrm_invoke: bad input <create_request_adv =
origin=3D"te_rsc_command" t=3D"crmd" version=3D"3.0.6" subt=3D"request" =
reference=3D"lrm_invoke-tengine-1408517719-30820" crm_task=3D"lrm_invoke" =
crm_sys_to=3D"lrmd" crm_sys_from=3D"tengine" crm_host_to=3D"n01" >
> crmd: [7452]: WARN: do_lrm_invoke: bad input   <crm_xml >
> crmd: [7452]: WARN: do_lrm_invoke: bad input     <rsc_op id=3D"64" =
operation=3D"cancel" operation_key=3D"vm-tmvp_monitor_60000" =
on_node=3D"n01" on_node_uuid=3D"n01" =
transition-key=3D"64:20579:0:1b0a6e79-af5a-41e4-8ced-299371e7922c" >
> crmd: [7452]: WARN: do_lrm_invoke: bad input       <primitive =
id=3D"vm-tmvp" long-id=3D"vm-tmvp" class=3D"ocf" provider=3D"niif" =
type=3D"TransientDomain" />
> crmd: [7452]: info: te_rsc_command: Initiating action 86: cancel =
vm-wfweb_monitor_60000 on n01 (local)
> crmd: [7452]: ERROR: lrm_add_rsc(870): failed to send a addrsc message =
to lrmd via ch_cmd channel.
> crmd: [7452]: ERROR: lrm_get_rsc(666): failed to send a getrsc message =
to lrmd via ch_cmd channel.
> corosync[6966]:   [pcmk  ] info: pcmk_ipc_exit: Client crmd =
(conn=3D0x1dc6ea0, async-conn=3D0x1dc6ea0) left
> pacemakerd: [7443]: WARN: Managed crmd process 7452 killed by signal =
11 [SIGSEGV - Segmentation violation].

Which created a condition in the crmd that it couldn't handle so it =
crashed too.

> pacemakerd: [7443]: notice: pcmk_child_exit: Child process crmd =
terminated with signal 11 (pid=3D7452, rc=3D0)
>=20
> However, it got restarted seamlessly, without the node being fenced, =
so
> I did not even notice this until now.  Should this have resulted in =
the
> node being fenced?

Depends how fast the node can respawn.

>=20
> But back to the issue at hand.  The Pacemaker shutdown seemed normal,
> apart from the bunch of messages like:
>=20
> crmd: [13794]: ERROR: verify_stopped: Resource vm-web5 was active at =
shutdown.  You may ignore this error if it is unmanaged.

In maintenance mode, everything is unmanaged. So that would be expected.

>=20
> appearing twice and warnings like:
>=20
> cib: [7447]: WARN: send_ipc_message: IPC Channel to 13794 is not =
connected
> cib: [7447]: WARN: send_via_callback_channel: Delivery of reply to =
client 13794/bf6f43a2-70db-40ac-a902-eabc3c12e20d failed
> cib: [7447]: WARN: do_local_notify: A-Sync reply to crmd failed: reply =
failed
> corosync[6966]:   [pcmk  ] WARN: route_ais_message: Sending message to =
local.crmd failed: ipc delivery failed (rc=3D-2)
>=20
> On reboot, corosync complained until the some Pacemaker components
> started:
>=20
> corosync[8461]:   [pcmk  ] WARN: route_ais_message: Sending message to =
local.cib failed: ipc delivery failed (rc=3D-2)
> corosync[8461]:   [pcmk  ] WARN: route_ais_message: Sending message to =
local.crmd failed: ipc delivery failed (rc=3D-2)
>=20
> Pacemaker then probed the resources on the local node (all was =
inactive):
>=20
> lrmd: [8946]: info: rsc:stonith-n01 probe[5] (pid 9081)
> lrmd: [8946]: info: rsc:dlm:0 probe[6] (pid 9082)
> [...]
> lrmd: [8946]: info: operation monitor[112] on vm-fir for client 8949: =
pid 12015 exited with return code 7
> crmd: [8949]: info: process_lrm_event: LRM operation vm-fir_monitor_0 =
(call=3D112, rc=3D7, cib-update=3D130, confirmed=3Dtrue) not running
> attrd: [8947]: notice: attrd_trigger_update: Sending flush op to all =
hosts for: probe_complete (true)
> attrd: [8947]: notice: attrd_perform_update: Sent update 4: =
probe_complete=3Dtrue
>=20
> Then I cleaned up some resources running on other nodes, which =
resulted
> in those showing up in the crm status output providing log lines like =
eg.:
>=20
> crmd: [8949]: WARN: status_from_rc: Action 4 (vm-web5_monitor_0) on =
n02 failed (target: 7 vs. rc: 0): Error
>=20
> Finally, I exited maintenance mode, and Pacemaker started every =
resource
> I did not clean up beforehand, concurrently with their already running
> instances:
>=20
> pengine: [8948]: notice: LogActions: Start   vm-web9#011(n03)
>=20
> I can provide more logs if this behavior is indeed unexpected, but it
> looks more like I miss the exact concept of maintenance mode.
>=20
>> The discovery usually happens at the point the cluster is started on =
a node.
>=20
> A local discovery did happen, but it could not find anything, as the
> cluster was started by the init scripts, well before any resource =
could
> have been moved to the freshly rebooted node (manually, to free the =
next
> node for rebooting).

Thats your problem then, you've started resources outside of the control =
of the cluster.
Two options... recurring monitor actions with role=3DStopped would have =
caught this or you can run crm_resource --cleanup after you've moved =
resources around.

>=20
>> Maintenance mode just prevents the cluster from doing anything about =
it.
>=20
> Fine.  So I should have restarted Pacemaker on each node before =
leaving
> maintenance mode, right?  Or is there a better way?

See above

>  (Unfortunately, I
> could not manage the rolling reboot through Pacemaker, as some =
DLM/cLVM
> freeze made the cluster inoperable in its normal way.)
>=20
>>> but it probably did not happen, as the cluster started up resources
>>> already running on other nodes, which is generally forbidden.  Given
>>> that all resources were running (though possibly migrated during the
>>> maintenance), what would have been the correct way of bringing the
>>> cluster out of maintenance mode?  This should have required no
>>> resource actions at all.  Would cleanup of all resources have =
helped?
>>> Or is there a better way?
>=20
> You say in the above thread that resource definitions can be changed:
> =
http://thread.gmane.org/gmane.linux.highavailability.user/39121/focus=3D39=
437
> Let me quote from there (starting with the words of Ulrich Windl):
>=20
>>>>> I think it's a common misconception that you can modify cluster
>>>>> resources while in maintenance mode:
>>>>=20
>>>> No, you _should_ be able to.  If that's not the case, its a bug.
>>>=20
>>> So the end of maintenance mode starts with a "re-probe"?
>>=20
>> No, but it doesn't need to. =20
>> The policy engine already knows if the resource definitions changed
>> and the recurring monitor ops will find out if any are not running.
>=20
> My experiences show that you may not *move around* resources while in
> maintenance mode.

Correct

>  That would indeed require a cluster-wide re-probe,
> which does not seem to happen (unless forced some way).  Probably =
there
> was some misunderstanding in the above discussion, I guess Ulrich =
meant
> moving resources when he wrote "modifying cluster resources".  Does =
this
> make sense?

No, I've reasonably sure he meant changing their definitions in the cib.
Or at least thats what I thought he meant at the time.

> --=20
> Thanks,
> Feri.
>=20
> --=20
> Linux-cluster mailing list
> Linux-cluster@redhat.com
> https://www.redhat.com/mailman/listinfo/linux-cluster


--Apple-Mail=_0B6CC477-554F-4EE4-9FEA-2CBB77437EEA
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment;
	filename=signature.asc
Content-Type: application/pgp-signature;
	name=signature.asc
Content-Description: Message signed with OpenPGP using GPGMail

-----BEGIN PGP SIGNATURE-----
Comment: GPGTools - http://gpgtools.org

iQIcBAEBCgAGBQJT/WSIAAoJEBTzwpg4iwmNPJsQAIUBZpyTx9McgIIDlwo3s6k8
Dllw97TFRCIAzQxoSFiq+bGstOhdpBl1iTcnNpSCCTUnabCJzjx6ZMpoT4MDCbDs
cU86gl0HP1VYHQKj6sJf1qE2CeyV0rYupeObqPgBuGT13gFp5J2cUwvyA8lKIt0F
pCtDTRSMrq/Vm+sqrFOdOkuIn7ITn+WIA+ecw88w/PJV2lQlD/LySq0CeObqfIqw
zmPAC9TVYu230XJrccjFJb77AL058Jt4ovF/ZCnOaDRwhBiquJ5j0o39sbj2CRg8
0MaoJ03qglT1pbQtmIdKSykQIdHDUFXlVtZpfLZ9wMR5n+7n2dgMkILOSrZhN0PM
j9s3ZZ54LvyZLmOznbP7T7fAyJ2vLVrZM41tUMJFFdpA7V2/L7jz1YJXavFfwenA
8xx46yg5psDqtmVccMPacfiPr0zlyQJEQ8/bGlVbrRc2CYGfAPTgxvtILd9LVbdV
kvQbGQHuGdj84YMzaGEFSD4UQlk964fmY1h7GKB3wN2lOJbY8kaEnDcJsAwAxqsh
aotI5FmbBcl7toFvGS4Pcs1hjiajvvRXkOEG1TN5ldcyoMQPlrrzSrAPHg5tK4G2
Qp2ujtSfkrJZxC0fYT3ycQ2x2aVRiY9qku15pZzxLXUblPw/muyTsQuJHBxRpNCn
jgD5L+gqSKAx0q6zik87
=yubI
-----END PGP SIGNATURE-----

--Apple-Mail=_0B6CC477-554F-4EE4-9FEA-2CBB77437EEA--

From wferi@niif.hu Wed Aug 27 13:09:57 2014
Received: from int-mx14.intmail.prod.int.phx2.redhat.com
	(int-mx14.intmail.prod.int.phx2.redhat.com [10.5.11.27])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7RH9v32003239 for <linux-cluster@listman.util.phx.redhat.com>;
	Wed, 27 Aug 2014 13:09:57 -0400
Received: from mx1.redhat.com (ext-mx12.extmail.prod.ext.phx2.redhat.com
	[10.5.110.17])
	by int-mx14.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7RH9v2U027478
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 13:09:57 -0400
Received: from listserv2.niif.hu (listserv2.niif.hu [193.225.14.155])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7RH9q8t027027
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES128-SHA bits=128 verify=NO)
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 13:09:54 -0400
Received: from business-188-142-225-206.business.broadband.hu
	([188.142.225.206] helo=lant.ki.iif.hu)
	by listserv2.niif.hu with esmtpsa (TLS1.2:DHE_RSA_AES_128_CBC_SHA1:128)
	(Exim 4.80) (envelope-from <wferi@niif.hu>) id 1XMgjH-0008FQ-HO
	for linux-cluster@redhat.com; Wed, 27 Aug 2014 19:09:51 +0200
Received: from wferi by lant.ki.iif.hu with local (Exim 4.80)
	(envelope-from <wferi@lant.ki.iif.hu>) id 1XMgjB-0007CR-UV
	for linux-cluster@redhat.com; Wed, 27 Aug 2014 19:09:45 +0200
From: Ferenc Wagner <wferi@niif.hu>
To: linux clustering <linux-cluster@redhat.com>
References: <8761hlickn.fsf@lant.ki.iif.hu>
	<67506B71-8594-4C16-82C1-F94779F59826@beekhof.net>
	<87iolf9mkc.fsf@lant.ki.iif.hu>
	<F1686CC9-7920-4B72-981D-D6057EC2B754@beekhof.net>
Date: Wed, 27 Aug 2014 19:09:45 +0200
In-Reply-To: <F1686CC9-7920-4B72-981D-D6057EC2B754@beekhof.net> (Andrew
	Beekhof's message of "Wed, 27 Aug 2014 14:54:33 +1000")
Message-ID: <87iold7tba.fsf@lant.ki.iif.hu>
User-Agent: Gnus/5.13 (Gnus v5.13) Emacs/23.4 (gnu/linux)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
X-RedHat-Spam-Score: -4.199  (BAYES_00,RCVD_IN_DNSWL_MED,RP_MATCHES_RCVD)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.27
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.17
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] on exiting maintenance mode
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Wed, 27 Aug 2014 17:09:58 -0000

Andrew Beekhof <andrew@beekhof.net> writes:

> On 27 Aug 2014, at 3:40 am, Ferenc Wagner <wferi@niif.hu> wrote:
>
>> Andrew Beekhof <andrew@beekhof.net> writes:
>> 
>>> On 22 Aug 2014, at 10:37 am, Ferenc Wagner <wferi@niif.hu> wrote:
>>> 
>>>> While my Pacemaker cluster was in maintenance mode, resources were moved
>>>> (by hand) between the nodes as I rebooted each node in turn.  In the end
>>>> the crm status output became perfectly empty, as the reboot of a given
>>>> node removed from the output the resources which were located on the
>>>> rebooted node at the time of entering maintenance mode.  I expected full
>>>> resource discovery on exiting maintenance mode,
>>
>> I experienced this with Pacemaker version 1.1.7.  I know it's old
>> and it suffers from crmd segfault on entering maintenance mode (cf.
>> http://thread.gmane.org/gmane.linux.highavailability.user/39121), but
>> works well generally so I did not get to upgrade it yet.  Now that I
>> mentioned the crmd segfault: I noted that it died on the DC when I
>> entered maintenance mode:
>> 
>> crmd: [7452]: info: te_rsc_command: Initiating action 64: cancel vm-tmvp_monitor_60000 on n01 (local)
>> crmd: [7452]: ERROR: lrm_get_rsc(666): failed to send a getrsc message to lrmd via ch_cmd channel.
>
> That looks like the lrmd died.

It did not die, at least not fully.  After entering maintenance mode
crmd asked lrmd to cancel the recurring monitor ops for all resources:

08:40:18 crmd: [7452]: info: do_te_invoke: Processing graph 20578 (ref=pe_calc-dc-1408516818-30681) derived from /var/lib/pengine/pe-input-848.bz2
08:40:18 crmd: [7452]: info: te_rsc_command: Initiating action 17: cancel dlm:0_monitor_120000 on n04
08:40:18 crmd: [7452]: info: te_rsc_command: Initiating action 84: cancel dlm:0_cancel_120000 on n01 (local)
08:40:18 lrmd: [7449]: info: cancel_op: operation monitor[194] on dlm:0 for client 7452, its parameters: [...] cancelled
08:40:18 crmd: [7452]: info: te_rsc_command: Initiating action 50: cancel dlm:2_monitor_120000 on n02

The stream of monitor op cancellation messages ended with:

08:40:18 crmd: [7452]: info: te_rsc_command: Initiating action 71: cancel vm-mdssq_monitor_60000 on n01 (local)
08:40:18 lrmd: [7449]: info: cancel_op: operation monitor[329] on vm-mdssq for client 7452, its parameters: [...] cancelled
08:40:18 crmd: [7452]: info: process_lrm_event: LRM operation vm-mdssq_monitor_60000 (call=329, status=1, cib-update=0, confirmed=true) Cancelled
08:40:18 crmd: [7452]: notice: run_graph: ==== Transition 20578 (Complete=87, Pending=0, Fired=0, Skipped=0, Incomplete=0, Source=/var/lib/pengine/pe-input-848.bz2): Complete
08:40:18 crmd: [7452]: notice: do_state_transition: State transition S_TRANSITION_ENGINE -> S_IDLE [ input=I_TE_SUCCESS cause=C_FSA_INTERNAL origin=notify_crmd ]
08:40:18 pengine: [7451]: notice: process_pe_message: Transition 20578: PEngine Input stored in: /var/lib/pengine/pe-input-848.bz2
08:41:28 crmd: [7452]: WARN: action_timer_callback: Timer popped (timeout=10000, abort_level=0, complete=true)
08:41:28 crmd: [7452]: WARN: action_timer_callback: Ignoring timeout while not in transition
[these two lines repeated several times]
08:41:28 crmd: [7452]: WARN: action_timer_callback: Timer popped (timeout=10000, abort_level=0, complete=true)
08:41:28 crmd: [7452]: WARN: action_timer_callback: Ignoring timeout while not in transition
08:41:38 crmd: [7452]: WARN: action_timer_callback: Timer popped (timeout=20000, abort_level=0, complete=true)
08:41:38 crmd: [7452]: WARN: action_timer_callback: Ignoring timeout while not in transition
08:48:05 cib: [7447]: info: cib_stats: Processed 159 operations (23207.00us average, 0% utilization) in the last 10min
08:55:18 crmd: [7452]: info: crm_timer_popped: PEngine Recheck Timer (I_PE_CALC) just popped (900000ms)
08:55:18 crmd: [7452]: notice: do_state_transition: State transition S_IDLE -> S_POLICY_ENGINE [ input=I_PE_CALC cause=C_TIMER_POPPED origin=crm_timer_popped ]
08:55:18 crmd: [7452]: info: do_state_transition: Progressed to state S_POLICY_ENGINE after C_TIMER_POPPED
08:55:19 pengine: [7451]: notice: stage6: Delaying fencing operations until there are resources to manage
08:55:19 crmd: [7452]: notice: do_state_transition: State transition S_POLICY_ENGINE -> S_TRANSITION_ENGINE [ input=I_PE_SUCCESS cause=C_IPC_MESSAGE origin=handle_response ]
08:55:19 crmd: [7452]: info: do_te_invoke: Processing graph 20579 (ref=pe_calc-dc-1408517718-30802) derived from /var/lib/pengine/pe-input-849.bz2
08:55:19 crmd: [7452]: info: te_rsc_command: Initiating action 17: cancel dlm:0_monitor_120000 on n04
08:55:19 crmd: [7452]: info: te_rsc_command: Initiating action 84: cancel dlm:0_cancel_120000 on n01 (local)
08:55:19 crmd: [7452]: info: cancel_op: No pending op found for dlm:0:194
08:55:19 lrmd: [7449]: info: on_msg_cancel_op: no operation with id 194

Interestingly, monitor[194], lastly mentioned by lrmd, was the very
first cancelled operation.

08:55:19 crmd: [7452]: info: te_rsc_command: Initiating action 50: cancel dlm:2_monitor_120000 on n02
08:55:19 crmd: [7452]: info: te_rsc_command: Initiating action 83: cancel vm-cedar_monitor_60000 on n01 (local)
08:55:19 crmd: [7452]: ERROR: lrm_get_rsc(673): failed to receive a reply message of getrsc.
08:55:19 crmd: [7452]: ERROR: lrm_get_rsc(666): failed to send a getrsc message to lrmd via ch_cmd channel.
08:55:19 crmd: [7452]: ERROR: lrm_add_rsc(870): failed to send a addrsc message to lrmd via ch_cmd channel.
08:55:19 crmd: [7452]: ERROR: lrm_get_rsc(666): failed to send a getrsc message to lrmd via ch_cmd channel.
08:55:19 crmd: [7452]: ERROR: get_lrm_resource: Could not add resource vm-cedar to LRM
08:55:19 crmd: [7452]: ERROR: do_lrm_invoke: Invalid resource definition
08:55:19 crmd: [7452]: WARN: do_lrm_invoke: bad input <create_request_adv origin="te_rsc_command" t="crmd" version="3.0.6" subt="request" reference="lrm_invoke-tengine-1408517719-30807" crm_task="lrm_invoke" crm_sys_to="lrmd" crm_sys_from="tengine" crm_host_to="n01" >
08:55:19 crmd: [7452]: WARN: do_lrm_invoke: bad input   <crm_xml >
08:55:19 crmd: [7452]: WARN: do_lrm_invoke: bad input     <rsc_op id="83" operation="cancel" operation_key="vm-cedar_monitor_60000" on_node="n01" on_node_uuid="n01" transition-key="83:20579:0:1b0a6e79-af5a-41e4-8ced-299371e7922c" >
08:55:19 crmd: [7452]: WARN: do_lrm_invoke: bad input       <primitive id="vm-cedar" long-id="vm-cedar" class="ocf" provider="niif" type="TransientDomain" />
08:55:19 crmd: [7452]: ERROR: log_data_element: Output truncated: available=727, needed=1374
08:55:19 crmd: [7452]: WARN: do_lrm_invoke: bad input       <attributes CRM_meta_call_id="195" [really very long]
08:55:19 crmd: [7452]: WARN: do_lrm_invoke: bad input     </rsc_op>
08:55:19 crmd: [7452]: WARN: do_lrm_invoke: bad input   </crm_xml>
08:55:19 crmd: [7452]: WARN: do_lrm_invoke: bad input </create_request_adv>

Blocks of messages like the above repeat a couple of times for other
resources, then crmd kicks the bucket and gets restarted:

08:55:19 corosync[6966]:   [pcmk  ] info: pcmk_ipc_exit: Client crmd (conn=0x1dc6ea0, async-conn=0x1dc6ea0) left
08:55:19 pacemakerd: [7443]: WARN: Managed crmd process 7452 killed by signal 11 [SIGSEGV - Segmentation violation].
08:55:19 pacemakerd: [7443]: notice: pcmk_child_exit: Child process crmd terminated with signal 11 (pid=7452, rc=0)
08:55:19 pacemakerd: [7443]: notice: pcmk_child_exit: Respawning failed child process: crmd
08:55:19 pacemakerd: [7443]: info: start_child: Forked child 13794 for process crmd
08:55:19 corosync[6966]:   [pcmk  ] WARN: route_ais_message: Sending message to local.crmd failed: ipc delivery failed (rc=-2)
08:55:19 crmd: [13794]: info: Invoked: /usr/lib/pacemaker/crmd 

Anyway, no further logs from lrmd after this point until hours later I
rebooted the machine:

14:37:06 pacemakerd: [7443]: notice: stop_child: Stopping lrmd: Sent -15 to process 7449
14:37:06 lrmd: [7449]: info: lrmd is shutting down
14:37:06 pacemakerd: [7443]: info: pcmk_child_exit: Child process lrmd exited (pid=7449, rc=0)

So lrmd was alive all the time.

> Which created a condition in the crmd that it couldn't handle so it
> crashed too.

Maybe their connection got severed somehow.

>> However, it got restarted seamlessly, without the node being fenced, so
>> I did not even notice this until now.  Should this have resulted in the
>> node being fenced?
>
> Depends how fast the node can respawn.

You mean how fast crmd can respawn?  How much time does it have to
respawn to avoid being fenced?

>> crmd: [13794]: ERROR: verify_stopped: Resource vm-web5 was active at shutdown.  You may ignore this error if it is unmanaged.
>
> In maintenance mode, everything is unmanaged. So that would be expected.

Is maintenance mode the same as unmanaging all resources?  I think the
latter does not cancel the monitor operations here...

>>> The discovery usually happens at the point the cluster is started on
>>> a node.
>> 
>> A local discovery did happen, but it could not find anything, as the
>> cluster was started by the init scripts, well before any resource could
>> have been moved to the freshly rebooted node (manually, to free the next
>> node for rebooting).
>
> Thats your problem then, you've started resources outside of the
> control of the cluster.

Some of them, yes, and moved the rest between the nodes.  All this
circumventing the cluster.

> Two options... recurring monitor actions with role=Stopped would have
> caught this

Even in maintenance mode?  Wouldn't they have been cancelled just like
the ordinary recurring monitor actions?

I guess adding them would run a recurring monitor operation for every
resource on every node, only with different expectations, right?

> or you can run crm_resource --cleanup after you've moved resources around.

I actually ran some crm resource cleanups for a couple of resources, and
those really were not started on exiting maintenance mode.

>>> Maintenance mode just prevents the cluster from doing anything about it.
>> 
>> Fine.  So I should have restarted Pacemaker on each node before leaving
>> maintenance mode, right?  Or is there a better way?
>
> See above

So crm_resource -r whatever -C is the way, for each resource separately.
Is there no way to do this for all resources at once?

>> You say in the above thread that resource definitions can be changed:
>> http://thread.gmane.org/gmane.linux.highavailability.user/39121/focus=39437
>> Let me quote from there (starting with the words of Ulrich Windl):
>> 
>>>>>> I think it's a common misconception that you can modify cluster
>>>>>> resources while in maintenance mode:
>>>>> 
>>>>> No, you _should_ be able to.  If that's not the case, its a bug.
>>>> 
>>>> So the end of maintenance mode starts with a "re-probe"?
>>> 
>>> No, but it doesn't need to.  
>>> The policy engine already knows if the resource definitions changed
>>> and the recurring monitor ops will find out if any are not running.
>> 
>> My experiences show that you may not *move around* resources while in
>> maintenance mode.
>
> Correct
>
>> That would indeed require a cluster-wide re-probe, which does not
>> seem to happen (unless forced some way).  Probably there was some
>> misunderstanding in the above discussion, I guess Ulrich meant moving
>> resources when he wrote "modifying cluster resources".  Does this
>> make sense?
>
> No, I've reasonably sure he meant changing their definitions in the cib.
> Or at least thats what I thought he meant at the time.

Nobody could blame you for that, because that's what it means.  But then
he inquired about a "re-probe", which fits more the problem of changing
the status of resources, not their definition.  Actually, I was so
firmly stuck in this mind set, that first I wanted to ask you to
reconsider, your response felt so much out of place.  That's all about
history for now...

After all this, I suggest to clarify this issue in the fine manual.
I've read it a couple of times, and still got the wrong impression.
-- 
Regards,
Feri.

From wferi@niif.hu Wed Aug 27 14:56:44 2014
Received: from int-mx11.intmail.prod.int.phx2.redhat.com
	(int-mx11.intmail.prod.int.phx2.redhat.com [10.5.11.24])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7RIuiT1029086 for <linux-cluster@listman.util.phx.redhat.com>;
	Wed, 27 Aug 2014 14:56:44 -0400
Received: from mx1.redhat.com (ext-mx13.extmail.prod.ext.phx2.redhat.com
	[10.5.110.18])
	by int-mx11.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7RIuiAx029161
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 14:56:44 -0400
Received: from listserv2.niif.hu (listserv2.niif.hu [193.225.14.155])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7RIueDL020190
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES128-SHA bits=128 verify=NO)
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 14:56:41 -0400
Received: from business-188-142-225-206.business.broadband.hu
	([188.142.225.206] helo=lant.ki.iif.hu)
	by listserv2.niif.hu with esmtpsa (TLS1.2:DHE_RSA_AES_128_CBC_SHA1:128)
	(Exim 4.80) (envelope-from <wferi@niif.hu>) id 1XMiOc-0002zw-Rg
	for linux-cluster@redhat.com; Wed, 27 Aug 2014 20:56:38 +0200
Received: from wferi by lant.ki.iif.hu with local (Exim 4.80)
	(envelope-from <wferi@lant.ki.iif.hu>) id 1XMiOW-0008Tl-CV
	for linux-cluster@redhat.com; Wed, 27 Aug 2014 20:56:32 +0200
From: Ferenc Wagner <wferi@niif.hu>
To: linux clustering <linux-cluster@redhat.com>
References: <871ts39e5c.fsf@lant.ki.iif.hu>
	<EBFD09CA-65DD-4759-B7F3-B73039FC6B3F@beekhof.net>
Date: Wed, 27 Aug 2014 20:56:32 +0200
In-Reply-To: <EBFD09CA-65DD-4759-B7F3-B73039FC6B3F@beekhof.net> (Andrew
	Beekhof's message of "Wed, 27 Aug 2014 09:46:18 +1000")
Message-ID: <877g1t7odb.fsf@lant.ki.iif.hu>
User-Agent: Gnus/5.13 (Gnus v5.13) Emacs/23.4 (gnu/linux)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
X-RedHat-Spam-Score: -4.199  (BAYES_00,RCVD_IN_DNSWL_MED,RP_MATCHES_RCVD)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.24
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.18
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] locating a starting resource
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Wed, 27 Aug 2014 18:56:44 -0000

Andrew Beekhof <andrew@beekhof.net> writes:

> On 27 Aug 2014, at 6:42 am, Ferenc Wagner <wferi@niif.hu> wrote:
>
>> crm_resource --locate finds the hosting node of a running (successfully
>> started) resource just fine.  Is there a way to similarly find out the
>> location of a resource *being* started, ie. whose resource agent is
>> already running the start action, but that action is not finished yet?
>
> You need to set record-pending=true in the op_defaults section.
> For some reason this is not yet documented :-/
>
> With this in place, crm_resource will find the correct location

I set it in a single start operation, and it works as advertised,
thanks!  At first I was suprised to see "Started" in the crm status
output while the resource was only starting, but the added order
constraint worked as expected, ie. the dependent resource started only
after the start action finished successfully.  This begs a bonus
question: how do I tell apart starting resources with record-pendig=true
and started resources?  crm_resource --locate does not help either.
-- 
Thanks,
Feri.

From andrew@beekhof.net Wed Aug 27 18:57:35 2014
Received: from int-mx13.intmail.prod.int.phx2.redhat.com
	(int-mx13.intmail.prod.int.phx2.redhat.com [10.5.11.26])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7RMvZsD009893 for <linux-cluster@listman.util.phx.redhat.com>;
	Wed, 27 Aug 2014 18:57:35 -0400
Received: from mx1.redhat.com (ext-mx13.extmail.prod.ext.phx2.redhat.com
	[10.5.110.18])
	by int-mx13.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7RMvZUB030063
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 18:57:35 -0400
Received: from out2-smtp.messagingengine.com (out2-smtp.messagingengine.com
	[66.111.4.26])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7RMvVf2022049
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256
	verify=NO)
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 18:57:32 -0400
Received: from compute5.internal (compute5.nyi.internal [10.202.2.45])
	by gateway2.nyi.internal (Postfix) with ESMTP id 6F8242093F
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 18:57:31 -0400 (EDT)
Received: from frontend1 ([10.202.2.160])
	by compute5.internal (MEProxy); Wed, 27 Aug 2014 18:57:31 -0400
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=beekhof.net; h=
	content-type:subject:mime-version:from:in-reply-to:date
	:message-id:references:to; s=mesmtp; bh=oOB5GrkNoFOF3MV4jfAd+/2M
	0No=; b=RyVN6NP31190oeX03PRodi9JvgHpeVAaPi9LMaZQBwaKsIZY57F7ZWjY
	aCkm4kr7BQo4QLrgCgD5ms/CFhqh7ojr9Ey8phr/NrRI688Jo5iMU4wYjRWKAywS
	DVFfEnwSpA/t/RhphDp1jbQskehqDJBTOY4o2CC3F+XwwJEk38k=
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=
	messagingengine.com; h=content-type:subject:mime-version:from
	:in-reply-to:date:message-id:references:to; s=smtpout; bh=oOB5Gr
	kNoFOF3MV4jfAd+/2M0No=; b=Pk4tE0U5Ujm0ydFeLukLz9VXrsfKZRSJDhoOXw
	zBx5RbVH7bAqhTjgtDxqPiyWaRh2bcb7oH4IJnKX3lKM1xoAgx0v8w4B3oGI8V/O
	lCGy9TcVOhGoHr38L4NaGzKUQOfCrfD521fDGyL4qI91UFvKZ7tNXQ6JI7uANRVO
	OHm10=
X-Sasl-enc: 7+fBmJrJA1qTkFvVXgHWScBdGEDX7aNAsnoz6r/ejPAu 1409180250
Received: from [172.16.1.5] (unknown [120.147.36.73])
	by mail.messagingengine.com (Postfix) with ESMTPA id 48121C00915
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 18:57:29 -0400 (EDT)
Content-Type: multipart/signed;
	boundary="Apple-Mail=_2E1D6BE2-4689-42AF-9228-6313297AE65F";
	protocol="application/pgp-signature"; micalg=pgp-sha512
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
From: Andrew Beekhof <andrew@beekhof.net>
In-Reply-To: <87iold7tba.fsf@lant.ki.iif.hu>
Date: Thu, 28 Aug 2014 08:57:26 +1000
X-Mao-Original-Outgoing-Id: 430873045.712283-a9f7dd994b632fdfeaffe5b6f3abd916
Message-Id: <749665C4-F970-4C43-9228-BCFD2EE1B442@beekhof.net>
References: <8761hlickn.fsf@lant.ki.iif.hu>
	<67506B71-8594-4C16-82C1-F94779F59826@beekhof.net>
	<87iolf9mkc.fsf@lant.ki.iif.hu>
	<F1686CC9-7920-4B72-981D-D6057EC2B754@beekhof.net>
	<87iold7tba.fsf@lant.ki.iif.hu>
To: linux clustering <linux-cluster@redhat.com>
X-RedHat-Spam-Score: -2.7  (BAYES_00, DKIM_SIGNED, DKIM_VALID, DKIM_VALID_AU,
	RCVD_IN_DNSWL_LOW)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.26
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.18
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] on exiting maintenance mode
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Wed, 27 Aug 2014 22:57:36 -0000


--Apple-Mail=_2E1D6BE2-4689-42AF-9228-6313297AE65F
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=us-ascii


On 28 Aug 2014, at 3:09 am, Ferenc Wagner <wferi@niif.hu> wrote:

> Andrew Beekhof <andrew@beekhof.net> writes:
>=20
>> On 27 Aug 2014, at 3:40 am, Ferenc Wagner <wferi@niif.hu> wrote:
>=20
>>> However, it got restarted seamlessly, without the node being fenced, =
so
>>> I did not even notice this until now.  Should this have resulted in =
the
>>> node being fenced?
>>=20
>> Depends how fast the node can respawn.
>=20
> You mean how fast crmd can respawn?  How much time does it have to
> respawn to avoid being fenced?

Until a new node can be elected DC, invoke the policy engine and start =
fencing.

>=20
>>> crmd: [13794]: ERROR: verify_stopped: Resource vm-web5 was active at =
shutdown.  You may ignore this error if it is unmanaged.
>>=20
>> In maintenance mode, everything is unmanaged. So that would be =
expected.
>=20
> Is maintenance mode the same as unmanaging all resources?  I think the
> latter does not cancel the monitor operations here...

Right. One cancels monitor operations too.

>=20
>>>> The discovery usually happens at the point the cluster is started =
on
>>>> a node.
>>>=20
>>> A local discovery did happen, but it could not find anything, as the
>>> cluster was started by the init scripts, well before any resource =
could
>>> have been moved to the freshly rebooted node (manually, to free the =
next
>>> node for rebooting).
>>=20
>> Thats your problem then, you've started resources outside of the
>> control of the cluster.
>=20
> Some of them, yes, and moved the rest between the nodes.  All this
> circumventing the cluster.
>=20
>> Two options... recurring monitor actions with role=3DStopped would =
have
>> caught this
>=20
> Even in maintenance mode?  Wouldn't they have been cancelled just like
> the ordinary recurring monitor actions?

Good point. Perhaps they wouldn't.

>=20
> I guess adding them would run a recurring monitor operation for every
> resource on every node, only with different expectations, right?
>=20
>> or you can run crm_resource --cleanup after you've moved resources =
around.
>=20
> I actually ran some crm resource cleanups for a couple of resources, =
and
> those really were not started on exiting maintenance mode.
>=20
>>>> Maintenance mode just prevents the cluster from doing anything =
about it.
>>>=20
>>> Fine.  So I should have restarted Pacemaker on each node before =
leaving
>>> maintenance mode, right?  Or is there a better way?
>>=20
>> See above
>=20
> So crm_resource -r whatever -C is the way, for each resource =
separately.
> Is there no way to do this for all resources at once?

I think you can just drop the -r

>=20
>>> You say in the above thread that resource definitions can be =
changed:
>>> =
http://thread.gmane.org/gmane.linux.highavailability.user/39121/focus=3D39=
437
>>> Let me quote from there (starting with the words of Ulrich Windl):
>>>=20
>>>>>>> I think it's a common misconception that you can modify cluster
>>>>>>> resources while in maintenance mode:
>>>>>>=20
>>>>>> No, you _should_ be able to.  If that's not the case, its a bug.
>>>>>=20
>>>>> So the end of maintenance mode starts with a "re-probe"?
>>>>=20
>>>> No, but it doesn't need to. =20
>>>> The policy engine already knows if the resource definitions changed
>>>> and the recurring monitor ops will find out if any are not running.
>>>=20
>>> My experiences show that you may not *move around* resources while =
in
>>> maintenance mode.
>>=20
>> Correct
>>=20
>>> That would indeed require a cluster-wide re-probe, which does not
>>> seem to happen (unless forced some way).  Probably there was some
>>> misunderstanding in the above discussion, I guess Ulrich meant =
moving
>>> resources when he wrote "modifying cluster resources".  Does this
>>> make sense?
>>=20
>> No, I've reasonably sure he meant changing their definitions in the =
cib.
>> Or at least thats what I thought he meant at the time.
>=20
> Nobody could blame you for that, because that's what it means.  But =
then
> he inquired about a "re-probe", which fits more the problem of =
changing
> the status of resources, not their definition.  Actually, I was so
> firmly stuck in this mind set, that first I wanted to ask you to
> reconsider, your response felt so much out of place.  That's all about
> history for now...
>=20
> After all this, I suggest to clarify this issue in the fine manual.
> I've read it a couple of times, and still got the wrong impression.

Which specific section do you suggest?

--Apple-Mail=_2E1D6BE2-4689-42AF-9228-6313297AE65F
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment;
	filename=signature.asc
Content-Type: application/pgp-signature;
	name=signature.asc
Content-Description: Message signed with OpenPGP using GPGMail

-----BEGIN PGP SIGNATURE-----
Comment: GPGTools - http://gpgtools.org

iQIcBAEBCgAGBQJT/mJVAAoJEBTzwpg4iwmN4akP/3huTSI0p73mw7DV71emNoeS
CnFgkDO56Oa+WJEkYAE5vPLyzOUaRm5+O4DL3qhH5Ytvc7LZPvQTgGOULEp+uQNM
OW36UqVviDJCAK+5DBI9CBcyX9ZuaOJQmWpFNMr8/Bv/axRmgUkweIexPLczJxtX
vQyVdVuBZVXT3FBVuF4AE40Q0gBM8mheWhSVg+nAPrxkPopy4YQQaVOQ1mnaoJn9
hOE7ddo0arwZD9Abr/Owh/mqndSSACvM6RfSjf95XLmWmHSBnnC4yBqZGHq6UFas
IyL+iLPN/MZ69DhgJvqUkxBz/VDt2/cE8mrOq0R9YrnHhIzJ0p7OJOgZQlO8d8vg
Km7uZYvDGWIZh/z6v05dhAobnIK+XK5oRwdQHFIckQG6EwQ3mr8R0Uit8UPK4IZq
vA3x0x7LO1/1jbIs0OjQMQzEfIHU78mZ/buhMjao7m3vw/noL83C2DyB6NCBQMkX
b6Bqm3jztXmYE9l+6jdG/u9eeRlz6WLaYbBjaysr1eARujE4JzRRDlbq31LGoNJr
rpNfhp2GgA9ZQ+nB16WyJW47eLqOKW9MDyemSXkgOuYe50jvkgGYFuSR/dC06316
uwgbkWb60YDCc7aPySBxnW+PnnLuv9J6X1KIxxVSXKy/Eldq1B/v3efAP+pWI7Qy
kQvD2RGE13Z2z9shSzCb
=OotU
-----END PGP SIGNATURE-----

--Apple-Mail=_2E1D6BE2-4689-42AF-9228-6313297AE65F--

From andrew@beekhof.net Wed Aug 27 18:58:27 2014
Received: from int-mx09.intmail.prod.int.phx2.redhat.com
	(int-mx09.intmail.prod.int.phx2.redhat.com [10.5.11.22])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7RMwRoY012628 for <linux-cluster@listman.util.phx.redhat.com>;
	Wed, 27 Aug 2014 18:58:27 -0400
Received: from mx1.redhat.com (ext-mx12.extmail.prod.ext.phx2.redhat.com
	[10.5.110.17])
	by int-mx09.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7RMwRu2025380
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 18:58:27 -0400
Received: from out2-smtp.messagingengine.com (out2-smtp.messagingengine.com
	[66.111.4.26])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7RMwPKX032709
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256
	verify=NO)
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 18:58:25 -0400
Received: from compute1.internal (compute1.nyi.internal [10.202.2.41])
	by gateway2.nyi.internal (Postfix) with ESMTP id 543E8202F0
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 18:58:25 -0400 (EDT)
Received: from frontend1 ([10.202.2.160])
	by compute1.internal (MEProxy); Wed, 27 Aug 2014 18:58:25 -0400
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=beekhof.net; h=
	content-type:subject:mime-version:from:in-reply-to:date
	:message-id:references:to; s=mesmtp; bh=/5fDtFciYGLWEldKgyKQkhU0
	Z5o=; b=kMmkyxqdF5OoRVdmb/pmtTPCDGE9PbPHpj4GvTEEuXo+AuahgWP2VcLE
	y16PKARDgf6ptpuxjl5H+8Fy24DoFyN21iYD3cGlPPf75AnPMrXa9LcmWRmPK/jD
	fhMuaxICHHfsoznzXEiW3vIJMiQj5hKUzFoQLV1+C0xlXJNDlF8=
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=
	messagingengine.com; h=content-type:subject:mime-version:from
	:in-reply-to:date:message-id:references:to; s=smtpout; bh=/5fDtF
	ciYGLWEldKgyKQkhU0Z5o=; b=hR0TWlwhGr1hqrc0QOVAFISFRm3CCZB55f86fF
	dvQcmY5R1zvc5pgY3l0EF5lKixn6j6jiZsdLAjyAjF3+mO2BA1AwCzWP3J4z+nUp
	OU0Psv8EdT0c9T7w1tuRocyAGchoAvCcsYSQIKihEttjItr/bEiezo8uG5ath/CK
	LzYZY=
X-Sasl-enc: VceHcSQJ9Q/D0k2+NaZq698Q1vQSKv1tEn5xI3C2qaND 1409180304
Received: from [172.16.1.5] (unknown [120.147.36.73])
	by mail.messagingengine.com (Postfix) with ESMTPA id 62986C00915
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 18:58:23 -0400 (EDT)
Content-Type: multipart/signed;
	boundary="Apple-Mail=_C52C0F03-C750-46CF-BC8E-FEFD5875A82C";
	protocol="application/pgp-signature"; micalg=pgp-sha512
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
From: Andrew Beekhof <andrew@beekhof.net>
In-Reply-To: <877g1t7odb.fsf@lant.ki.iif.hu>
Date: Thu, 28 Aug 2014 08:58:23 +1000
X-Mao-Original-Outgoing-Id: 430873102.228596-8bbe89e4ef4c2b166e448d2fe87f661a
Message-Id: <9D0DD413-AB20-4F25-ADF5-02D8471EAA18@beekhof.net>
References: <871ts39e5c.fsf@lant.ki.iif.hu>
	<EBFD09CA-65DD-4759-B7F3-B73039FC6B3F@beekhof.net>
	<877g1t7odb.fsf@lant.ki.iif.hu>
To: linux clustering <linux-cluster@redhat.com>
X-RedHat-Spam-Score: -3.1  (BAYES_00, DCC_REPUT_00_12, DKIM_SIGNED, DKIM_VALID,
	DKIM_VALID_AU, RCVD_IN_DNSWL_LOW)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.22
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.17
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] locating a starting resource
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Wed, 27 Aug 2014 22:58:27 -0000


--Apple-Mail=_C52C0F03-C750-46CF-BC8E-FEFD5875A82C
Content-Transfer-Encoding: 7bit
Content-Type: text/plain;
	charset=us-ascii


On 28 Aug 2014, at 4:56 am, Ferenc Wagner <wferi@niif.hu> wrote:

> Andrew Beekhof <andrew@beekhof.net> writes:
> 
>> On 27 Aug 2014, at 6:42 am, Ferenc Wagner <wferi@niif.hu> wrote:
>> 
>>> crm_resource --locate finds the hosting node of a running (successfully
>>> started) resource just fine.  Is there a way to similarly find out the
>>> location of a resource *being* started, ie. whose resource agent is
>>> already running the start action, but that action is not finished yet?
>> 
>> You need to set record-pending=true in the op_defaults section.
>> For some reason this is not yet documented :-/
>> 
>> With this in place, crm_resource will find the correct location
> 
> I set it in a single start operation, and it works as advertised,
> thanks!  At first I was suprised to see "Started" in the crm status
> output while the resource was only starting, but the added order
> constraint worked as expected, ie. the dependent resource started only
> after the start action finished successfully.  This begs a bonus
> question: how do I tell apart starting resources with record-pendig=true
> and started resources?

I'm reasonably sure we don't expose that via crm_resource.
Seems like a reasonable thing to do though.

crm_mon /might/ show pending though.


>  crm_resource --locate does not help either.
> -- 
> Thanks,
> Feri.
> 
> -- 
> Linux-cluster mailing list
> Linux-cluster@redhat.com
> https://www.redhat.com/mailman/listinfo/linux-cluster


--Apple-Mail=_C52C0F03-C750-46CF-BC8E-FEFD5875A82C
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment;
	filename=signature.asc
Content-Type: application/pgp-signature;
	name=signature.asc
Content-Description: Message signed with OpenPGP using GPGMail

-----BEGIN PGP SIGNATURE-----
Comment: GPGTools - http://gpgtools.org

iQIcBAEBCgAGBQJT/mKOAAoJEBTzwpg4iwmNz14QAM3NcIrsyPW4B55FSwmz2U77
D6MCZSlz+QS1zc4IN7cw45khjqy9Vufgd2cwh9rSEKqsjvAPO1dNxtR2ENEH7/HB
/chM1IHkZxhPwnPtWjWXi4xIKlaaHovuNj16C77OGPTHd+78D/Q/ZgNSCRWquZOT
YET4TIeKKBIn8IXspyhcJ5GJpD9xqUfglXv0Lh/zwNISdJAcmJau6gw+CtriAt4a
wzeBgyRHxLbcT+MR9jkR/+UWFK84j5C0aNi8LtvAuzg/Sk0LrhbIlv//Ga/z++2f
ryaUAFWnc/zkNNkq5MUXRYQ8Xhky8s3zAEs8PUCWaOc4GNQfkvbyuwdE3j3oM6Xw
saLQj5CGB253u8lXgSb5gB9AONud5yCPeyG4sKnbYnrvOEpAJWLYxoPrA5zpkZM9
NP1lXX2esR+AN3JNYkFhQcyHVBYUTsfbeQdLY+m+VA/8Jj5Qz6tEPtnOXC+dg71+
CJB62BbUj8K8ImIwirkuJ0rEUqMr23zKvXaXwZFFwhOIupJdHXCkIST7hK8oBGaL
x+ujCLxWze1LBkkiynIIIRQH1yA4sqVPOnHY4SbrJyZYasjKoPsoZhdAMPhHJSS2
r6f+ULcuQGBzGeFu0BAdHIpbwz2NYZoNy2OZfsJvMKIdZFEHEK7OvIlz1cVBmoRU
6c2+/NOmttGmNTvhS1mc
=PONX
-----END PGP SIGNATURE-----

--Apple-Mail=_C52C0F03-C750-46CF-BC8E-FEFD5875A82C--

From neale@sinenomine.net Thu Aug 28 15:11:27 2014
Received: from int-mx13.intmail.prod.int.phx2.redhat.com
	(int-mx13.intmail.prod.int.phx2.redhat.com [10.5.11.26])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7SJBRXP025600 for <linux-cluster@listman.util.phx.redhat.com>;
	Thu, 28 Aug 2014 15:11:27 -0400
Received: from mx1.redhat.com (ext-mx11.extmail.prod.ext.phx2.redhat.com
	[10.5.110.16])
	by int-mx13.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7SJBQWY027796
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 15:11:26 -0400
Received: from smtp81.ord1c.emailsrvr.com (smtp81.ord1c.emailsrvr.com
	[108.166.43.81])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7SJBOoW025958
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=NO)
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 15:11:25 -0400
Received: from smtp11.relay.ord1c.emailsrvr.com (localhost.localdomain
	[127.0.0.1])
	by smtp11.relay.ord1c.emailsrvr.com (SMTP Server) with ESMTP id
	CAC70180532
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 15:11:24 -0400 (EDT)
X-SMTPDoctor-Processed: csmtpprox 2.7.1
Received: from localhost (localhost.localdomain [127.0.0.1])
	by smtp11.relay.ord1c.emailsrvr.com (SMTP Server) with ESMTP id
	C618A18052E
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 15:11:24 -0400 (EDT)
X-Virus-Scanned: OK
Received: from smtp192.mex05.mlsrvr.com (unknown [184.106.31.85])
	by smtp11.relay.ord1c.emailsrvr.com (SMTP Server) with ESMTPS id
	AF3DD180533
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 15:11:24 -0400 (EDT)
Received: from ORD2MBX02F.mex05.mlsrvr.com ([fe80::92e2:baff:fe11:e744]) by
	ORD2HUB33.mex05.mlsrvr.com ([::1]) with mapi id 14.03.0169.001;
	Thu, 28 Aug 2014 14:11:24 -0500
From: Neale Ferguson <neale@sinenomine.net>
To: linux clustering <linux-cluster@redhat.com>
Thread-Topic: Delaying fencing during shutdown
Thread-Index: AQHPwvPcVqNV8nY850K87bEJVRucgQ==
Date: Thu, 28 Aug 2014 19:11:24 +0000
Message-ID: <0E22A6F6-977A-4E58-A0ED-9D596D6B1A20@sinenomine.net>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
x-originating-ip: [96.247.193.74]
Content-Type: text/plain; charset="us-ascii"
Content-ID: <389D3249065C9D4FA01F724CB3611D53@mex05.mlsrvr.com>
MIME-Version: 1.0
X-RedHat-Spam-Score: -2.311  (BAYES_00, DCC_REPUT_00_12, RCVD_IN_DNSWL_NONE,
	SPF_PASS)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.26
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.16
Content-Transfer-Encoding: 8bit
X-MIME-Autoconverted: from quoted-printable to 8bit by
	lists01.pubmisc.prod.ext.phx2.redhat.com id s7SJBRXP025600
X-loop: linux-cluster@redhat.com
Subject: [Linux-cluster] Delaying fencing during shutdown
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Thu, 28 Aug 2014 19:11:27 -0000

Hi,
 In a two node cluster I shutdown one of the nodes and the other node notices the shutdown but on rare occasions that node will then fence the node that is shutting down. I assume Is this a situation where setting post_fail_delay would be useful or setting the totem timeout to something higher than its default.

Neale

From wferi@niif.hu Thu Aug 28 19:00:35 2014
Received: from int-mx14.intmail.prod.int.phx2.redhat.com
	(int-mx14.intmail.prod.int.phx2.redhat.com [10.5.11.27])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7SN0ZX1030083 for <linux-cluster@listman.util.phx.redhat.com>;
	Thu, 28 Aug 2014 19:00:35 -0400
Received: from mx1.redhat.com (ext-mx15.extmail.prod.ext.phx2.redhat.com
	[10.5.110.20])
	by int-mx14.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7SN0ZX8016714
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 19:00:35 -0400
Received: from listserv2.niif.hu (listserv2.niif.hu [193.225.14.155])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7SN0VKr018933
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES128-SHA bits=128 verify=NO)
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 19:00:33 -0400
Received: from business-188-142-225-206.business.broadband.hu
	([188.142.225.206] helo=lant.ki.iif.hu)
	by listserv2.niif.hu with esmtpsa (TLS1.2:DHE_RSA_AES_128_CBC_SHA1:128)
	(Exim 4.80) (envelope-from <wferi@niif.hu>) id 1XN8gA-0001gV-E0
	for linux-cluster@redhat.com; Fri, 29 Aug 2014 01:00:30 +0200
Received: from wferi by lant.ki.iif.hu with local (Exim 4.80)
	(envelope-from <wferi@lant.ki.iif.hu>) id 1XN8g4-000679-N0
	for linux-cluster@redhat.com; Fri, 29 Aug 2014 01:00:24 +0200
From: Ferenc Wagner <wferi@niif.hu>
To: linux clustering <linux-cluster@redhat.com>
References: <871ts39e5c.fsf@lant.ki.iif.hu>
	<EBFD09CA-65DD-4759-B7F3-B73039FC6B3F@beekhof.net>
	<877g1t7odb.fsf@lant.ki.iif.hu>
	<9D0DD413-AB20-4F25-ADF5-02D8471EAA18@beekhof.net>
Date: Fri, 29 Aug 2014 01:00:24 +0200
In-Reply-To: <9D0DD413-AB20-4F25-ADF5-02D8471EAA18@beekhof.net> (Andrew
	Beekhof's message of "Thu, 28 Aug 2014 08:58:23 +1000")
Message-ID: <87sikgb4on.fsf@lant.ki.iif.hu>
User-Agent: Gnus/5.13 (Gnus v5.13) Emacs/23.4 (gnu/linux)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
X-RedHat-Spam-Score: -4.199  (BAYES_00,RCVD_IN_DNSWL_MED,RP_MATCHES_RCVD)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.27
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.20
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] locating a starting resource
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Thu, 28 Aug 2014 23:00:35 -0000

Andrew Beekhof <andrew@beekhof.net> writes:

> On 28 Aug 2014, at 4:56 am, Ferenc Wagner <wferi@niif.hu> wrote:
>
>> Andrew Beekhof <andrew@beekhof.net> writes:
>> 
>>> On 27 Aug 2014, at 6:42 am, Ferenc Wagner <wferi@niif.hu> wrote:
>>> 
>>>> crm_resource --locate finds the hosting node of a running (successfully
>>>> started) resource just fine.  Is there a way to similarly find out the
>>>> location of a resource *being* started, ie. whose resource agent is
>>>> already running the start action, but that action is not finished yet?
>>> 
>>> You need to set record-pending=true in the op_defaults section.
>>> For some reason this is not yet documented :-/
>>> 
>>> With this in place, crm_resource will find the correct location
>> 
>> I set it in a single start operation, and it works as advertised,
>> thanks!  At first I was suprised to see "Started" in the crm status
>> output while the resource was only starting, but the added order
>> constraint worked as expected, ie. the dependent resource started only
>> after the start action finished successfully.  This begs a bonus
>> question: how do I tell apart starting resources with record-pendig=true
>> and started resources?
>
> I'm reasonably sure we don't expose that via crm_resource.
> Seems like a reasonable thing to do though.
>
> crm_mon /might/ show pending though.

Version 1.1.7 does not.  Looks like call-id="-1" signs the pending
operations of an <lrm_resource>, so pulling this info out of the CIB
is not too complicated.
-- 
Regards,
Feri.

From wferi@niif.hu Thu Aug 28 20:55:08 2014
Received: from int-mx14.intmail.prod.int.phx2.redhat.com
	(int-mx14.intmail.prod.int.phx2.redhat.com [10.5.11.27])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7T0t7vm007876 for <linux-cluster@listman.util.phx.redhat.com>;
	Thu, 28 Aug 2014 20:55:07 -0400
Received: from mx1.redhat.com (ext-mx14.extmail.prod.ext.phx2.redhat.com
	[10.5.110.19])
	by int-mx14.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7T0t7AN024149
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 20:55:07 -0400
Received: from listserv2.niif.hu (listserv2.niif.hu [193.225.14.155])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7T0t4f3016282
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES128-SHA bits=128 verify=NO)
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 20:55:05 -0400
Received: from business-188-142-225-206.business.broadband.hu
	([188.142.225.206] helo=lant.ki.iif.hu)
	by listserv2.niif.hu with esmtpsa (TLS1.2:DHE_RSA_AES_128_CBC_SHA1:128)
	(Exim 4.80) (envelope-from <wferi@niif.hu>) id 1XNAT1-0006MW-Ak
	for linux-cluster@redhat.com; Fri, 29 Aug 2014 02:55:03 +0200
Received: from wferi by lant.ki.iif.hu with local (Exim 4.80)
	(envelope-from <wferi@lant.ki.iif.hu>) id 1XNASv-0008OW-Gv
	for linux-cluster@redhat.com; Fri, 29 Aug 2014 02:54:57 +0200
From: Ferenc Wagner <wferi@niif.hu>
To: linux clustering <linux-cluster@redhat.com>
References: <8761hlickn.fsf@lant.ki.iif.hu>
	<67506B71-8594-4C16-82C1-F94779F59826@beekhof.net>
	<87iolf9mkc.fsf@lant.ki.iif.hu>
	<F1686CC9-7920-4B72-981D-D6057EC2B754@beekhof.net>
	<87iold7tba.fsf@lant.ki.iif.hu>
	<749665C4-F970-4C43-9228-BCFD2EE1B442@beekhof.net>
Date: Fri, 29 Aug 2014 02:54:57 +0200
In-Reply-To: <749665C4-F970-4C43-9228-BCFD2EE1B442@beekhof.net> (Andrew
	Beekhof's message of "Thu, 28 Aug 2014 08:57:26 +1000")
Message-ID: <87oav4azdq.fsf@lant.ki.iif.hu>
User-Agent: Gnus/5.13 (Gnus v5.13) Emacs/23.4 (gnu/linux)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
X-RedHat-Spam-Score: -4.199  (BAYES_00,RCVD_IN_DNSWL_MED,RP_MATCHES_RCVD)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.27
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.19
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] on exiting maintenance mode
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Fri, 29 Aug 2014 00:55:08 -0000

Andrew Beekhof <andrew@beekhof.net> writes:

> On 28 Aug 2014, at 3:09 am, Ferenc Wagner <wferi@niif.hu> wrote:
>
>> So crm_resource -r whatever -C is the way, for each resource separately.
>> Is there no way to do this for all resources at once?
>
> I think you can just drop the -r

Unfortunately, that does not work under version 1.1.7:

$ sudo crm_resource -C
Error performing operation: The object/attribute does not exist

>> Andrew Beekhof <andrew@beekhof.net> writes:
>> 
>>> On 27 Aug 2014, at 3:40 am, Ferenc Wagner <wferi@niif.hu> wrote:
>>> 
>>>> My experiences show that you may not *move around* resources while in
>>>> maintenance mode.
>>> 
>>> Correct
>>> 
>>>> That would indeed require a cluster-wide re-probe, which does not
>>>> seem to happen (unless forced some way).
>> 
>> After all this, I suggest to clarify this issue in the fine manual.
>> I've read it a couple of times, and still got the wrong impression.
>
> Which specific section do you suggest?

5.7.1. Monitoring Resources for Failure

Some points worth adding/emphasizing would be:
1. documentation of the role property (role=Master is mentioned later,
   but role=Stopped never)
2. In maintenance mode, monitor operations don't run
3. If management of a resource is switched off, its role=Started monitor
   operation continues running until failure, then the role=Stopped
   kicks in (I'm guessing here; also, what about the other nodes?)
4. When management is enabled again, no re-probe happens, the cluster
   expects the last state and location to be still valid
5. so don't even move unmanaged resources
6. unless you started a resource somewhere before starting the cluster
   on that node, or you cleaned up the resource
7. same is true for maintenance mode, but for all resources.

I have to agree that most of this is evident once you know it.
Unfortunately, it's also easy to get wrong while learning the ropes.
For example, hastexo has some good information online:
http://www.hastexo.com/resources/hints-and-kinks/maintenance-active-pacemaker-clusters
But from the sentence "in maintenance mode, you can stop or restart
cluster resources at will" I still miss the constraint of not moving the
resource between the nodes.  Also, setting enabled="false" works funny,
it did not get rid of the monitor operation before I set the resource to
managed, and deleting the setting or changing it to true did bring it
back.  I had to restart the resource to have monitor ops again.  Why?
-- 
Thanks,
Feri.

From wferi@niif.hu Thu Aug 28 20:57:59 2014
Received: from int-mx14.intmail.prod.int.phx2.redhat.com
	(int-mx14.intmail.prod.int.phx2.redhat.com [10.5.11.27])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7T0vxer007626 for <linux-cluster@listman.util.phx.redhat.com>;
	Thu, 28 Aug 2014 20:57:59 -0400
Received: from mx1.redhat.com (ext-mx14.extmail.prod.ext.phx2.redhat.com
	[10.5.110.19])
	by int-mx14.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7T0vxxZ025662
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 20:57:59 -0400
Received: from listserv2.niif.hu (listserv2.niif.hu [193.225.14.155])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7T0vuH2017705
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES128-SHA bits=128 verify=NO)
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 20:57:58 -0400
Received: from business-188-142-225-206.business.broadband.hu
	([188.142.225.206] helo=lant.ki.iif.hu)
	by listserv2.niif.hu with esmtpsa (TLS1.2:DHE_RSA_AES_128_CBC_SHA1:128)
	(Exim 4.80) (envelope-from <wferi@niif.hu>) id 1XNAVo-0006UT-HQ
	for linux-cluster@redhat.com; Fri, 29 Aug 2014 02:57:56 +0200
Received: from wferi by lant.ki.iif.hu with local (Exim 4.80)
	(envelope-from <wferi@lant.ki.iif.hu>) id 1XNAVi-0008S0-OZ
	for linux-cluster@redhat.com; Fri, 29 Aug 2014 02:57:50 +0200
From: Ferenc Wagner <wferi@niif.hu>
To: linux clustering <linux-cluster@redhat.com>
References: <871ts39e5c.fsf@lant.ki.iif.hu>
	<EBFD09CA-65DD-4759-B7F3-B73039FC6B3F@beekhof.net>
	<877g1t7odb.fsf@lant.ki.iif.hu>
	<9D0DD413-AB20-4F25-ADF5-02D8471EAA18@beekhof.net>
Date: Fri, 29 Aug 2014 02:57:50 +0200
In-Reply-To: <9D0DD413-AB20-4F25-ADF5-02D8471EAA18@beekhof.net> (Andrew
	Beekhof's message of "Thu, 28 Aug 2014 08:58:23 +1000")
Message-ID: <87ha0waz8x.fsf@lant.ki.iif.hu>
User-Agent: Gnus/5.13 (Gnus v5.13) Emacs/23.4 (gnu/linux)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
X-RedHat-Spam-Score: -4.199  (BAYES_00,RCVD_IN_DNSWL_MED,RP_MATCHES_RCVD)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.27
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.19
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] locating a starting resource
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Fri, 29 Aug 2014 00:57:59 -0000

Andrew Beekhof <andrew@beekhof.net> writes:

> On 28 Aug 2014, at 4:56 am, Ferenc Wagner <wferi@niif.hu> wrote:
>
>> Andrew Beekhof <andrew@beekhof.net> writes:
>> 
>>> On 27 Aug 2014, at 6:42 am, Ferenc Wagner <wferi@niif.hu> wrote:
>>> 
>>>> crm_resource --locate finds the hosting node of a running (successfully
>>>> started) resource just fine.  Is there a way to similarly find out the
>>>> location of a resource *being* started, ie. whose resource agent is
>>>> already running the start action, but that action is not finished yet?
>>> 
>>> You need to set record-pending=true in the op_defaults section.
>>> For some reason this is not yet documented :-/
>>> 
>>> With this in place, crm_resource will find the correct location
>> 
>> I set it in a single start operation, and it works as advertised,
>> thanks!  At first I was suprised to see "Started" in the crm status
>> output while the resource was only starting, but the added order
>> constraint worked as expected, ie. the dependent resource started only
>> after the start action finished successfully.  This begs a bonus
>> question: how do I tell apart starting resources with record-pendig=true
>> and started resources?
>
> I'm reasonably sure we don't expose that via crm_resource.
> Seems like a reasonable thing to do though.

crm_resource -O outputs lines like this:
[...] Started : vm-elm_start_0 (node=lant, call=-1, rc=14): pending
which seems good enough for now.
-- 
Thanks,
Feri.

From andrew@beekhof.net Thu Aug 28 22:31:43 2014
Received: from int-mx14.intmail.prod.int.phx2.redhat.com
	(int-mx14.intmail.prod.int.phx2.redhat.com [10.5.11.27])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7T2Vhsp031861 for <linux-cluster@listman.util.phx.redhat.com>;
	Thu, 28 Aug 2014 22:31:43 -0400
Received: from mx1.redhat.com (ext-mx12.extmail.prod.ext.phx2.redhat.com
	[10.5.110.17])
	by int-mx14.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7T2VhiB030865
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 22:31:43 -0400
Received: from out1-smtp.messagingengine.com (out1-smtp.messagingengine.com
	[66.111.4.25])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7T2VfZD019942
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256
	verify=NO)
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 22:31:42 -0400
Received: from compute2.internal (compute2.nyi.internal [10.202.2.42])
	by gateway2.nyi.internal (Postfix) with ESMTP id 0B26820FDC
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 22:31:41 -0400 (EDT)
Received: from frontend2 ([10.202.2.161])
	by compute2.internal (MEProxy); Thu, 28 Aug 2014 22:31:41 -0400
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=beekhof.net; h=
	content-type:subject:mime-version:from:in-reply-to:date
	:message-id:references:to; s=mesmtp; bh=AltXKMWUIJOpAeOhZ11rHg3Y
	rK4=; b=A7RB+PXYlaoweuDUvZjEkvcUEsOe0a/dsg7bi1uET+YKiHeSnQYfldrL
	iHcNSDpH94w2vjEozhSfovRbSQqSOwrzU0NRsEyrlisog+OqpHCVT0FqiUOZaTiC
	J9A7En5mtty+BaFG5lSJaoafEMXESpU+UngLy0rF7ZxG5Piidcg=
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=
	messagingengine.com; h=content-type:subject:mime-version:from
	:in-reply-to:date:message-id:references:to; s=smtpout; bh=AltXKM
	WUIJOpAeOhZ11rHg3YrK4=; b=rB8fxgCmWaBjTQA9vl2mW573MDM+NT6T6BzpET
	iIsIK1wG41LK2lcOMh89ZEf9fqvY8a4K0u/h/oQoERKsTtx6D/dZVxpK9zIQ6nMH
	a4huO8JpwichRTU8sVgmzcogve06qS9jnQN4S2T7MXAScgb1BtKYhtdthaDhkPeP
	JVlz4=
X-Sasl-enc: 5BoLpHCkbSKYPUp0//TnMEXMT45tTVK6KTdZc4zH3aDh 1409279500
Received: from [172.16.1.5] (unknown [120.147.36.73])
	by mail.messagingengine.com (Postfix) with ESMTPA id E17E76800AE
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 22:31:39 -0400 (EDT)
Content-Type: multipart/signed;
	boundary="Apple-Mail=_78114DBC-6CD7-4DA5-99DA-2BAD0B09CD04";
	protocol="application/pgp-signature"; micalg=pgp-sha512
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
From: Andrew Beekhof <andrew@beekhof.net>
In-Reply-To: <87ha0waz8x.fsf@lant.ki.iif.hu>
Date: Fri, 29 Aug 2014 12:31:36 +1000
X-Mao-Original-Outgoing-Id: 430972295.366025-ba6a479a86622422c04446c04181ddc7
Message-Id: <77CDE52A-401F-4851-ABFB-3A643F9913CD@beekhof.net>
References: <871ts39e5c.fsf@lant.ki.iif.hu>
	<EBFD09CA-65DD-4759-B7F3-B73039FC6B3F@beekhof.net>
	<877g1t7odb.fsf@lant.ki.iif.hu>
	<9D0DD413-AB20-4F25-ADF5-02D8471EAA18@beekhof.net>
	<87ha0waz8x.fsf@lant.ki.iif.hu>
To: linux clustering <linux-cluster@redhat.com>
X-RedHat-Spam-Score: -3.1  (BAYES_00, DCC_REPUT_00_12, DKIM_SIGNED, DKIM_VALID,
	DKIM_VALID_AU, RCVD_IN_DNSWL_LOW)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.27
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.17
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] locating a starting resource
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Fri, 29 Aug 2014 02:31:45 -0000


--Apple-Mail=_78114DBC-6CD7-4DA5-99DA-2BAD0B09CD04
Content-Transfer-Encoding: 7bit
Content-Type: text/plain;
	charset=us-ascii


On 29 Aug 2014, at 10:57 am, Ferenc Wagner <wferi@niif.hu> wrote:

> Andrew Beekhof <andrew@beekhof.net> writes:
> 
>> On 28 Aug 2014, at 4:56 am, Ferenc Wagner <wferi@niif.hu> wrote:
>> 
>>> Andrew Beekhof <andrew@beekhof.net> writes:
>>> 
>>>> On 27 Aug 2014, at 6:42 am, Ferenc Wagner <wferi@niif.hu> wrote:
>>>> 
>>>>> crm_resource --locate finds the hosting node of a running (successfully
>>>>> started) resource just fine.  Is there a way to similarly find out the
>>>>> location of a resource *being* started, ie. whose resource agent is
>>>>> already running the start action, but that action is not finished yet?
>>>> 
>>>> You need to set record-pending=true in the op_defaults section.
>>>> For some reason this is not yet documented :-/
>>>> 
>>>> With this in place, crm_resource will find the correct location
>>> 
>>> I set it in a single start operation, and it works as advertised,
>>> thanks!  At first I was suprised to see "Started" in the crm status
>>> output while the resource was only starting, but the added order
>>> constraint worked as expected, ie. the dependent resource started only
>>> after the start action finished successfully.  This begs a bonus
>>> question: how do I tell apart starting resources with record-pendig=true
>>> and started resources?
>> 
>> I'm reasonably sure we don't expose that via crm_resource.
>> Seems like a reasonable thing to do though.
> 
> crm_resource -O outputs lines like this:
> [...] Started : vm-elm_start_0 (node=lant, call=-1, rc=14): pending
> which seems good enough for now.
> -- 

More recent versions also have:

       -j, --pending
              Display pending state if 'record-pending' is enabled



--Apple-Mail=_78114DBC-6CD7-4DA5-99DA-2BAD0B09CD04
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment;
	filename=signature.asc
Content-Type: application/pgp-signature;
	name=signature.asc
Content-Description: Message signed with OpenPGP using GPGMail

-----BEGIN PGP SIGNATURE-----
Comment: GPGTools - http://gpgtools.org

iQIcBAEBCgAGBQJT/+YHAAoJEBTzwpg4iwmNZQEP/iI2ZJAU5b+DASpujBIJtVFf
6PRsYYLOtzFHEDckefeBS09f1c1Mcy7+JgE8YK+hZGmdpdnTl3d2dLoKQvEUF27I
a+6VqbnJLsMLF95BY7TI6Z9H5zdN7qq/H7bdZnROv3uYPjX4/6O7hHmbfgLYIi8C
/WOx7OPRNf6Zd5U3nYVpwuGHMhSJfcqjvvAOGTbpnJpXeGSO9rZ1yqgAxfpRi4N3
xlOdIu9IrefsZXFUTNCGop50wtew8hILxWRxToTBEd1TcvX9hkOU+bZWY3uaMWgr
j6KV1mU8EJuD+7+/5wZ//67q03OLIt2euHZmCNPeXvADhDzO6R4wKArFntOAqPY7
u6VErMzqNBp3geZFt/PPCHTdaeyqj/FGi3JJzEhDVrT8RUSxDISG+MMkr7iNc6cE
gqTcyZJXz4C+GKmAYqDP53X1Ru99ZoHMRxePB0ON7ksw2CU0Iv7A2OrWbKgpepSC
dCbJQDSRVU/kMkw128IM+q6U7llspQFL0TzTEdQX/kRqg9iLcT7QoVwbu2vYWY3q
9Yz9zQN9ww9HRjmE91uqaMNriWJNPLI1E4kWS6pY1BrnZKrivQxV98CqdrmMAWyM
Gfi7wT/NjNPUeY0sMAWNjq6WGnhQQ+HDESqBSfFpricwcZ2aB7iM8C5u4nY9LAhc
vkBldu+vYkMHcPhvpZQa
=kQPo
-----END PGP SIGNATURE-----

--Apple-Mail=_78114DBC-6CD7-4DA5-99DA-2BAD0B09CD04--

From andrew@beekhof.net Thu Aug 28 22:32:57 2014
Received: from int-mx13.intmail.prod.int.phx2.redhat.com
	(int-mx13.intmail.prod.int.phx2.redhat.com [10.5.11.26])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7T2Wv36014169 for <linux-cluster@listman.util.phx.redhat.com>;
	Thu, 28 Aug 2014 22:32:57 -0400
Received: from mx1.redhat.com (ext-mx11.extmail.prod.ext.phx2.redhat.com
	[10.5.110.16])
	by int-mx13.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7T2WvQ5020641
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 22:32:57 -0400
Received: from out1-smtp.messagingengine.com (out1-smtp.messagingengine.com
	[66.111.4.25])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7T2WtqF024534
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256
	verify=NO)
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 22:32:55 -0400
Received: from compute1.internal (compute1.nyi.internal [10.202.2.41])
	by gateway2.nyi.internal (Postfix) with ESMTP id E7BCE20DF0
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 22:32:54 -0400 (EDT)
Received: from frontend1 ([10.202.2.160])
	by compute1.internal (MEProxy); Thu, 28 Aug 2014 22:32:54 -0400
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=beekhof.net; h=
	content-type:subject:mime-version:from:in-reply-to:date
	:message-id:references:to; s=mesmtp; bh=KbSO10OzWnlQYmYC98xUha5R
	UHU=; b=gZBmdOQj048xaS2VBLrZhhTFRIe5YEhYHg0ePyVb8J6qRmWuKwMxUsDb
	Lw/l5WGvMBlNd3mA9gy6OTrrCcBVOu1BxZDz1uS/FBgL+FIT6ylqWsTse9LhInmh
	kFnLMapdN2rLhccpjLdlEvmvtxyGyRU+wLrhIxpPcBHgd1zLna8=
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=
	messagingengine.com; h=content-type:subject:mime-version:from
	:in-reply-to:date:message-id:references:to; s=smtpout; bh=KbSO10
	OzWnlQYmYC98xUha5RUHU=; b=iNPVV2GQJjIoQCQREZCDHW2DRL847nyK4LvOnx
	5AiCN83c4LcE8yjTgyxvKMxINEpFtjMuUv+EKBZskDEgr+k/0y53czXKDVXN6JTU
	iLuO/LjEsir4YvPzO75R0iakR1u+NWpJsA5t75TqR9127pIOnhpWtVgBBs/rut7E
	dm41I=
X-Sasl-enc: QVu0PWMzVIqumM1NvvcbWIbsYTtC92KrHcZD8lw2rznh 1409279574
Received: from [172.16.1.5] (unknown [120.147.36.73])
	by mail.messagingengine.com (Postfix) with ESMTPA id C9880C00915
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 22:32:53 -0400 (EDT)
Content-Type: multipart/signed;
	boundary="Apple-Mail=_FA81BDE8-0B0F-45E1-9630-C6BEEE3A84BB";
	protocol="application/pgp-signature"; micalg=pgp-sha512
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
From: Andrew Beekhof <andrew@beekhof.net>
In-Reply-To: <87oav4azdq.fsf@lant.ki.iif.hu>
Date: Fri, 29 Aug 2014 12:32:50 +1000
X-Mao-Original-Outgoing-Id: 430972370.225842-2dc111f357f7b562b84b389df38c063e
Message-Id: <B9EF793F-8CCC-4AA7-82C9-CFDD64EC9537@beekhof.net>
References: <8761hlickn.fsf@lant.ki.iif.hu>
	<67506B71-8594-4C16-82C1-F94779F59826@beekhof.net>
	<87iolf9mkc.fsf@lant.ki.iif.hu>
	<F1686CC9-7920-4B72-981D-D6057EC2B754@beekhof.net>
	<87iold7tba.fsf@lant.ki.iif.hu>
	<749665C4-F970-4C43-9228-BCFD2EE1B442@beekhof.net>
	<87oav4azdq.fsf@lant.ki.iif.hu>
To: linux clustering <linux-cluster@redhat.com>
X-RedHat-Spam-Score: -3.1  (BAYES_00, DCC_REPUT_00_12, DKIM_SIGNED, DKIM_VALID,
	DKIM_VALID_AU, RCVD_IN_DNSWL_LOW)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.26
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.16
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] on exiting maintenance mode
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Fri, 29 Aug 2014 02:32:57 -0000


--Apple-Mail=_FA81BDE8-0B0F-45E1-9630-C6BEEE3A84BB
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=us-ascii


On 29 Aug 2014, at 10:54 am, Ferenc Wagner <wferi@niif.hu> wrote:

> Andrew Beekhof <andrew@beekhof.net> writes:
>=20
>> On 28 Aug 2014, at 3:09 am, Ferenc Wagner <wferi@niif.hu> wrote:
>>=20
>>> So crm_resource -r whatever -C is the way, for each resource =
separately.
>>> Is there no way to do this for all resources at once?
>>=20
>> I think you can just drop the -r
>=20
> Unfortunately, that does not work under version 1.1.7:

You know what I'm going to say here right?

>=20
> $ sudo crm_resource -C
> Error performing operation: The object/attribute does not exist
>=20
>>> Andrew Beekhof <andrew@beekhof.net> writes:
>>>=20
>>>> On 27 Aug 2014, at 3:40 am, Ferenc Wagner <wferi@niif.hu> wrote:
>>>>=20
>>>>> My experiences show that you may not *move around* resources while =
in
>>>>> maintenance mode.
>>>>=20
>>>> Correct
>>>>=20
>>>>> That would indeed require a cluster-wide re-probe, which does not
>>>>> seem to happen (unless forced some way).
>>>=20
>>> After all this, I suggest to clarify this issue in the fine manual.
>>> I've read it a couple of times, and still got the wrong impression.
>>=20
>> Which specific section do you suggest?
>=20
> 5.7.1. Monitoring Resources for Failure

Ok, I'll endeavour to improve that section :)

>=20
> Some points worth adding/emphasizing would be:
> 1. documentation of the role property (role=3DMaster is mentioned =
later,
>   but role=3DStopped never)
> 2. In maintenance mode, monitor operations don't run
> 3. If management of a resource is switched off, its role=3DStarted =
monitor
>   operation continues running until failure, then the role=3DStopped
>   kicks in (I'm guessing here; also, what about the other nodes?)
> 4. When management is enabled again, no re-probe happens, the cluster
>   expects the last state and location to be still valid
> 5. so don't even move unmanaged resources
> 6. unless you started a resource somewhere before starting the cluster
>   on that node, or you cleaned up the resource
> 7. same is true for maintenance mode, but for all resources.
>=20
> I have to agree that most of this is evident once you know it.
> Unfortunately, it's also easy to get wrong while learning the ropes.
> For example, hastexo has some good information online:
> =
http://www.hastexo.com/resources/hints-and-kinks/maintenance-active-pacema=
ker-clusters
> But from the sentence "in maintenance mode, you can stop or restart
> cluster resources at will" I still miss the constraint of not moving =
the
> resource between the nodes.  Also, setting enabled=3D"false" works =
funny,
> it did not get rid of the monitor operation before I set the resource =
to
> managed, and deleting the setting or changing it to true did bring it
> back.  I had to restart the resource to have monitor ops again.  Why?
> --=20
> Thanks,
> Feri.
>=20
> --=20
> Linux-cluster mailing list
> Linux-cluster@redhat.com
> https://www.redhat.com/mailman/listinfo/linux-cluster


--Apple-Mail=_FA81BDE8-0B0F-45E1-9630-C6BEEE3A84BB
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment;
	filename=signature.asc
Content-Type: application/pgp-signature;
	name=signature.asc
Content-Description: Message signed with OpenPGP using GPGMail

-----BEGIN PGP SIGNATURE-----
Comment: GPGTools - http://gpgtools.org

iQIcBAEBCgAGBQJT/+ZSAAoJEBTzwpg4iwmNiCQP/3v7xK9u45dpWN1bwQrsDXPb
IHilG5BVK9+SULkmttJk9ozvDhu67p3NGXwFlAKK3YXh+6ifT7LU2HO30v8nF8wn
YCCl+tWqMZCGQHjtkD3iHHYo8EEi0nWc7qgG3A0Spv91uBwYe6ahr4jwNXjvHf/i
UVqc5LVqjQMmCgGQNdDqL1bKNAFm5BjNuuou5rI3DSeihkB6SzTe666jj5ZxL5Lx
hNBEPwTT5jWETphFGY8GvKUXaqvJpyck1LUwGgjKxH0YJdUDFDmkA7ylvml2Sm99
f+k2T6yTV5/cKYIkLShjhmFlRf8fxONOLKwCtY9WP5oeLxG6RjCLxFBHXCbRUTi1
qIAcakqlkVLq8B8uLU19TriMYBZsBdRnbocIyk6tz3nsA/6QNt2rlXSRLjaplaSK
/y/v9aiobnjIV6QLiP/mFbmOMTd36WH44TFbnR8OzkxACRwCanJS+IgFWpj3SMj1
5AkAPCv4vaLP20Z71ND9mVXCUpYXPqXdoJcTRuy9gn87JnD3/aNS32Gc1aoJgtjO
oAMwhfqP/0+HmJUShaIZNFv8lQLllGvEwSuGhtl/MrYieCtVlHR5acdF9UcFPrpd
HPJOISUrR5xWeNLGwUJVgEbUQ/wfuQexwWhSqIw8Y000BoRXtu3yFpNMBYXsyZsf
VifSW8QS9xkI4MKdcgAL
=eH30
-----END PGP SIGNATURE-----

--Apple-Mail=_FA81BDE8-0B0F-45E1-9630-C6BEEE3A84BB--

From manish631@rediffmail.com Sat Aug 30 10:12:52 2014
Received: from int-mx10.intmail.prod.int.phx2.redhat.com
	(int-mx10.intmail.prod.int.phx2.redhat.com [10.5.11.23])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7UECqZB002468 for <linux-cluster@listman.util.phx.redhat.com>;
	Sat, 30 Aug 2014 10:12:52 -0400
Received: from mx1.redhat.com (ext-mx12.extmail.prod.ext.phx2.redhat.com
	[10.5.110.17])
	by int-mx10.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7UECqBG002701
	for <linux-cluster@redhat.com>; Sat, 30 Aug 2014 10:12:52 -0400
Received: from rediffmail.com (f5mail-224-126.rediffmail.com [114.31.224.126])
	by mx1.redhat.com (8.14.4/8.14.4) with SMTP id s7UECnSM031071
	for <linux-cluster@redhat.com>; Sat, 30 Aug 2014 10:12:50 -0400
Received: (qmail 4357 invoked by uid 510); 30 Aug 2014 14:12:42 -0000
Comment: DomainKeys? See http://antispam.yahoo.com/domainkeys
DomainKey-Signature: a=rsa-sha1; q=dns; c=nofws; s=redf; d=rediffmail.com;
	b=TRMRSvrvfbGzwzIbSWahvnvpyeXsT/PC9+h8IDBsThXuZ8C51wcuukWAltEo4rqHvZo5EVjMIVkYs96nfoi0ssTG13Kh+GC7Q/uR4Kq4tNfNEb9j3xB7/0qWIUx2adq2SBBA30Sh6mmz2fPsoqY6sbGw6lHYfCIEUStRvm13j38=
	; 
x-m-msg: asd54ad564ad7aa6sd5as6d5; a6da7d6asas6dasd77; 5dad65ad5sd;
X-CTCH-Spam: Unknown
X-CTCH-VOD: Unknown
X-CTCH-Flags: : 0
X-CTCH-RefID: str=0001.0A160208.5401DBDC.00AB, ss=1, re=0.000, recu=0.000,
	reip=0.000, cl=1, cld=1, fgs=0
X-REDF-OSEN: manish631@rediffmail.com
Date: 30 Aug 2014 14:12:42 -0000
Message-ID: <20140830141242.4308.qmail@f5mail-224-126.rediffmail.com>
MIME-Version: 1.0
To: <linux-cluster@redhat.com>
Received: from unknown 49.248.116.50 by rediffmail.com via HTTP;
	30 Aug 2014 14:12:41 -0000
Sender: manish631@rediffmail.com
From: "manish vaidya" <manish631@rediffmail.com>
Content-Type: multipart/alternative;
	boundary="=_5ec549d7dafe8e83843cb95689c45dbb"
X-RedHat-Spam-Score: 0.954  (BAYES_50, DKIM_SIGNED, DKIM_VALID, DKIM_VALID_AU,
	FREEMAIL_ENVFROM_END_DIGIT, FREEMAIL_FROM, HTML_IMAGE_ONLY_32,
	HTML_MESSAGE, MSGID_FROM_MTA_HEADER, RCVD_IN_DNSWL_NONE,
	RP_MATCHES_RCVD, SPF_HELO_PASS, SPF_PASS, T_REMOTE_IMAGE,
	UNPARSEABLE_RELAY)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.23
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.17
X-loop: linux-cluster@redhat.com
Subject: [Linux-cluster] =?utf-8?q?Please_help_me_on_cluster_error?=
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Sat, 30 Aug 2014 14:12:52 -0000

--=_5ec549d7dafe8e83843cb95689c45dbb
Content-Transfer-Encoding: 7bit
Content-Type: text/plain; charset="UTF-8"

i created four node cluster in kvm enviorment But i faced error when create new pv such as pvcreate /dev/sdb1
got error , lock from node 2 & lock from node3

also strange cluster logs

Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5e

    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5e
    5f
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5f
    60
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 61
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 63
    64
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 69
    6a
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 78
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 84
    85
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 9a
    9b


Please help me on this issue
--=_5ec549d7dafe8e83843cb95689c45dbb
Content-Transfer-Encoding: quoted-printable
Content-Type: text/html; charset="UTF-8"

i created four node cluster in kvm enviorment But i faced error when create=
 new pv such as pvcreate /dev/sdb1<br />
got error , lock from node 2 & lock from node3<br />
<br />
also strange cluster logs<br />
<br />
Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5e<br />
<br />
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5e<br /=
>
    5f<br />
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5f<br /=
>
    60<br />
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 61<br /=
>
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 63<br /=
>
    64<br />
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 69<br /=
>
    6a<br />
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 78<br /=
>
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 84<br /=
>
    85<br />
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 9a<br /=
>
    9b<br />
<br />
<br />
Please help me on this issue<br><Table border=3D0 Width=3D100% Height=3D57 =
cellspacing=3D0 cellpadding=3D0 style=3D"font-family:Verdana;font-size:11px=
;line-height:15px;"><TR><td><A HREF=3D"http://sigads.rediff.com/RealMedia/a=
ds/click_nx.ads/www.rediffmail.com/signatureline.htm@Middle?" target=3D"_bl=
ank"><IMG SRC=3D"http://sigads.rediff.com/RealMedia/ads/adstream_nx.ads/www=
.rediffmail.com/signatureline.htm@Middle"></A></td></TR></Table><table cell=
padding=3D"0" cellspacing=3D"0"><tbody><tr><td><div style=3D"font-family: A=
rial, Helvetica, sans-serif; font-size:14px">Get your own <span style=3D"pa=
dding-bottom: 0px; background-color: #cc0000; padding-left: 3px; padding-RI=
GHT: 3px; font-family: Arial, Helvetica, sans-serif; color: #ffffff; font-s=
ize: 12px; padding-top: 0px"><b>FREE</b></span> website,  <span style=3D"pa=
dding-bottom: 0px; background-color: #c00; padding-left: 3px; padding-RIGHT=
: 3px; font-family: Arial, Helvetica, sans-serif; color: #ffffff; font-size=
: 12px; padding-top: 0px"><b>FREE</b></span> domain &amp; <span style=3D"pa=
dding-bottom: 0px; background-color: #c00; padding-left: 3px; padding-RIGHT=
: 3px; font-family: Arial, Helvetica, sans-serif; color: #ffffff; font-size=
: 12px; padding-top: 0px"><b>FREE</b></span> mobile app with Company email.=
 &nbsp;</div></td><td><a href=3D"http://track.rediff.com/click?url=3D___htt=
p://businessemail.rediff.com/email-ids-for-companies-with-less-than-50-empl=
oyees?sc_cid=3Dsign-1-10-13___&cmp=3Dhost&lnk=3Dsign-1-10-13&nsrv1=3Dhost" =
style=3D"font-family: Arial, Helvetica, sans-serif; color: #fff; font-size:=
 14px; color:#0000cc" target=3D"_blank"><b>Know More ></b></a><!-- <in-put =
type=3D"button" cl-ass=3D"button" on-click=3D"parent.location=3D&#39;http:/=
/track.rediff.com/click?url=3D___http://businessemail.rediff.com/company-em=
ail-hosting-services?sc_cid=3Dsignature-23-9-13___&amp;cmp=3Dsignature-23-9=
-13&amp;lnk=3Dmypagelogout&amp;nsrv1=3Dhost&#39;" value=3D"Know more &gt;">=
 </input> --></td></tr></tbody></table>
--=_5ec549d7dafe8e83843cb95689c45dbb--

From emi2fast@gmail.com Sat Aug 30 10:53:10 2014
Received: from int-mx11.intmail.prod.int.phx2.redhat.com
	(int-mx11.intmail.prod.int.phx2.redhat.com [10.5.11.24])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7UErAmR002829 for <linux-cluster@listman.util.phx.redhat.com>;
	Sat, 30 Aug 2014 10:53:10 -0400
Received: from mx1.redhat.com (ext-mx15.extmail.prod.ext.phx2.redhat.com
	[10.5.110.20])
	by int-mx11.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7UErAJ2024934
	for <linux-cluster@redhat.com>; Sat, 30 Aug 2014 10:53:10 -0400
Received: from mail-oi0-f54.google.com (mail-oi0-f54.google.com
	[209.85.218.54])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7UEr87I015893
	(version=TLSv1/SSLv3 cipher=RC4-SHA bits=128 verify=FAIL)
	for <linux-cluster@redhat.com>; Sat, 30 Aug 2014 10:53:09 -0400
Received: by mail-oi0-f54.google.com with SMTP id a3so2365842oib.41
	for <linux-cluster@redhat.com>; Sat, 30 Aug 2014 07:53:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=gmail.com; s=20120113;
	h=mime-version:in-reply-to:references:date:message-id:subject:from:to
	:content-type; bh=Hbn9Gj25ryGkB5cvaUGKfJLEZPRRcTCx62wrwUuPXa8=;
	b=dVF7uKz14Ce3x/7Z93L52s7a9dVvISHXVBUDOqGSwhg8P/Ifxtl1nRNxpl1VwiUn02
	cd1txPkNMvVXYjPyWKzaNQVquwKxTgnUmiYUGfTUy7QK0mxtUtqx5m3Zq4BWgqXnOgt+
	EuQcXSpn84aun4w/GQ8m9pVC+R4ctIB+U4sZ9mZ6bBuT1eIXBa2MiqTq32t5SWQOzXqL
	bmlMBquxg6V0LLGlEts0HIjrzfX8Xt158bpk3veNo+46XYuBF6K1Jyn10QkaIHKVJhbs
	/8LLrCp8VOdIrYZAYO8/+qag9zaL/6inKtwihWGjRjJg4F9sx3QKV6TE/tjY+EcfLAMe
	bcZA==
MIME-Version: 1.0
X-Received: by 10.60.37.165 with SMTP id z5mr16829466oej.16.1409410388407;
	Sat, 30 Aug 2014 07:53:08 -0700 (PDT)
Received: by 10.76.113.211 with HTTP; Sat, 30 Aug 2014 07:53:08 -0700 (PDT)
In-Reply-To: <20140830141242.4308.qmail@f5mail-224-126.rediffmail.com>
References: <20140830141242.4308.qmail@f5mail-224-126.rediffmail.com>
Date: Sat, 30 Aug 2014 16:53:08 +0200
Message-ID: <CAE7pJ3AKUvW1ia3MAPWjE1Mrwfu10F=dCt+E48nicSJDjbJhrA@mail.gmail.com>
From: emmanuel segura <emi2fast@gmail.com>
To: linux clustering <linux-cluster@redhat.com>
Content-Type: multipart/alternative; boundary=089e013c65768b3ab50501d9ed2b
X-RedHat-Spam-Score: -1.689  (BAYES_05, DCC_REPUT_00_12, DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU, FREEMAIL_FROM, HTML_MESSAGE,
	RCVD_IN_DNSWL_LOW, SPF_PASS, T_REMOTE_IMAGE)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.24
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.20
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] Please help me on cluster error
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Sat, 30 Aug 2014 14:53:10 -0000

--089e013c65768b3ab50501d9ed2b
Content-Type: text/plain; charset=UTF-8

are you using clvmd? if your answer is = yes, you need to be sure, you pv
is visibile to your cluster nodes


2014-08-30 16:12 GMT+02:00 manish vaidya <manish631@rediffmail.com>:

> i created four node cluster in kvm enviorment But i faced error when
> create new pv such as pvcreate /dev/sdb1
> got error , lock from node 2 & lock from node3
>
> also strange cluster logs
>
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5e
>
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5e
> 5f
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5f
> 60
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 61
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 63
> 64
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 69
> 6a
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 78
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 84
> 85
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 9a
> 9b
>
>
> Please help me on this issue
>
> <http://sigads.rediff.com/RealMedia/ads/click_nx.ads/www.rediffmail.com/signatureline.htm@Middle?>
> Get your own *FREE* website, *FREE* domain & *FREE* mobile app with
> Company email.
> *Know More >*
> <http://track.rediff.com/click?url=___http://businessemail.rediff.com/email-ids-for-companies-with-less-than-50-employees?sc_cid=sign-1-10-13___&cmp=host&lnk=sign-1-10-13&nsrv1=host>
> --
> Linux-cluster mailing list
> Linux-cluster@redhat.com
> https://www.redhat.com/mailman/listinfo/linux-cluster
>



-- 
esta es mi vida e me la vivo hasta que dios quiera

--089e013c65768b3ab50501d9ed2b
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">are you using clvmd? if your answer is =3D yes, you need t=
o be sure, you pv is visibile to your cluster nodes<br></div><div class=3D"=
gmail_extra"><br><br><div class=3D"gmail_quote">2014-08-30 16:12 GMT+02:00 =
manish vaidya <span dir=3D"ltr">&lt;<a href=3D"mailto:manish631@rediffmail.=
com" target=3D"_blank">manish631@rediffmail.com</a>&gt;</span>:<br>
<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex">i created four node cluster in kvm enviormen=
t But i faced error when create new pv such as pvcreate /dev/sdb1<br>
got error , lock from node 2 &amp; lock from node3<br>
<br>
also strange cluster logs<br>
<br>
Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5e<br>
<br>
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5e<br>
    5f<br>
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5f<br>
    60<br>
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 61<br>
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 63<br>
    64<br>
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 69<br>
    6a<br>
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 78<br>
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 84<br>
    85<br>
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 9a<br>
    9b<br>
<br>
<br>
Please help me on this issue<br><table style=3D"font-family:Verdana;font-si=
ze:11px;line-height:15px" border=3D"0" cellpadding=3D"0" cellspacing=3D"0" =
height=3D"57" width=3D"100%"><tbody><tr><td><a href=3D"http://sigads.rediff=
.com/RealMedia/ads/click_nx.ads/www.rediffmail.com/signatureline.htm@Middle=
?" target=3D"_blank"><img src=3D"http://sigads.rediff.com/RealMedia/ads/ads=
tream_nx.ads/www.rediffmail.com/signatureline.htm@Middle"></a></td>
</tr></tbody></table><table cellpadding=3D"0" cellspacing=3D"0"><tbody><tr>=
<td><div style=3D"font-family:Arial,Helvetica,sans-serif;font-size:14px">Ge=
t your own <span style=3D"padding-bottom:0px;background-color:#cc0000;paddi=
ng-left:3px;padding-RIGHT:3px;font-family:Arial,Helvetica,sans-serif;color:=
#ffffff;font-size:12px;padding-top:0px"><b>FREE</b></span> website,  <span =
style=3D"padding-bottom:0px;background-color:#c00;padding-left:3px;padding-=
RIGHT:3px;font-family:Arial,Helvetica,sans-serif;color:#ffffff;font-size:12=
px;padding-top:0px"><b>FREE</b></span> domain &amp; <span style=3D"padding-=
bottom:0px;background-color:#c00;padding-left:3px;padding-RIGHT:3px;font-fa=
mily:Arial,Helvetica,sans-serif;color:#ffffff;font-size:12px;padding-top:0p=
x"><b>FREE</b></span> mobile app with Company email. =C2=A0</div>
</td><td><a href=3D"http://track.rediff.com/click?url=3D___http://businesse=
mail.rediff.com/email-ids-for-companies-with-less-than-50-employees?sc_cid=
=3Dsign-1-10-13___&amp;cmp=3Dhost&amp;lnk=3Dsign-1-10-13&amp;nsrv1=3Dhost" =
style=3D"font-family:Arial,Helvetica,sans-serif;color:#fff;font-size:14px;c=
olor:#0000cc" target=3D"_blank"><b>Know More &gt;</b></a></td>
</tr></tbody></table><br>--<br>
Linux-cluster mailing list<br>
<a href=3D"mailto:Linux-cluster@redhat.com">Linux-cluster@redhat.com</a><br=
>
<a href=3D"https://www.redhat.com/mailman/listinfo/linux-cluster" target=3D=
"_blank">https://www.redhat.com/mailman/listinfo/linux-cluster</a><br></blo=
ckquote></div><br><br clear=3D"all"><br>-- <br>esta es mi vida e me la vivo=
 hasta que dios quiera
</div>

--089e013c65768b3ab50501d9ed2b--

From lists@alteeve.ca Sat Aug 30 12:35:56 2014
Received: from int-mx13.intmail.prod.int.phx2.redhat.com
	(int-mx13.intmail.prod.int.phx2.redhat.com [10.5.11.26])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7UGZup3031201 for <linux-cluster@listman.util.phx.redhat.com>;
	Sat, 30 Aug 2014 12:35:56 -0400
Received: from mx1.redhat.com (ext-mx14.extmail.prod.ext.phx2.redhat.com
	[10.5.110.19])
	by int-mx13.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7UGZuk3004206
	for <linux-cluster@redhat.com>; Sat, 30 Aug 2014 12:35:56 -0400
Received: from vm08-mail01.alteeve.ca (mail.alteeve.ca [65.39.153.71])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7UGZsBj012676
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256
	verify=NO)
	for <linux-cluster@redhat.com>; Sat, 30 Aug 2014 12:35:54 -0400
Received: from lepuny.alteeve.ca (dhcp-108-168-20-201.cable.user.start.ca
	[108.168.20.201])
	by vm08-mail01.alteeve.ca (Postfix) with ESMTPSA id 3BDC7200DF
	for <linux-cluster@redhat.com>; Sat, 30 Aug 2014 12:35:50 -0400 (EDT)
Message-ID: <5401FD68.8000407@alteeve.ca>
Date: Sat, 30 Aug 2014 12:35:52 -0400
From: Digimer <lists@alteeve.ca>
User-Agent: Mozilla/5.0 (X11; Linux x86_64;
	rv:24.0) Gecko/20100101 Thunderbird/24.7.0
MIME-Version: 1.0
To: linux clustering <linux-cluster@redhat.com>
References: <20140830141242.4308.qmail@f5mail-224-126.rediffmail.com>
In-Reply-To: <20140830141242.4308.qmail@f5mail-224-126.rediffmail.com>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed
Content-Transfer-Encoding: 7bit
X-RedHat-Spam-Score: -1.899  (BAYES_00,RP_MATCHES_RCVD)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.26
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.19
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] Please help me on cluster error
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Sat, 30 Aug 2014 16:35:56 -0000

Can you share your cluster information please?

This could be a network problem, as the messages below happen when the 
network between the nodes isn't fast enough or has too long latency and 
cluster traffic is considered lost and re-requested.

If you don't have fencing working properly, and if a network issue 
caused a node to be declared lost, clustered LVM (and anything else 
using cluster locking) will fail (by design).

If you share your configuration and more of your logs, it will help us 
understand what is happening. Please also tell us what version of the 
cluster software you're using.

digimer

On 30/08/14 10:12 AM, manish vaidya wrote:
> i created four node cluster in kvm enviorment But i faced error when
> create new pv such as pvcreate /dev/sdb1
> got error , lock from node 2 & lock from node3
>
> also strange cluster logs
>
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5e
>
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5e
> 5f
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5f
> 60
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 61
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 63
> 64
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 69
> 6a
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 78
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 84
> 85
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 9a
> 9b
>
>
> Please help me on this issue
> <http://sigads.rediff.com/RealMedia/ads/click_nx.ads/www.rediffmail.com/signatureline.htm@Middle?>
>
> Get your own *FREE* website, *FREE* domain & *FREE* mobile app with
> Company email.
> 	*Know More >*
> <http://track.rediff.com/click?url=___http://businessemail.rediff.com/email-ids-for-companies-with-less-than-50-employees?sc_cid=sign-1-10-13___&cmp=host&lnk=sign-1-10-13&nsrv1=host>
>
>
>


-- 
Digimer
Papers and Projects: https://alteeve.ca/w/
What if the cure for cancer is trapped in the mind of a person without 
access to education?

From neale@sinenomine.net Wed Aug 20 00:45:17 2014
Received: from int-mx14.intmail.prod.int.phx2.redhat.com
	(int-mx14.intmail.prod.int.phx2.redhat.com [10.5.11.27])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7K4jHeR015884 for <linux-cluster@listman.util.phx.redhat.com>;
	Wed, 20 Aug 2014 00:45:17 -0400
Received: from mx1.redhat.com (ext-mx15.extmail.prod.ext.phx2.redhat.com
	[10.5.110.20])
	by int-mx14.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7K4jGcC021921
	for <linux-cluster@redhat.com>; Wed, 20 Aug 2014 00:45:17 -0400
Received: from smtp81.ord1c.emailsrvr.com (smtp81.ord1c.emailsrvr.com
	[108.166.43.81])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7K4jEZw019452
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=NO)
	for <linux-cluster@redhat.com>; Wed, 20 Aug 2014 00:45:15 -0400
Received: from smtp11.relay.ord1c.emailsrvr.com (localhost.localdomain
	[127.0.0.1])
	by smtp11.relay.ord1c.emailsrvr.com (SMTP Server) with ESMTP id
	A85261806F2
	for <linux-cluster@redhat.com>; Wed, 20 Aug 2014 00:45:13 -0400 (EDT)
X-SMTPDoctor-Processed: csmtpprox 2.7.1
Received: from localhost (localhost.localdomain [127.0.0.1])
	by smtp11.relay.ord1c.emailsrvr.com (SMTP Server) with ESMTP id
	A59A7180708
	for <linux-cluster@redhat.com>; Wed, 20 Aug 2014 00:45:13 -0400 (EDT)
X-Virus-Scanned: OK
Received: from smtp192.mex05.mlsrvr.com (unknown [184.106.31.85])
	by smtp11.relay.ord1c.emailsrvr.com (SMTP Server) with ESMTPS id
	9567D1806F2
	for <linux-cluster@redhat.com>; Wed, 20 Aug 2014 00:45:13 -0400 (EDT)
Received: from ORD2MBX02F.mex05.mlsrvr.com ([fe80::92e2:baff:fe11:e744]) by
	ORD2HUB34.mex05.mlsrvr.com ([::1]) with mapi id 14.03.0169.001;
	Tue, 19 Aug 2014 23:45:13 -0500
From: Neale Ferguson <neale@sinenomine.net>
To: linux clustering <linux-cluster@redhat.com>
Thread-Topic: clvmd not terminating
Thread-Index: AQHPvDGHbMQA89shcEK5X3Oh+KJ7xQ==
Date: Wed, 20 Aug 2014 04:45:12 +0000
Message-ID: <6A0A9B9D-E287-468C-B784-3F0A8DAB4E23@sinenomine.net>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
x-originating-ip: [96.247.193.74]
Content-Type: text/plain; charset="us-ascii"
Content-ID: <61CB4A60538E474990DF067418E63097@mex05.mlsrvr.com>
MIME-Version: 1.0
X-RedHat-Spam-Score: -2.311  (BAYES_00, DCC_REPUT_00_12, RCVD_IN_DNSWL_NONE,
	SPF_PASS)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.27
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.20
Content-Transfer-Encoding: 8bit
X-MIME-Autoconverted: from quoted-printable to 8bit by
	lists01.pubmisc.prod.ext.phx2.redhat.com id s7K4jHeR015884
X-loop: linux-cluster@redhat.com
Subject: [Linux-cluster] clvmd not terminating
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Wed, 20 Aug 2014 04:45:17 -0000

We have a sporadic situation where we are attempting to shutdown/restart both nodes of a two node cluster. One shutdowns completely but one sometimes hangs with:

[root@aude2mq036nabzi ~]# service cman stop
Stopping cluster:
   Leaving fence domain... found dlm lockspace /sys/kernel/dlm/clvmd
fence_tool: cannot leave due to active systems
[FAILED]

When the other node is brought back up it has problems with clvmd:

># pvscan
  connect() failed on local socket: Connection refused
  Internal cluster locking initialisation failed.
  WARNING: Falling back to local file-based locking.
  Volume Groups with the clustered attribute will be inaccessible.

Sometimes it works fine but very occasionally we get the above situation. I've encountered the fence message before, usually when the fence devices were incorrectly configured but it would always fail because of this. Before I get too far into investigation mode I wondered if the above symptoms ring any bells for anyone.

Neale


From prvs=53095580F9=ricks@alldigital.com Wed Aug 20 01:05:02 2014
Received: from int-mx10.intmail.prod.int.phx2.redhat.com
	(int-mx10.intmail.prod.int.phx2.redhat.com [10.5.11.23])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7K551TR019047 for <linux-cluster@listman.util.phx.redhat.com>;
	Wed, 20 Aug 2014 01:05:01 -0400
Received: from mx1.redhat.com (ext-mx13.extmail.prod.ext.phx2.redhat.com
	[10.5.110.18])
	by int-mx10.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7K551a9015332
	for <linux-cluster@redhat.com>; Wed, 20 Aug 2014 01:05:01 -0400
Received: from corp.alldigital.com (mex2-r1.alldigital.com [70.183.30.217])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7K54wNX017148
	(version=TLSv1/SSLv3 cipher=AES128-SHA bits=128 verify=FAIL)
	for <linux-cluster@redhat.com>; Wed, 20 Aug 2014 01:04:59 -0400
Received: from [192.168.0.205] (98.154.220.140) by mex2-r1.corp.alldigital.com
	(192.168.4.180) with Microsoft SMTP Server (TLS) id 14.1.438.0;
	Tue, 19 Aug 2014 22:04:56 -0700
Date: Tue, 19 Aug 2014 22:04:56 -0700
Message-ID: <r7ytrr41hwxiir22jyobpqmr.1408511096413@email.android.com>
Importance: normal
From: ricks <ricks@alldigital.com>
To: Neale Ferguson <neale@sinenomine.net>, linux clustering
	<linux-cluster@redhat.com>
MIME-Version: 1.0
Content-Type: multipart/alternative;
	boundary="--_com.android.email_2654625065860350"
X-Originating-IP: [98.154.220.140]
X-RedHat-Spam-Score: -2.569  (BAYES_00, HTML_MESSAGE, RP_MATCHES_RCVD,
	SPF_HELO_PASS, SPF_PASS)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.23
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.18
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] clvmd not terminating
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Wed, 20 Aug 2014 05:05:02 -0000

----_com.android.email_2654625065860350
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64

SnVzdCBpc3N1ZWQuIFNob3VsZCB0YWtlIDEwLTIwIG1pbnV0ZXMgdG8gZ28gdGhyb3VnaC7CoAoK
ClNlbnQgZnJvbSBteSBWZXJpem9uIFdpcmVsZXNzIDRHIExURSBzbWFydHBob25lCgo8ZGl2Pi0t
LS0tLS0tIE9yaWdpbmFsIG1lc3NhZ2UgLS0tLS0tLS08L2Rpdj48ZGl2PkZyb206IE5lYWxlIEZl
cmd1c29uIDxuZWFsZUBzaW5lbm9taW5lLm5ldD4gPC9kaXY+PGRpdj5EYXRlOjA4LzE5LzIwMTQg
IDk6NDUgUE0gIChHTVQtMDc6MDApIDwvZGl2PjxkaXY+VG86IGxpbnV4IGNsdXN0ZXJpbmcgPGxp
bnV4LWNsdXN0ZXJAcmVkaGF0LmNvbT4gPC9kaXY+PGRpdj5TdWJqZWN0OiBbTGludXgtY2x1c3Rl
cl0gY2x2bWQgbm90IHRlcm1pbmF0aW5nIDwvZGl2PjxkaXY+CjwvZGl2PldlIGhhdmUgYSBzcG9y
YWRpYyBzaXR1YXRpb24gd2hlcmUgd2UgYXJlIGF0dGVtcHRpbmcgdG8gc2h1dGRvd24vcmVzdGFy
dCBib3RoIG5vZGVzIG9mIGEgdHdvIG5vZGUgY2x1c3Rlci4gT25lIHNodXRkb3ducyBjb21wbGV0
ZWx5IGJ1dCBvbmUgc29tZXRpbWVzIGhhbmdzIHdpdGg6Cgpbcm9vdEBhdWRlMm1xMDM2bmFiemkg
fl0jIHNlcnZpY2UgY21hbiBzdG9wClN0b3BwaW5nIGNsdXN0ZXI6CiAgIExlYXZpbmcgZmVuY2Ug
ZG9tYWluLi4uIGZvdW5kIGRsbSBsb2Nrc3BhY2UgL3N5cy9rZXJuZWwvZGxtL2Nsdm1kCmZlbmNl
X3Rvb2w6IGNhbm5vdCBsZWF2ZSBkdWUgdG8gYWN0aXZlIHN5c3RlbXMKW0ZBSUxFRF0KCldoZW4g
dGhlIG90aGVyIG5vZGUgaXMgYnJvdWdodCBiYWNrIHVwIGl0IGhhcyBwcm9ibGVtcyB3aXRoIGNs
dm1kOgoKPiMgcHZzY2FuCiAgY29ubmVjdCgpIGZhaWxlZCBvbiBsb2NhbCBzb2NrZXQ6IENvbm5l
Y3Rpb24gcmVmdXNlZAogIEludGVybmFsIGNsdXN0ZXIgbG9ja2luZyBpbml0aWFsaXNhdGlvbiBm
YWlsZWQuCiAgV0FSTklORzogRmFsbGluZyBiYWNrIHRvIGxvY2FsIGZpbGUtYmFzZWQgbG9ja2lu
Zy4KICBWb2x1bWUgR3JvdXBzIHdpdGggdGhlIGNsdXN0ZXJlZCBhdHRyaWJ1dGUgd2lsbCBiZSBp
bmFjY2Vzc2libGUuCgpTb21ldGltZXMgaXQgd29ya3MgZmluZSBidXQgdmVyeSBvY2Nhc2lvbmFs
bHkgd2UgZ2V0IHRoZSBhYm92ZSBzaXR1YXRpb24uIEkndmUgZW5jb3VudGVyZWQgdGhlIGZlbmNl
IG1lc3NhZ2UgYmVmb3JlLCB1c3VhbGx5IHdoZW4gdGhlIGZlbmNlIGRldmljZXMgd2VyZSBpbmNv
cnJlY3RseSBjb25maWd1cmVkIGJ1dCBpdCB3b3VsZCBhbHdheXMgZmFpbCBiZWNhdXNlIG9mIHRo
aXMuIEJlZm9yZSBJIGdldCB0b28gZmFyIGludG8gaW52ZXN0aWdhdGlvbiBtb2RlIEkgd29uZGVy
ZWQgaWYgdGhlIGFib3ZlIHN5bXB0b21zIHJpbmcgYW55IGJlbGxzIGZvciBhbnlvbmUuCgpOZWFs
ZQoKLS0gCkxpbnV4LWNsdXN0ZXIgbWFpbGluZyBsaXN0CkxpbnV4LWNsdXN0ZXJAcmVkaGF0LmNv
bQpodHRwczovL3d3dy5yZWRoYXQuY29tL21haWxtYW4vbGlzdGluZm8vbGludXgtY2x1c3Rlcgo=

----_com.android.email_2654625065860350
Content-Type: text/html; charset="utf-8"
Content-Transfer-Encoding: base64
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----_com.android.email_2654625065860350--


From prvs=6309501FB3=ricks@alldigital.com Wed Aug 20 01:07:19 2014
Received: from int-mx11.intmail.prod.int.phx2.redhat.com
	(int-mx11.intmail.prod.int.phx2.redhat.com [10.5.11.24])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7K57JLY003953 for <linux-cluster@listman.util.phx.redhat.com>;
	Wed, 20 Aug 2014 01:07:19 -0400
Received: from mx1.redhat.com (ext-mx14.extmail.prod.ext.phx2.redhat.com
	[10.5.110.19])
	by int-mx11.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7K57JnP004107
	for <linux-cluster@redhat.com>; Wed, 20 Aug 2014 01:07:19 -0400
Received: from corp.alldigital.com (mex2-r1.alldigital.com [70.183.30.217])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7K57H15014817
	(version=TLSv1/SSLv3 cipher=AES128-SHA bits=128 verify=FAIL)
	for <linux-cluster@redhat.com>; Wed, 20 Aug 2014 01:07:17 -0400
Received: from [192.168.0.205] (98.154.220.140) by mex2-r1.corp.alldigital.com
	(192.168.4.180) with Microsoft SMTP Server (TLS) id 14.1.438.0;
	Tue, 19 Aug 2014 22:07:16 -0700
Date: Tue, 19 Aug 2014 22:07:16 -0700
Message-ID: <guopjf57t8r0ygaycyb82dbr.1408511236324@email.android.com>
Importance: normal
From: ricks <ricks@alldigital.com>
To: Neale Ferguson <neale@sinenomine.net>, linux clustering
	<linux-cluster@redhat.com>
MIME-Version: 1.0
Content-Type: multipart/alternative;
	boundary="--_com.android.email_2656025805880140"
X-Originating-IP: [98.154.220.140]
X-RedHat-Spam-Score: -2.569  (BAYES_00, HTML_MESSAGE, RP_MATCHES_RCVD,
	SPF_HELO_PASS, SPF_PASS)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.24
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.19
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] clvmd not terminating
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Wed, 20 Aug 2014 05:07:19 -0000

----_com.android.email_2656025805880140
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
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----_com.android.email_2656025805880140
Content-Type: text/html; charset="utf-8"
Content-Transfer-Encoding: base64
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----_com.android.email_2656025805880140--


From wferi@niif.hu Thu Aug 21 20:37:55 2014
Received: from int-mx13.intmail.prod.int.phx2.redhat.com
	(int-mx13.intmail.prod.int.phx2.redhat.com [10.5.11.26])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7M0btnx024641 for <linux-cluster@listman.util.phx.redhat.com>;
	Thu, 21 Aug 2014 20:37:55 -0400
Received: from mx1.redhat.com (ext-mx15.extmail.prod.ext.phx2.redhat.com
	[10.5.110.20])
	by int-mx13.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7M0bt4m022567
	for <linux-cluster@redhat.com>; Thu, 21 Aug 2014 20:37:55 -0400
Received: from listserv2.niif.hu (listserv2.niif.hu [193.225.14.155])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7M0bpsO007144
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES128-SHA bits=128 verify=NO)
	for <linux-cluster@redhat.com>; Thu, 21 Aug 2014 20:37:52 -0400
Received: from business-188-142-225-206.business.broadband.hu
	([188.142.225.206] helo=lant.ki.iif.hu)
	by listserv2.niif.hu with esmtpsa (TLS1.2:DHE_RSA_AES_128_CBC_SHA1:128)
	(Exim 4.80) (envelope-from <wferi@niif.hu>) id 1XKcrV-0006n6-SR
	for linux-cluster@redhat.com; Fri, 22 Aug 2014 02:37:49 +0200
Received: from wferi by lant.ki.iif.hu with local (Exim 4.80)
	(envelope-from <wferi@lant.ki.iif.hu>) id 1XKcrQ-0004CM-6t
	for linux-cluster@redhat.com; Fri, 22 Aug 2014 02:37:44 +0200
From: Ferenc Wagner <wferi@niif.hu>
To: linux clustering <linux-cluster@redhat.com>
Date: Fri, 22 Aug 2014 02:37:44 +0200
Message-ID: <8761hlickn.fsf@lant.ki.iif.hu>
User-Agent: Gnus/5.13 (Gnus v5.13) Emacs/23.4 (gnu/linux)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
X-RedHat-Spam-Score: -4.868  (BAYES_00,RCVD_IN_DNSWL_MED,RP_MATCHES_RCVD)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.26
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.20
X-loop: linux-cluster@redhat.com
Subject: [Linux-cluster] on exiting maintenance mode
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Fri, 22 Aug 2014 00:37:55 -0000

Hi,

While my Pacemaker cluster was in maintenance mode, resources were moved
(by hand) between the nodes as I rebooted each node in turn.  In the end
the crm status output became perfectly empty, as the reboot of a given
node removed from the output the resources which were located on the
rebooted node at the time of entering maintenance mode.  I expected full
resource discovery on exiting maintenance mode, but it probably did not
happen, as the cluster started up resources already running on other
nodes, which is generally forbidden.  Given that all resources were
running (though possibly migrated during the maintenance), what would
have been the correct way of bringing the cluster out of maintenance
mode?  This should have required no resource actions at all.  Would
cleanup of all resources have helped?  Or is there a better way?
-- 
Thanks,
Feri.


From vasil.val@gmail.com Tue Aug 26 02:56:32 2014
Received: from int-mx13.intmail.prod.int.phx2.redhat.com
	(int-mx13.intmail.prod.int.phx2.redhat.com [10.5.11.26])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7Q6uWeV015148 for <linux-cluster@listman.util.phx.redhat.com>;
	Tue, 26 Aug 2014 02:56:32 -0400
Received: from mx1.redhat.com (ext-mx14.extmail.prod.ext.phx2.redhat.com
	[10.5.110.19])
	by int-mx13.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7Q6uWfS024998
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 02:56:32 -0400
Received: from mail-la0-f54.google.com (mail-la0-f54.google.com
	[209.85.215.54])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7Q6uTXg020608
	(version=TLSv1/SSLv3 cipher=RC4-SHA bits=128 verify=FAIL)
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 02:56:31 -0400
Received: by mail-la0-f54.google.com with SMTP id hz20so14556792lab.13
	for <linux-cluster@redhat.com>; Mon, 25 Aug 2014 23:56:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=gmail.com; s=20120113;
	h=mime-version:date:message-id:subject:from:to:content-type;
	bh=l41Tr1EZts/OOXBhMx1A8w5H77OBDfyjrP8mUv+RV10=;
	b=jKoJoWe9+hjJFclhLuZD4CkRh9wajv3AOjeKjFbmAS9Je7ZoYPepZy8/bR986/Gnl9
	0JrKeiYZCy4x3lBMwhQcOXOuD3Vkubk/1SQoVsLUSSrC0g4cHhxgI2I2KwTVQ/E7w0o3
	x62LOSLoOxSSWTV1Rv/QlgRBCL5Fvtkj+w3yEBjd06rGlMCJcdHSbe/dBzV4qWnmlV1A
	biZcjuiwI1uPfOcX2vAFOwsbQFfcaz/pQUjqBrN0n+pfLURlhirjyDG2LIOK5JOr6ZLY
	zyxM1uwqWGCfP37kSJm/rLkPolXTZGl6eVCtrT+s5Lnil22P8cYnwveZeYC3pduigf7L
	lf+Q==
MIME-Version: 1.0
X-Received: by 10.152.9.170 with SMTP id a10mr12493889lab.79.1409036189485;
	Mon, 25 Aug 2014 23:56:29 -0700 (PDT)
Received: by 10.25.162.199 with HTTP; Mon, 25 Aug 2014 23:56:29 -0700 (PDT)
Date: Tue, 26 Aug 2014 09:56:29 +0300
Message-ID: <CAFZxf=L12UCn6nEnd_LtRWL6_P=wOALfArR6DxhL9RU5iA2Tnw@mail.gmail.com>
From: Vasil Valchev <vasil.val@gmail.com>
To: linux-cluster@redhat.com
Content-Type: multipart/alternative; boundary=001a1132f49a8cd2d9050182cde2
X-RedHat-Spam-Score: -3.099  (BAYES_00, DCC_REPUT_00_12, DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU, FREEMAIL_FROM, HTML_MESSAGE,
	RCVD_IN_DNSWL_LOW, SPF_PASS)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.26
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.19
X-loop: linux-cluster@redhat.com
Subject: [Linux-cluster] totem token & post_fail_delay question
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Tue, 26 Aug 2014 06:56:32 -0000

--001a1132f49a8cd2d9050182cde2
Content-Type: text/plain; charset=UTF-8

Hello,

I have a cluster that sometimes has intermittent network issues on the
heartbeat network.
Unfortunately improving the network is not an option, so I am looking for a
way to tolerate longer interruptions.

Previously it seemed to me the post_fail_delay option is suitable, but
after some research it might not be what I am looking for.

If I am correct, when a member leaves (due to token timeout) the cluster
will wait the post_fail_delay before fencing. If the member rejoins before
that, it will still be fenced, because it has previous state?
>From a recent fencing on this cluster there is a strange message:

Aug 24 06:20:45 node2 openais[29048]: [MAIN ] Not killing node node1cl
despite it rejoining the cluster with existing state, it has a lower node ID

What does this mean?

And lastly is increasing the totem token timeout the way to go?


Thanks,
Vasil Valchev

--001a1132f49a8cd2d9050182cde2
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hello,<div><br></div><div>I have a cluster that sometimes =
has intermittent network issues on the heartbeat network.</div><div>Unfortu=
nately improving the network is not an option, so I am looking for a way to=
 tolerate longer interruptions.</div>
<div><br></div><div>Previously it seemed to me the post_fail_delay option i=
s suitable, but after some research it might not be what I am looking for.<=
br></div><div><br></div><div>If I am correct, when a member leaves (due to =
token timeout) the cluster will wait the post_fail_delay before fencing. If=
 the member rejoins before that, it will still be fenced, because it has pr=
evious state?</div>
<div>From a recent fencing on this cluster there is a strange message:</div=
><div><br></div><div>Aug 24 06:20:45 node2 openais[29048]: [MAIN ] Not kill=
ing node node1cl despite it rejoining the cluster with existing state, it h=
as a lower node ID<br>
</div><div><br></div><div>What does this mean?</div><div><br></div><div>And=
 lastly is increasing the totem token timeout the way to go?<br></div><div>=
<br></div><div><br></div><div>Thanks,</div><div>Vasil Valchev</div></div>

--001a1132f49a8cd2d9050182cde2--


From andrew@beekhof.net Tue Aug 26 03:40:55 2014
Received: from int-mx09.intmail.prod.int.phx2.redhat.com
	(int-mx09.intmail.prod.int.phx2.redhat.com [10.5.11.22])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7Q7et0l028870 for <linux-cluster@listman.util.phx.redhat.com>;
	Tue, 26 Aug 2014 03:40:55 -0400
Received: from mx1.redhat.com (ext-mx14.extmail.prod.ext.phx2.redhat.com
	[10.5.110.19])
	by int-mx09.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7Q7etCu010141
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 03:40:55 -0400
Received: from out2-smtp.messagingengine.com (out2-smtp.messagingengine.com
	[66.111.4.26])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7Q7esSL002132
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256
	verify=NO)
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 03:40:54 -0400
Received: from compute3.internal (compute3.nyi.internal [10.202.2.43])
	by gateway2.nyi.internal (Postfix) with ESMTP id 9EABE20905
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 03:40:53 -0400 (EDT)
Received: from frontend1 ([10.202.2.160])
	by compute3.internal (MEProxy); Tue, 26 Aug 2014 03:40:53 -0400
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=beekhof.net; h=
	content-type:subject:mime-version:from:in-reply-to:date
	:message-id:references:to; s=mesmtp; bh=lpGMbSjwbDJoOHwzTvPWp19X
	ajc=; b=QBtvL1PfPKKAd9FLNRpJvOYuvY09tzwPUSGFfngP3JK24TCbHWMLATxn
	TghuwVIr2brtFbW5YEtmON0iJKaLlQo5yCWrjUPia1rfYlnkvQKzVFW62OoUVBHj
	Gm9KelvQw6bRSEusNUmpStj+6nW8CLWrcXtosDtXSoF+AJDZERA=
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=
	messagingengine.com; h=content-type:subject:mime-version:from
	:in-reply-to:date:message-id:references:to; s=smtpout; bh=lpGMbS
	jwbDJoOHwzTvPWp19Xajc=; b=brfQei8XnfgyFj8Uo3FBDexp9PU3+8lR7Irzpj
	+xdex9GrEDDmKMkUSha4cIBhwX9vmgVBTJAxcjcfg3y96zCmPXbzhuQlirV4tqr9
	MWO6pr2a6aG3G9ojyrHcl9ZULO6am0rXsQfFUU5nYnQu36fyznH2uvo2CnmGqPHb
	wyku0=
X-Sasl-enc: ezexVGhEMn13dmTLP9aGcGN3i8f8ZETzpWftYyUVNP7U 1409038853
Received: from [172.16.1.5] (unknown [120.147.36.73])
	by mail.messagingengine.com (Postfix) with ESMTPA id AF7FEC008FC
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 03:40:52 -0400 (EDT)
Content-Type: multipart/signed;
	boundary="Apple-Mail=_2F0ED310-AFCD-4A8C-8864-BDE9C09FD9B4";
	protocol="application/pgp-signature"; micalg=pgp-sha512
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
From: Andrew Beekhof <andrew@beekhof.net>
In-Reply-To: <8761hlickn.fsf@lant.ki.iif.hu>
Date: Tue, 26 Aug 2014 17:40:50 +1000
X-Mao-Original-Outgoing-Id: 430731649.193825-613619ee84c2461a7b15b12a65503e51
Message-Id: <67506B71-8594-4C16-82C1-F94779F59826@beekhof.net>
References: <8761hlickn.fsf@lant.ki.iif.hu>
To: linux clustering <linux-cluster@redhat.com>
X-RedHat-Spam-Score: -1.201  (BAYES_20, DCC_REPUT_00_12, DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU, RCVD_IN_DNSWL_LOW)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.22
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.19
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] on exiting maintenance mode
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Tue, 26 Aug 2014 07:40:55 -0000


--Apple-Mail=_2F0ED310-AFCD-4A8C-8864-BDE9C09FD9B4
Content-Transfer-Encoding: 7bit
Content-Type: text/plain;
	charset=us-ascii


On 22 Aug 2014, at 10:37 am, Ferenc Wagner <wferi@niif.hu> wrote:

> Hi,
> 
> While my Pacemaker cluster was in maintenance mode, resources were moved
> (by hand) between the nodes as I rebooted each node in turn.  In the end
> the crm status output became perfectly empty, as the reboot of a given
> node removed from the output the resources which were located on the
> rebooted node at the time of entering maintenance mode.  I expected full
> resource discovery on exiting maintenance mode,

Version and logs?

The discovery usually happens at the point the cluster is started on a node.
Maintenance mode just prevents the cluster from doing anything about it.

> but it probably did not
> happen, as the cluster started up resources already running on other
> nodes, which is generally forbidden.  Given that all resources were
> running (though possibly migrated during the maintenance), what would
> have been the correct way of bringing the cluster out of maintenance
> mode?  This should have required no resource actions at all.  Would
> cleanup of all resources have helped?  Or is there a better way?
> -- 
> Thanks,
> Feri.
> 
> -- 
> Linux-cluster mailing list
> Linux-cluster@redhat.com
> https://www.redhat.com/mailman/listinfo/linux-cluster


--Apple-Mail=_2F0ED310-AFCD-4A8C-8864-BDE9C09FD9B4
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment;
	filename=signature.asc
Content-Type: application/pgp-signature;
	name=signature.asc
Content-Description: Message signed with OpenPGP using GPGMail

-----BEGIN PGP SIGNATURE-----
Comment: GPGTools - http://gpgtools.org

iQIcBAEBCgAGBQJT/DoBAAoJEBTzwpg4iwmNP6UP/RD/otuXqJXlcQBTph2M3ija
1iCvUUe4LV7VrpUUz7g7gZFFWadHHBqWi+F3rE4eeARA0qtasBvnEM+35AKmMB8R
PELKXy3jIyvBhZcsojUbiheGKJqsqmeWbiGtUkwFJ9KQMfTgdzHAVuiVDddDKS00
lwSBIRzLvzJbszlLE3IxduciYFuM2jH9rUv5sCsnXWTE2xZaJBkyjUGob3rkK13J
UzJOKUT2cKORNMRrctqOC15f0IyDB1Wdg0XmRv3jqH93l1XzQe3vDAl9LJZkCwd0
/qdSfdKbOWPf/Eb6EB4j9VX/qtbQoxqOlXgSpzTxh0bNUAwci1DeWPb+LcaH3pmE
dtp+nmm808jQAEJ8vpm3xfeyRSPofjThMNkpoKYtkRvfiP5c8G4T3sfEU22BDkV8
bJJ0bgyVhwnfea8LqYBy/b1zLmTzHlx2JXNptHc1G9iCdcDI19xt7rG86Zaxs/GR
OgbWHg4cI/DdUYxcXBem0T+Kxi9J91ChF4rmXwWlUXajP6FOMMx1XNWeMNVqb58I
F/jJa5uAnlwt9SJnk65wwSux1Ly0X68KXrcKJIrxQKMl/jKgYm5Ef4p+Bdh3deZL
D2oiMdj78Qh75u2Wu5JmHT1j2QzrFrQc3HaLgslb3ZofViPoj190gDpnFo0pK2Rn
fjxbxxuKfTDJYBVm4s2M
=IrHk
-----END PGP SIGNATURE-----

--Apple-Mail=_2F0ED310-AFCD-4A8C-8864-BDE9C09FD9B4--


From emi2fast@gmail.com Tue Aug 26 04:18:04 2014
Received: from int-mx14.intmail.prod.int.phx2.redhat.com
	(int-mx14.intmail.prod.int.phx2.redhat.com [10.5.11.27])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7Q8I4kE016989 for <linux-cluster@listman.util.phx.redhat.com>;
	Tue, 26 Aug 2014 04:18:04 -0400
Received: from mx1.redhat.com (ext-mx13.extmail.prod.ext.phx2.redhat.com
	[10.5.110.18])
	by int-mx14.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7Q8I36n015692
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 04:18:03 -0400
Received: from mail-ob0-f177.google.com (mail-ob0-f177.google.com
	[209.85.214.177])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7Q8I1Po006206
	(version=TLSv1/SSLv3 cipher=RC4-SHA bits=128 verify=FAIL)
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 04:18:02 -0400
Received: by mail-ob0-f177.google.com with SMTP id wp18so11140761obc.22
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 01:18:01 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=gmail.com; s=20120113;
	h=mime-version:in-reply-to:references:date:message-id:subject:from:to
	:content-type; bh=Dsw7Kibdmsb5py0hlWCOL0lMjSuTamrY0T0rDJtHuhM=;
	b=0Tx1yyfiO0r3FdJJFW1U5LtoezZdRDXaGdEuD2T18nPRbx7iDATULubq1pFfxWWhoX
	vI0INiBCVXG6eVj48/z7PkqsnYktVC3+MDE5+b7AJmoIEOiRdQF59O3dhstGSAdXPrey
	EmKkhauYmvnq7DwNaqtWBlevBEGaZ8gLeNMgtiAQUyGrCzXb+MbQYDlo/Oji6dWfdosC
	cMsB/TvK2htYipY0knkFEBsdpWEMMOLSts0gzb6rZi5gtt4fYhaf575A2QlY/J6XLmcV
	1NP4/KWn8Ein6VbNi6d9f1qhb1Z/3qhW5ko0xPQRymt/2RNqKIcqxlp/T4zVE7Zxr2to
	MS/Q==
MIME-Version: 1.0
X-Received: by 10.182.191.39 with SMTP id gv7mr5719429obc.14.1409040699953;
	Tue, 26 Aug 2014 01:11:39 -0700 (PDT)
Received: by 10.76.156.71 with HTTP; Tue, 26 Aug 2014 01:11:39 -0700 (PDT)
In-Reply-To: <CAFZxf=L12UCn6nEnd_LtRWL6_P=wOALfArR6DxhL9RU5iA2Tnw@mail.gmail.com>
References: <CAFZxf=L12UCn6nEnd_LtRWL6_P=wOALfArR6DxhL9RU5iA2Tnw@mail.gmail.com>
Date: Tue, 26 Aug 2014 10:11:39 +0200
Message-ID: <CAE7pJ3BsQtgeO9pHquqtA_STd_HCTh=RXTrNEMFNQzRSM2A0Tg@mail.gmail.com>
From: emmanuel segura <emi2fast@gmail.com>
To: linux clustering <linux-cluster@redhat.com>
Content-Type: text/plain; charset=UTF-8
X-RedHat-Spam-Score: -0.801  (BAYES_20, DKIM_SIGNED, DKIM_VALID, DKIM_VALID_AU,
	FREEMAIL_FROM, RCVD_IN_DNSWL_LOW, SPF_PASS)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.27
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.18
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] totem token & post_fail_delay question
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Tue, 26 Aug 2014 08:18:04 -0000

from man fenced

Post-fail delay is the number of seconds the daemon will wait before
fencing any victims after a domain member fails.

It's used for delay the fence action.

2014-08-26 8:56 GMT+02:00 Vasil Valchev <vasil.val@gmail.com>:
> Hello,
>
> I have a cluster that sometimes has intermittent network issues on the
> heartbeat network.
> Unfortunately improving the network is not an option, so I am looking for a
> way to tolerate longer interruptions.
>
> Previously it seemed to me the post_fail_delay option is suitable, but after
> some research it might not be what I am looking for.
>
> If I am correct, when a member leaves (due to token timeout) the cluster
> will wait the post_fail_delay before fencing. If the member rejoins before
> that, it will still be fenced, because it has previous state?
> From a recent fencing on this cluster there is a strange message:
>
> Aug 24 06:20:45 node2 openais[29048]: [MAIN ] Not killing node node1cl
> despite it rejoining the cluster with existing state, it has a lower node ID
>
> What does this mean?
>
> And lastly is increasing the totem token timeout the way to go?
>
>
> Thanks,
> Vasil Valchev
>
> --
> Linux-cluster mailing list
> Linux-cluster@redhat.com
> https://www.redhat.com/mailman/listinfo/linux-cluster



-- 
esta es mi vida e me la vivo hasta que dios quiera


From ccaulfie@redhat.com Tue Aug 26 04:23:16 2014
Received: from int-mx09.intmail.prod.int.phx2.redhat.com
	(int-mx09.intmail.prod.int.phx2.redhat.com [10.5.11.22])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7Q8NGVS018071 for <linux-cluster@listman.util.phx.redhat.com>;
	Tue, 26 Aug 2014 04:23:16 -0400
Received: from rhdesktop.chrissie.net (vpn1-5-107.ams2.redhat.com
	[10.36.5.107])
	by int-mx09.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7Q8NEoH030103
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES128-SHA bits=128 verify=NO)
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 04:23:16 -0400
Message-ID: <53FC43F2.2010003@redhat.com>
Date: Tue, 26 Aug 2014 09:23:14 +0100
From: Christine Caulfield <ccaulfie@redhat.com>
User-Agent: Mozilla/5.0 (X11; Linux x86_64;
	rv:24.0) Gecko/20100101 Thunderbird/24.7.0
MIME-Version: 1.0
To: linux-cluster@redhat.com
References: <CAFZxf=L12UCn6nEnd_LtRWL6_P=wOALfArR6DxhL9RU5iA2Tnw@mail.gmail.com>
In-Reply-To: <CAFZxf=L12UCn6nEnd_LtRWL6_P=wOALfArR6DxhL9RU5iA2Tnw@mail.gmail.com>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed
Content-Transfer-Encoding: 7bit
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.22
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] totem token & post_fail_delay question
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Tue, 26 Aug 2014 08:23:16 -0000

On 26/08/14 07:56, Vasil Valchev wrote:
> Hello,
>
> I have a cluster that sometimes has intermittent network issues on the
> heartbeat network.
> Unfortunately improving the network is not an option, so I am looking
> for a way to tolerate longer interruptions.
>
> Previously it seemed to me the post_fail_delay option is suitable, but
> after some research it might not be what I am looking for.
>
> If I am correct, when a member leaves (due to token timeout) the cluster
> will wait the post_fail_delay before fencing. If the member rejoins
> before that, it will still be fenced, because it has previous state?
>  From a recent fencing on this cluster there is a strange message:
>
> Aug 24 06:20:45 node2 openais[29048]: [MAIN ] Not killing node node1cl
> despite it rejoining the cluster with existing state, it has a lower node ID
>
> What does this mean?
>

It's an attempt by cman to sort out which node to kill in the situation 
where a node rejoins too quickly. If both nodes try to send a 'kill' 
message then then both nodes would leave the cluster leaving you with no 
active nodes. So cman (and fencing) prioritise the node with the lowest 
nodeID in an attempt at a tie-break. you should see a corresponding 
message on the other node:
"Killing node %s because it has rejoined the cluster with existing state 
and has higher node ID"


> And lastly is increasing the totem token timeout the way to go?
>

if there is no option for improving the network situation then, yes, 
increasing token timeout is probably your best option.

Chrissie


From emi2fast@gmail.com Tue Aug 26 06:08:51 2014
Received: from int-mx14.intmail.prod.int.phx2.redhat.com
	(int-mx14.intmail.prod.int.phx2.redhat.com [10.5.11.27])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7QA8pdR010966 for <linux-cluster@listman.util.phx.redhat.com>;
	Tue, 26 Aug 2014 06:08:51 -0400
Received: from mx1.redhat.com (ext-mx13.extmail.prod.ext.phx2.redhat.com
	[10.5.110.18])
	by int-mx14.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7QA8pgj008148
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 06:08:51 -0400
Received: from mail-oa0-f42.google.com (mail-oa0-f42.google.com
	[209.85.219.42])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7QA8mfV011558
	(version=TLSv1/SSLv3 cipher=RC4-SHA bits=128 verify=FAIL)
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 06:08:49 -0400
Received: by mail-oa0-f42.google.com with SMTP id n16so11838661oag.1
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 03:08:48 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=gmail.com; s=20120113;
	h=mime-version:in-reply-to:references:date:message-id:subject:from:to
	:content-type; bh=5PyVKw5n4J+ACT7HhUVclBOeNPGqV3NCneINBh14tj8=;
	b=kVuhDofYENzbZwd+5iMAmmaiHJxp3FBLG6GHkuV92XQW4Mhj3/wn9Fgblyhl8BqaXu
	KLcTCYBIid94pYkNfTMYaMDn7KMTDcHB7tsuwiBVzWzFl2GuUgT8OLwM0TQnjjtM8tRy
	NjM/LQ4/2uxziB6aESAcBMaaApsuv/13n9PvduBu/rb1nEXPRw4c4QE+qr0NRr5ovCbq
	Yns4hfkL19NTMOXBNTd3t+jg/hJrph50cQxV48XQBHDdNvR4eb9LZmOLtrFXtGrou377
	NWnZXNzuuU+6M1ngG9W5gnDaEpWTbAhL399flBEeooweKzZ/ewO8hGon6trLEZdBF/Bp
	mnYg==
MIME-Version: 1.0
X-Received: by 10.182.191.39 with SMTP id gv7mr6196997obc.14.1409047728198;
	Tue, 26 Aug 2014 03:08:48 -0700 (PDT)
Received: by 10.76.156.71 with HTTP; Tue, 26 Aug 2014 03:08:48 -0700 (PDT)
In-Reply-To: <53FC43F2.2010003@redhat.com>
References: <CAFZxf=L12UCn6nEnd_LtRWL6_P=wOALfArR6DxhL9RU5iA2Tnw@mail.gmail.com>
	<53FC43F2.2010003@redhat.com>
Date: Tue, 26 Aug 2014 12:08:48 +0200
Message-ID: <CAE7pJ3Ae29bUz4CRo41Lv+8CSUD9cQC35zr2mP0A__Bvu40eZQ@mail.gmail.com>
From: emmanuel segura <emi2fast@gmail.com>
To: linux clustering <linux-cluster@redhat.com>
Content-Type: text/plain; charset=UTF-8
X-RedHat-Spam-Score: -2.7  (BAYES_00, DKIM_SIGNED, DKIM_VALID, DKIM_VALID_AU,
	FREEMAIL_FROM, RCVD_IN_DNSWL_LOW, SPF_PASS)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.27
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.18
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] totem token & post_fail_delay question
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Tue, 26 Aug 2014 10:08:51 -0000

i think, you are talking about:

 Post-join delay is the number of seconds the daemon will wait before
fencing any victims after a node joins the domain.



2014-08-26 10:23 GMT+02:00 Christine Caulfield <ccaulfie@redhat.com>:
> On 26/08/14 07:56, Vasil Valchev wrote:
>>
>> Hello,
>>
>> I have a cluster that sometimes has intermittent network issues on the
>> heartbeat network.
>> Unfortunately improving the network is not an option, so I am looking
>> for a way to tolerate longer interruptions.
>>
>> Previously it seemed to me the post_fail_delay option is suitable, but
>> after some research it might not be what I am looking for.
>>
>> If I am correct, when a member leaves (due to token timeout) the cluster
>> will wait the post_fail_delay before fencing. If the member rejoins
>> before that, it will still be fenced, because it has previous state?
>>  From a recent fencing on this cluster there is a strange message:
>>
>> Aug 24 06:20:45 node2 openais[29048]: [MAIN ] Not killing node node1cl
>> despite it rejoining the cluster with existing state, it has a lower node
>> ID
>>
>> What does this mean?
>>
>
> It's an attempt by cman to sort out which node to kill in the situation
> where a node rejoins too quickly. If both nodes try to send a 'kill' message
> then then both nodes would leave the cluster leaving you with no active
> nodes. So cman (and fencing) prioritise the node with the lowest nodeID in
> an attempt at a tie-break. you should see a corresponding message on the
> other node:
> "Killing node %s because it has rejoined the cluster with existing state and
> has higher node ID"
>
>
>
>> And lastly is increasing the totem token timeout the way to go?
>>
>
> if there is no option for improving the network situation then, yes,
> increasing token timeout is probably your best option.
>
> Chrissie
>
>
> --
> Linux-cluster mailing list
> Linux-cluster@redhat.com
> https://www.redhat.com/mailman/listinfo/linux-cluster



-- 
esta es mi vida e me la vivo hasta que dios quiera


From wferi@niif.hu Tue Aug 26 13:40:31 2014
Received: from int-mx10.intmail.prod.int.phx2.redhat.com
	(int-mx10.intmail.prod.int.phx2.redhat.com [10.5.11.23])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7QHeVrm005010 for <linux-cluster@listman.util.phx.redhat.com>;
	Tue, 26 Aug 2014 13:40:31 -0400
Received: from mx1.redhat.com (ext-mx16.extmail.prod.ext.phx2.redhat.com
	[10.5.110.21])
	by int-mx10.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7QHeVAx005837
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 13:40:31 -0400
Received: from listserv2.niif.hu (listserv2.niif.hu [193.225.14.155])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7QHePPt000593
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES128-SHA bits=128 verify=NO)
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 13:40:27 -0400
Received: from business-188-142-225-206.business.broadband.hu
	([188.142.225.206] helo=lant.ki.iif.hu)
	by listserv2.niif.hu with esmtpsa (TLS1.2:DHE_RSA_AES_128_CBC_SHA1:128)
	(Exim 4.80) (envelope-from <wferi@niif.hu>) id 1XMKjI-0001Ob-UQ
	for linux-cluster@redhat.com; Tue, 26 Aug 2014 19:40:24 +0200
Received: from wferi by lant.ki.iif.hu with local (Exim 4.80)
	(envelope-from <wferi@lant.ki.iif.hu>) id 1XMKjD-0002SY-Bs
	for linux-cluster@redhat.com; Tue, 26 Aug 2014 19:40:19 +0200
From: Ferenc Wagner <wferi@niif.hu>
To: linux clustering <linux-cluster@redhat.com>
References: <8761hlickn.fsf@lant.ki.iif.hu>
	<67506B71-8594-4C16-82C1-F94779F59826@beekhof.net>
Date: Tue, 26 Aug 2014 19:40:19 +0200
In-Reply-To: <67506B71-8594-4C16-82C1-F94779F59826@beekhof.net> (Andrew
	Beekhof's message of "Tue, 26 Aug 2014 17:40:50 +1000")
Message-ID: <87iolf9mkc.fsf@lant.ki.iif.hu>
User-Agent: Gnus/5.13 (Gnus v5.13) Emacs/23.4 (gnu/linux)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
X-RedHat-Spam-Score: -4.199  (BAYES_00,RCVD_IN_DNSWL_MED,RP_MATCHES_RCVD)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.23
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.21
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] on exiting maintenance mode
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Tue, 26 Aug 2014 17:40:31 -0000

Andrew Beekhof <andrew@beekhof.net> writes:

> On 22 Aug 2014, at 10:37 am, Ferenc Wagner <wferi@niif.hu> wrote:
>
>> While my Pacemaker cluster was in maintenance mode, resources were moved
>> (by hand) between the nodes as I rebooted each node in turn.  In the end
>> the crm status output became perfectly empty, as the reboot of a given
>> node removed from the output the resources which were located on the
>> rebooted node at the time of entering maintenance mode.  I expected full
>> resource discovery on exiting maintenance mode,
>
> Version and logs?

(The more interesting part comes later, please skip to the theoretical
part if you're short on time. :)

I left those out, as I don't expect the actual behavior to be a bug.
But I experienced this with Pacemaker version 1.1.7.  I know it's old
and it suffers from crmd segfault on entering maintenance mode (cf.
http://thread.gmane.org/gmane.linux.highavailability.user/39121), but
works well generally so I did not get to upgrade it yet.  Now that I
mentioned the crmd segfault: I noted that it died on the DC when I
entered maintenance mode:

crmd: [7452]: info: te_rsc_command: Initiating action 64: cancel vm-tmvp_monitor_60000 on n01 (local)
crmd: [7452]: ERROR: lrm_get_rsc(666): failed to send a getrsc message to lrmd via ch_cmd channel.
crmd: [7452]: ERROR: get_lrm_resource: Could not add resource vm-tmvp to LRM
crmd: [7452]: ERROR: do_lrm_invoke: Invalid resource definition
crmd: [7452]: WARN: do_lrm_invoke: bad input <create_request_adv origin="te_rsc_command" t="crmd" version="3.0.6" subt="request" reference="lrm_invoke-tengine-1408517719-30820" crm_task="lrm_invoke" crm_sys_to="lrmd" crm_sys_from="tengine" crm_host_to="n01" >
crmd: [7452]: WARN: do_lrm_invoke: bad input   <crm_xml >
crmd: [7452]: WARN: do_lrm_invoke: bad input     <rsc_op id="64" operation="cancel" operation_key="vm-tmvp_monitor_60000" on_node="n01" on_node_uuid="n01" transition-key="64:20579:0:1b0a6e79-af5a-41e4-8ced-299371e7922c" >
crmd: [7452]: WARN: do_lrm_invoke: bad input       <primitive id="vm-tmvp" long-id="vm-tmvp" class="ocf" provider="niif" type="TransientDomain" />
crmd: [7452]: info: te_rsc_command: Initiating action 86: cancel vm-wfweb_monitor_60000 on n01 (local)
crmd: [7452]: ERROR: lrm_add_rsc(870): failed to send a addrsc message to lrmd via ch_cmd channel.
crmd: [7452]: ERROR: lrm_get_rsc(666): failed to send a getrsc message to lrmd via ch_cmd channel.
corosync[6966]:   [pcmk  ] info: pcmk_ipc_exit: Client crmd (conn=0x1dc6ea0, async-conn=0x1dc6ea0) left
pacemakerd: [7443]: WARN: Managed crmd process 7452 killed by signal 11 [SIGSEGV - Segmentation violation].
pacemakerd: [7443]: notice: pcmk_child_exit: Child process crmd terminated with signal 11 (pid=7452, rc=0)

However, it got restarted seamlessly, without the node being fenced, so
I did not even notice this until now.  Should this have resulted in the
node being fenced?

But back to the issue at hand.  The Pacemaker shutdown seemed normal,
apart from the bunch of messages like:

crmd: [13794]: ERROR: verify_stopped: Resource vm-web5 was active at shutdown.  You may ignore this error if it is unmanaged.

appearing twice and warnings like:

cib: [7447]: WARN: send_ipc_message: IPC Channel to 13794 is not connected
cib: [7447]: WARN: send_via_callback_channel: Delivery of reply to client 13794/bf6f43a2-70db-40ac-a902-eabc3c12e20d failed
cib: [7447]: WARN: do_local_notify: A-Sync reply to crmd failed: reply failed
corosync[6966]:   [pcmk  ] WARN: route_ais_message: Sending message to local.crmd failed: ipc delivery failed (rc=-2)

On reboot, corosync complained until the some Pacemaker components
started:

corosync[8461]:   [pcmk  ] WARN: route_ais_message: Sending message to local.cib failed: ipc delivery failed (rc=-2)
corosync[8461]:   [pcmk  ] WARN: route_ais_message: Sending message to local.crmd failed: ipc delivery failed (rc=-2)

Pacemaker then probed the resources on the local node (all was inactive):

lrmd: [8946]: info: rsc:stonith-n01 probe[5] (pid 9081)
lrmd: [8946]: info: rsc:dlm:0 probe[6] (pid 9082)
[...]
lrmd: [8946]: info: operation monitor[112] on vm-fir for client 8949: pid 12015 exited with return code 7
crmd: [8949]: info: process_lrm_event: LRM operation vm-fir_monitor_0 (call=112, rc=7, cib-update=130, confirmed=true) not running
attrd: [8947]: notice: attrd_trigger_update: Sending flush op to all hosts for: probe_complete (true)
attrd: [8947]: notice: attrd_perform_update: Sent update 4: probe_complete=true

Then I cleaned up some resources running on other nodes, which resulted
in those showing up in the crm status output providing log lines like eg.:

crmd: [8949]: WARN: status_from_rc: Action 4 (vm-web5_monitor_0) on n02 failed (target: 7 vs. rc: 0): Error

Finally, I exited maintenance mode, and Pacemaker started every resource
I did not clean up beforehand, concurrently with their already running
instances:

pengine: [8948]: notice: LogActions: Start   vm-web9#011(n03)

I can provide more logs if this behavior is indeed unexpected, but it
looks more like I miss the exact concept of maintenance mode.

> The discovery usually happens at the point the cluster is started on a node.

A local discovery did happen, but it could not find anything, as the
cluster was started by the init scripts, well before any resource could
have been moved to the freshly rebooted node (manually, to free the next
node for rebooting).

> Maintenance mode just prevents the cluster from doing anything about it.

Fine.  So I should have restarted Pacemaker on each node before leaving
maintenance mode, right?  Or is there a better way?  (Unfortunately, I
could not manage the rolling reboot through Pacemaker, as some DLM/cLVM
freeze made the cluster inoperable in its normal way.)

>> but it probably did not happen, as the cluster started up resources
>> already running on other nodes, which is generally forbidden.  Given
>> that all resources were running (though possibly migrated during the
>> maintenance), what would have been the correct way of bringing the
>> cluster out of maintenance mode?  This should have required no
>> resource actions at all.  Would cleanup of all resources have helped?
>> Or is there a better way?

You say in the above thread that resource definitions can be changed:
http://thread.gmane.org/gmane.linux.highavailability.user/39121/focus=39437
Let me quote from there (starting with the words of Ulrich Windl):

>>>> I think it's a common misconception that you can modify cluster
>>>> resources while in maintenance mode:
>>> 
>>> No, you _should_ be able to.  If that's not the case, its a bug.
>> 
>> So the end of maintenance mode starts with a "re-probe"?
>
> No, but it doesn't need to.  
> The policy engine already knows if the resource definitions changed
> and the recurring monitor ops will find out if any are not running.

My experiences show that you may not *move around* resources while in
maintenance mode.  That would indeed require a cluster-wide re-probe,
which does not seem to happen (unless forced some way).  Probably there
was some misunderstanding in the above discussion, I guess Ulrich meant
moving resources when he wrote "modifying cluster resources".  Does this
make sense?
-- 
Thanks,
Feri.


From wferi@niif.hu Tue Aug 26 16:42:16 2014
Received: from int-mx14.intmail.prod.int.phx2.redhat.com
	(int-mx14.intmail.prod.int.phx2.redhat.com [10.5.11.27])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7QKgGg2026596 for <linux-cluster@listman.util.phx.redhat.com>;
	Tue, 26 Aug 2014 16:42:16 -0400
Received: from mx1.redhat.com (ext-mx11.extmail.prod.ext.phx2.redhat.com
	[10.5.110.16])
	by int-mx14.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7QKgGtS017221
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 16:42:16 -0400
Received: from listserv2.niif.hu (listserv2.niif.hu [193.225.14.155])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7QKgDXq004275
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES128-SHA bits=128 verify=NO)
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 16:42:14 -0400
Received: from business-188-142-225-206.business.broadband.hu
	([188.142.225.206] helo=lant.ki.iif.hu)
	by listserv2.niif.hu with esmtpsa (TLS1.2:DHE_RSA_AES_128_CBC_SHA1:128)
	(Exim 4.80) (envelope-from <wferi@niif.hu>) id 1XMNZE-00060m-Pt
	for linux-cluster@redhat.com; Tue, 26 Aug 2014 22:42:12 +0200
Received: from wferi by lant.ki.iif.hu with local (Exim 4.80)
	(envelope-from <wferi@lant.ki.iif.hu>) id 1XMNZ9-00034y-4Z
	for linux-cluster@redhat.com; Tue, 26 Aug 2014 22:42:07 +0200
From: Ferenc Wagner <wferi@niif.hu>
To: linux clustering <linux-cluster@redhat.com>
Date: Tue, 26 Aug 2014 22:42:07 +0200
Message-ID: <871ts39e5c.fsf@lant.ki.iif.hu>
User-Agent: Gnus/5.13 (Gnus v5.13) Emacs/23.4 (gnu/linux)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
X-RedHat-Spam-Score: -4.199  (BAYES_00,RCVD_IN_DNSWL_MED,RP_MATCHES_RCVD)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.27
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.16
X-loop: linux-cluster@redhat.com
Subject: [Linux-cluster] locating a starting resource
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Tue, 26 Aug 2014 20:42:16 -0000

Hi,

crm_resource --locate finds the hosting node of a running (successfully
started) resource just fine.  Is there a way to similarly find out the
location of a resource *being* started, ie. whose resource agent is
already running the start action, but that action is not finished yet?
-- 
Thanks,
Feri.


From andrew@beekhof.net Tue Aug 26 19:46:25 2014
Received: from int-mx10.intmail.prod.int.phx2.redhat.com
	(int-mx10.intmail.prod.int.phx2.redhat.com [10.5.11.23])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7QNkPMw006678 for <linux-cluster@listman.util.phx.redhat.com>;
	Tue, 26 Aug 2014 19:46:25 -0400
Received: from mx1.redhat.com (ext-mx16.extmail.prod.ext.phx2.redhat.com
	[10.5.110.21])
	by int-mx10.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7QNkPxd012218
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 19:46:25 -0400
Received: from out2-smtp.messagingengine.com (out2-smtp.messagingengine.com
	[66.111.4.26])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7QNkNVC018076
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256
	verify=NO)
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 19:46:24 -0400
Received: from compute3.internal (compute3.nyi.internal [10.202.2.43])
	by gateway2.nyi.internal (Postfix) with ESMTP id 1D29920873
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 19:46:23 -0400 (EDT)
Received: from frontend2 ([10.202.2.161])
	by compute3.internal (MEProxy); Tue, 26 Aug 2014 19:46:23 -0400
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=beekhof.net; h=
	content-type:subject:mime-version:from:in-reply-to:date
	:message-id:references:to; s=mesmtp; bh=7cFv7ahPDzav8AG5LM6MRTtD
	eI4=; b=fYo8Hu2dF1EU1pWZPqqXroNvpjdi1tmiMoVKDcJIXL6UGt2giXtv4xp8
	xkR5DkrpCzosxiLuUK7pq61DFzJrOlqz1p3uxl9x8aHjyuoQgHSnpPqjBtWIBqkP
	etJJRx4keGveZ5SpQA9Rb76C1VmaeblagGmfpDFWGOztBeSh5+8=
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=
	messagingengine.com; h=content-type:subject:mime-version:from
	:in-reply-to:date:message-id:references:to; s=smtpout; bh=7cFv7a
	hPDzav8AG5LM6MRTtDeI4=; b=A/YB4kpRY7LdiONhO13J/eKttHE5GzSjR/tkWw
	mGAOoT+QYKxL6Owna8zAwbpNXc6c7BWmI+A8k83hnLc1ouCNy0E6qxAEkgjc1M4M
	N3r6HChv+dcDOFMF0IxftK9xqVstpY7FBFdvQVZF9llDlL6VH89GWyD12cClZvZb
	TBP6A=
X-Sasl-enc: rWzPpp7FFgxl18UZyBaVvXAJ5yzSd0oqGZQW0AzlvDuM 1409096782
Received: from [172.16.1.5] (unknown [120.147.36.73])
	by mail.messagingengine.com (Postfix) with ESMTPA id 119DF68025E
	for <linux-cluster@redhat.com>; Tue, 26 Aug 2014 19:46:21 -0400 (EDT)
Content-Type: multipart/signed;
	boundary="Apple-Mail=_DFE6F4EF-BA08-470A-8141-D2A338C34BBD";
	protocol="application/pgp-signature"; micalg=pgp-sha512
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
From: Andrew Beekhof <andrew@beekhof.net>
In-Reply-To: <871ts39e5c.fsf@lant.ki.iif.hu>
Date: Wed, 27 Aug 2014 09:46:18 +1000
X-Mao-Original-Outgoing-Id: 430789577.368869-4039416e340ef1d4f8e3718d80c9a0fd
Message-Id: <EBFD09CA-65DD-4759-B7F3-B73039FC6B3F@beekhof.net>
References: <871ts39e5c.fsf@lant.ki.iif.hu>
To: linux clustering <linux-cluster@redhat.com>
X-RedHat-Spam-Score: -3.1  (BAYES_00, DCC_REPUT_00_12, DKIM_SIGNED, DKIM_VALID,
	DKIM_VALID_AU, RCVD_IN_DNSWL_LOW)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.23
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.21
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] locating a starting resource
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Tue, 26 Aug 2014 23:46:25 -0000


--Apple-Mail=_DFE6F4EF-BA08-470A-8141-D2A338C34BBD
Content-Transfer-Encoding: 7bit
Content-Type: text/plain;
	charset=us-ascii


On 27 Aug 2014, at 6:42 am, Ferenc Wagner <wferi@niif.hu> wrote:

> Hi,
> 
> crm_resource --locate finds the hosting node of a running (successfully
> started) resource just fine.  Is there a way to similarly find out the
> location of a resource *being* started, ie. whose resource agent is
> already running the start action, but that action is not finished yet?

You need to set record-pending=true in the op_defaults section.
For some reason this is not yet documented :-/

With this in place, crm_resource will find the correct location


> -- 
> Thanks,
> Feri.
> 
> -- 
> Linux-cluster mailing list
> Linux-cluster@redhat.com
> https://www.redhat.com/mailman/listinfo/linux-cluster


--Apple-Mail=_DFE6F4EF-BA08-470A-8141-D2A338C34BBD
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment;
	filename=signature.asc
Content-Type: application/pgp-signature;
	name=signature.asc
Content-Description: Message signed with OpenPGP using GPGMail

-----BEGIN PGP SIGNATURE-----
Comment: GPGTools - http://gpgtools.org

iQIcBAEBCgAGBQJT/RxJAAoJEBTzwpg4iwmNMhUP/RuR2tgorkHT7OV/fU6QWbJW
aYD1I34rgtnjbl3u3fcQpLEq7zl0onSVwCooV6AxI7Shz+ilfAiMDkmGkrK33ZMy
OKd3rBx9kOCYcN7fi/G+747A55bJSDDxbeWMgutS9l4R0abYBFMiuHLqj0RZPb+3
jl7twExjN/dTdZpQWXTqevXGHfdcWRux8V+kO80DLJ9K4F0izVcWoCjWSeA29G5L
pQWKZheDnNajAeObWT+g4TEbWevZ/JLtMf61TXF7z6EpB5c2NZy95xRxpDqe0v2H
EOTrlGvTffulqOvtun+ePaPTIn8/AEf0sJFV/69ZunSBPXj/CV7Jw3xJZwVzOQ0A
fPV3vr462t7Kk8QhuFtE9bLFvX8ApDJ6WrTpLMppD+ft/QmwyLom/0HwMzV8lL1O
AhiVorPbbeY9nMLc0QDWp7zOwt2fzVjwAgQ1DBLwbGL7mfZZZpH9MYzqg7zDB8T7
sXW/ktZyGVJWc5fiXhyLeTaDHXP6ozWfKSa1Ohr/LpCtx7UnDOsSTo/Rp4l9v0BZ
vd4BvnaP8DsPdMej89XzF/TpyEK4TrpqIt6AcrJdRZRqoMv42rrd3g2BZaaCkrYd
tL2olks72aFFwp23SrU56CzhYyL6Vw0jKAxaHOwTaiLvq28TUiS8CqEFUhlnKJD3
sCM6ynW4g3uwCsQs1aWx
=X4K8
-----END PGP SIGNATURE-----

--Apple-Mail=_DFE6F4EF-BA08-470A-8141-D2A338C34BBD--


From andrew@beekhof.net Wed Aug 27 00:54:45 2014
Received: from int-mx14.intmail.prod.int.phx2.redhat.com
	(int-mx14.intmail.prod.int.phx2.redhat.com [10.5.11.27])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7R4sjjI012629 for <linux-cluster@listman.util.phx.redhat.com>;
	Wed, 27 Aug 2014 00:54:45 -0400
Received: from mx1.redhat.com (ext-mx13.extmail.prod.ext.phx2.redhat.com
	[10.5.110.18])
	by int-mx14.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7R4sjAG011155
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 00:54:45 -0400
Received: from out2-smtp.messagingengine.com (out2-smtp.messagingengine.com
	[66.111.4.26])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7R4sgEr008543
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256
	verify=NO)
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 00:54:43 -0400
Received: from compute2.internal (compute2.nyi.internal [10.202.2.42])
	by gateway2.nyi.internal (Postfix) with ESMTP id 5B3AA20914
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 00:54:42 -0400 (EDT)
Received: from frontend1 ([10.202.2.160])
	by compute2.internal (MEProxy); Wed, 27 Aug 2014 00:54:42 -0400
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=beekhof.net; h=
	content-type:subject:mime-version:from:in-reply-to:date
	:message-id:references:to; s=mesmtp; bh=LVr4oO7C8AzGDM29g7UpaRgm
	mko=; b=OvOZZ+i0o/cJHVaPrAYtgq6yIJCo3uc2utdL5LokNH6MvTo/zzXJPWjm
	fYwjXnHCRFdkCvcHcN4vjNgVZUJMlGbMG/5CqmNW3Ck1Ab3SN5qUFrapAq1sC17Q
	4yWvswgaFvqePWbQcV5xosHmQlwA+bXWDlKV89InD6DTbB7Dr5E=
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=
	messagingengine.com; h=content-type:subject:mime-version:from
	:in-reply-to:date:message-id:references:to; s=smtpout; bh=LVr4oO
	7C8AzGDM29g7UpaRgmmko=; b=X0golLNZzHNXezhgKS3OKrdWYlO04dq3bCfuTM
	1kS1oxAusE7pwqhYfTX62htM71qZ2FR1wWRwG8O97MD34rkMjmRnoIUB8enFgLsi
	S0ms995NubZUah2oQa4Af4SK+27hNGY8hkX/8J1D5KdOoPSFChsREVXsQ5OI4jl8
	dS4xA=
X-Sasl-enc: fYRcX1Dk671vKSEg4zXk7xFbR/OIUz2IFVWXO808zaRK 1409115281
Received: from [172.16.1.5] (unknown [120.147.36.73])
	by mail.messagingengine.com (Postfix) with ESMTPA id 0050DC0091B
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 00:54:40 -0400 (EDT)
Content-Type: multipart/signed;
	boundary="Apple-Mail=_0B6CC477-554F-4EE4-9FEA-2CBB77437EEA";
	protocol="application/pgp-signature"; micalg=pgp-sha512
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
From: Andrew Beekhof <andrew@beekhof.net>
In-Reply-To: <87iolf9mkc.fsf@lant.ki.iif.hu>
Date: Wed, 27 Aug 2014 14:54:33 +1000
X-Mao-Original-Outgoing-Id: 430808071.527142-ed874a6411107a0e216331c9e8a4cd94
Message-Id: <F1686CC9-7920-4B72-981D-D6057EC2B754@beekhof.net>
References: <8761hlickn.fsf@lant.ki.iif.hu>
	<67506B71-8594-4C16-82C1-F94779F59826@beekhof.net>
	<87iolf9mkc.fsf@lant.ki.iif.hu>
To: linux clustering <linux-cluster@redhat.com>
X-RedHat-Spam-Score: -2.7  (BAYES_00, DKIM_SIGNED, DKIM_VALID, DKIM_VALID_AU,
	RCVD_IN_DNSWL_LOW)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.27
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.18
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] on exiting maintenance mode
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Wed, 27 Aug 2014 04:54:45 -0000


--Apple-Mail=_0B6CC477-554F-4EE4-9FEA-2CBB77437EEA
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=us-ascii


On 27 Aug 2014, at 3:40 am, Ferenc Wagner <wferi@niif.hu> wrote:

> Andrew Beekhof <andrew@beekhof.net> writes:
>=20
>> On 22 Aug 2014, at 10:37 am, Ferenc Wagner <wferi@niif.hu> wrote:
>>=20
>>> While my Pacemaker cluster was in maintenance mode, resources were =
moved
>>> (by hand) between the nodes as I rebooted each node in turn.  In the =
end
>>> the crm status output became perfectly empty, as the reboot of a =
given
>>> node removed from the output the resources which were located on the
>>> rebooted node at the time of entering maintenance mode.  I expected =
full
>>> resource discovery on exiting maintenance mode,
>>=20
>> Version and logs?
>=20
> (The more interesting part comes later, please skip to the theoretical
> part if you're short on time. :)
>=20
> I left those out, as I don't expect the actual behavior to be a bug.
> But I experienced this with Pacemaker version 1.1.7.  I know it's old

No kidding :)

> and it suffers from crmd segfault on entering maintenance mode (cf.
> http://thread.gmane.org/gmane.linux.highavailability.user/39121), but
> works well generally so I did not get to upgrade it yet.  Now that I
> mentioned the crmd segfault: I noted that it died on the DC when I
> entered maintenance mode:
>=20
> crmd: [7452]: info: te_rsc_command: Initiating action 64: cancel =
vm-tmvp_monitor_60000 on n01 (local)
> crmd: [7452]: ERROR: lrm_get_rsc(666): failed to send a getrsc message =
to lrmd via ch_cmd channel.

That looks like the lrmd died.

> crmd: [7452]: ERROR: get_lrm_resource: Could not add resource vm-tmvp =
to LRM
> crmd: [7452]: ERROR: do_lrm_invoke: Invalid resource definition
> crmd: [7452]: WARN: do_lrm_invoke: bad input <create_request_adv =
origin=3D"te_rsc_command" t=3D"crmd" version=3D"3.0.6" subt=3D"request" =
reference=3D"lrm_invoke-tengine-1408517719-30820" crm_task=3D"lrm_invoke" =
crm_sys_to=3D"lrmd" crm_sys_from=3D"tengine" crm_host_to=3D"n01" >
> crmd: [7452]: WARN: do_lrm_invoke: bad input   <crm_xml >
> crmd: [7452]: WARN: do_lrm_invoke: bad input     <rsc_op id=3D"64" =
operation=3D"cancel" operation_key=3D"vm-tmvp_monitor_60000" =
on_node=3D"n01" on_node_uuid=3D"n01" =
transition-key=3D"64:20579:0:1b0a6e79-af5a-41e4-8ced-299371e7922c" >
> crmd: [7452]: WARN: do_lrm_invoke: bad input       <primitive =
id=3D"vm-tmvp" long-id=3D"vm-tmvp" class=3D"ocf" provider=3D"niif" =
type=3D"TransientDomain" />
> crmd: [7452]: info: te_rsc_command: Initiating action 86: cancel =
vm-wfweb_monitor_60000 on n01 (local)
> crmd: [7452]: ERROR: lrm_add_rsc(870): failed to send a addrsc message =
to lrmd via ch_cmd channel.
> crmd: [7452]: ERROR: lrm_get_rsc(666): failed to send a getrsc message =
to lrmd via ch_cmd channel.
> corosync[6966]:   [pcmk  ] info: pcmk_ipc_exit: Client crmd =
(conn=3D0x1dc6ea0, async-conn=3D0x1dc6ea0) left
> pacemakerd: [7443]: WARN: Managed crmd process 7452 killed by signal =
11 [SIGSEGV - Segmentation violation].

Which created a condition in the crmd that it couldn't handle so it =
crashed too.

> pacemakerd: [7443]: notice: pcmk_child_exit: Child process crmd =
terminated with signal 11 (pid=3D7452, rc=3D0)
>=20
> However, it got restarted seamlessly, without the node being fenced, =
so
> I did not even notice this until now.  Should this have resulted in =
the
> node being fenced?

Depends how fast the node can respawn.

>=20
> But back to the issue at hand.  The Pacemaker shutdown seemed normal,
> apart from the bunch of messages like:
>=20
> crmd: [13794]: ERROR: verify_stopped: Resource vm-web5 was active at =
shutdown.  You may ignore this error if it is unmanaged.

In maintenance mode, everything is unmanaged. So that would be expected.

>=20
> appearing twice and warnings like:
>=20
> cib: [7447]: WARN: send_ipc_message: IPC Channel to 13794 is not =
connected
> cib: [7447]: WARN: send_via_callback_channel: Delivery of reply to =
client 13794/bf6f43a2-70db-40ac-a902-eabc3c12e20d failed
> cib: [7447]: WARN: do_local_notify: A-Sync reply to crmd failed: reply =
failed
> corosync[6966]:   [pcmk  ] WARN: route_ais_message: Sending message to =
local.crmd failed: ipc delivery failed (rc=3D-2)
>=20
> On reboot, corosync complained until the some Pacemaker components
> started:
>=20
> corosync[8461]:   [pcmk  ] WARN: route_ais_message: Sending message to =
local.cib failed: ipc delivery failed (rc=3D-2)
> corosync[8461]:   [pcmk  ] WARN: route_ais_message: Sending message to =
local.crmd failed: ipc delivery failed (rc=3D-2)
>=20
> Pacemaker then probed the resources on the local node (all was =
inactive):
>=20
> lrmd: [8946]: info: rsc:stonith-n01 probe[5] (pid 9081)
> lrmd: [8946]: info: rsc:dlm:0 probe[6] (pid 9082)
> [...]
> lrmd: [8946]: info: operation monitor[112] on vm-fir for client 8949: =
pid 12015 exited with return code 7
> crmd: [8949]: info: process_lrm_event: LRM operation vm-fir_monitor_0 =
(call=3D112, rc=3D7, cib-update=3D130, confirmed=3Dtrue) not running
> attrd: [8947]: notice: attrd_trigger_update: Sending flush op to all =
hosts for: probe_complete (true)
> attrd: [8947]: notice: attrd_perform_update: Sent update 4: =
probe_complete=3Dtrue
>=20
> Then I cleaned up some resources running on other nodes, which =
resulted
> in those showing up in the crm status output providing log lines like =
eg.:
>=20
> crmd: [8949]: WARN: status_from_rc: Action 4 (vm-web5_monitor_0) on =
n02 failed (target: 7 vs. rc: 0): Error
>=20
> Finally, I exited maintenance mode, and Pacemaker started every =
resource
> I did not clean up beforehand, concurrently with their already running
> instances:
>=20
> pengine: [8948]: notice: LogActions: Start   vm-web9#011(n03)
>=20
> I can provide more logs if this behavior is indeed unexpected, but it
> looks more like I miss the exact concept of maintenance mode.
>=20
>> The discovery usually happens at the point the cluster is started on =
a node.
>=20
> A local discovery did happen, but it could not find anything, as the
> cluster was started by the init scripts, well before any resource =
could
> have been moved to the freshly rebooted node (manually, to free the =
next
> node for rebooting).

Thats your problem then, you've started resources outside of the control =
of the cluster.
Two options... recurring monitor actions with role=3DStopped would have =
caught this or you can run crm_resource --cleanup after you've moved =
resources around.

>=20
>> Maintenance mode just prevents the cluster from doing anything about =
it.
>=20
> Fine.  So I should have restarted Pacemaker on each node before =
leaving
> maintenance mode, right?  Or is there a better way?

See above

>  (Unfortunately, I
> could not manage the rolling reboot through Pacemaker, as some =
DLM/cLVM
> freeze made the cluster inoperable in its normal way.)
>=20
>>> but it probably did not happen, as the cluster started up resources
>>> already running on other nodes, which is generally forbidden.  Given
>>> that all resources were running (though possibly migrated during the
>>> maintenance), what would have been the correct way of bringing the
>>> cluster out of maintenance mode?  This should have required no
>>> resource actions at all.  Would cleanup of all resources have =
helped?
>>> Or is there a better way?
>=20
> You say in the above thread that resource definitions can be changed:
> =
http://thread.gmane.org/gmane.linux.highavailability.user/39121/focus=3D39=
437
> Let me quote from there (starting with the words of Ulrich Windl):
>=20
>>>>> I think it's a common misconception that you can modify cluster
>>>>> resources while in maintenance mode:
>>>>=20
>>>> No, you _should_ be able to.  If that's not the case, its a bug.
>>>=20
>>> So the end of maintenance mode starts with a "re-probe"?
>>=20
>> No, but it doesn't need to. =20
>> The policy engine already knows if the resource definitions changed
>> and the recurring monitor ops will find out if any are not running.
>=20
> My experiences show that you may not *move around* resources while in
> maintenance mode.

Correct

>  That would indeed require a cluster-wide re-probe,
> which does not seem to happen (unless forced some way).  Probably =
there
> was some misunderstanding in the above discussion, I guess Ulrich =
meant
> moving resources when he wrote "modifying cluster resources".  Does =
this
> make sense?

No, I've reasonably sure he meant changing their definitions in the cib.
Or at least thats what I thought he meant at the time.

> --=20
> Thanks,
> Feri.
>=20
> --=20
> Linux-cluster mailing list
> Linux-cluster@redhat.com
> https://www.redhat.com/mailman/listinfo/linux-cluster


--Apple-Mail=_0B6CC477-554F-4EE4-9FEA-2CBB77437EEA
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment;
	filename=signature.asc
Content-Type: application/pgp-signature;
	name=signature.asc
Content-Description: Message signed with OpenPGP using GPGMail

-----BEGIN PGP SIGNATURE-----
Comment: GPGTools - http://gpgtools.org

iQIcBAEBCgAGBQJT/WSIAAoJEBTzwpg4iwmNPJsQAIUBZpyTx9McgIIDlwo3s6k8
Dllw97TFRCIAzQxoSFiq+bGstOhdpBl1iTcnNpSCCTUnabCJzjx6ZMpoT4MDCbDs
cU86gl0HP1VYHQKj6sJf1qE2CeyV0rYupeObqPgBuGT13gFp5J2cUwvyA8lKIt0F
pCtDTRSMrq/Vm+sqrFOdOkuIn7ITn+WIA+ecw88w/PJV2lQlD/LySq0CeObqfIqw
zmPAC9TVYu230XJrccjFJb77AL058Jt4ovF/ZCnOaDRwhBiquJ5j0o39sbj2CRg8
0MaoJ03qglT1pbQtmIdKSykQIdHDUFXlVtZpfLZ9wMR5n+7n2dgMkILOSrZhN0PM
j9s3ZZ54LvyZLmOznbP7T7fAyJ2vLVrZM41tUMJFFdpA7V2/L7jz1YJXavFfwenA
8xx46yg5psDqtmVccMPacfiPr0zlyQJEQ8/bGlVbrRc2CYGfAPTgxvtILd9LVbdV
kvQbGQHuGdj84YMzaGEFSD4UQlk964fmY1h7GKB3wN2lOJbY8kaEnDcJsAwAxqsh
aotI5FmbBcl7toFvGS4Pcs1hjiajvvRXkOEG1TN5ldcyoMQPlrrzSrAPHg5tK4G2
Qp2ujtSfkrJZxC0fYT3ycQ2x2aVRiY9qku15pZzxLXUblPw/muyTsQuJHBxRpNCn
jgD5L+gqSKAx0q6zik87
=yubI
-----END PGP SIGNATURE-----

--Apple-Mail=_0B6CC477-554F-4EE4-9FEA-2CBB77437EEA--


From wferi@niif.hu Wed Aug 27 13:09:57 2014
Received: from int-mx14.intmail.prod.int.phx2.redhat.com
	(int-mx14.intmail.prod.int.phx2.redhat.com [10.5.11.27])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7RH9v32003239 for <linux-cluster@listman.util.phx.redhat.com>;
	Wed, 27 Aug 2014 13:09:57 -0400
Received: from mx1.redhat.com (ext-mx12.extmail.prod.ext.phx2.redhat.com
	[10.5.110.17])
	by int-mx14.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7RH9v2U027478
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 13:09:57 -0400
Received: from listserv2.niif.hu (listserv2.niif.hu [193.225.14.155])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7RH9q8t027027
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES128-SHA bits=128 verify=NO)
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 13:09:54 -0400
Received: from business-188-142-225-206.business.broadband.hu
	([188.142.225.206] helo=lant.ki.iif.hu)
	by listserv2.niif.hu with esmtpsa (TLS1.2:DHE_RSA_AES_128_CBC_SHA1:128)
	(Exim 4.80) (envelope-from <wferi@niif.hu>) id 1XMgjH-0008FQ-HO
	for linux-cluster@redhat.com; Wed, 27 Aug 2014 19:09:51 +0200
Received: from wferi by lant.ki.iif.hu with local (Exim 4.80)
	(envelope-from <wferi@lant.ki.iif.hu>) id 1XMgjB-0007CR-UV
	for linux-cluster@redhat.com; Wed, 27 Aug 2014 19:09:45 +0200
From: Ferenc Wagner <wferi@niif.hu>
To: linux clustering <linux-cluster@redhat.com>
References: <8761hlickn.fsf@lant.ki.iif.hu>
	<67506B71-8594-4C16-82C1-F94779F59826@beekhof.net>
	<87iolf9mkc.fsf@lant.ki.iif.hu>
	<F1686CC9-7920-4B72-981D-D6057EC2B754@beekhof.net>
Date: Wed, 27 Aug 2014 19:09:45 +0200
In-Reply-To: <F1686CC9-7920-4B72-981D-D6057EC2B754@beekhof.net> (Andrew
	Beekhof's message of "Wed, 27 Aug 2014 14:54:33 +1000")
Message-ID: <87iold7tba.fsf@lant.ki.iif.hu>
User-Agent: Gnus/5.13 (Gnus v5.13) Emacs/23.4 (gnu/linux)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
X-RedHat-Spam-Score: -4.199  (BAYES_00,RCVD_IN_DNSWL_MED,RP_MATCHES_RCVD)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.27
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.17
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] on exiting maintenance mode
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Wed, 27 Aug 2014 17:09:58 -0000

Andrew Beekhof <andrew@beekhof.net> writes:

> On 27 Aug 2014, at 3:40 am, Ferenc Wagner <wferi@niif.hu> wrote:
>
>> Andrew Beekhof <andrew@beekhof.net> writes:
>> 
>>> On 22 Aug 2014, at 10:37 am, Ferenc Wagner <wferi@niif.hu> wrote:
>>> 
>>>> While my Pacemaker cluster was in maintenance mode, resources were moved
>>>> (by hand) between the nodes as I rebooted each node in turn.  In the end
>>>> the crm status output became perfectly empty, as the reboot of a given
>>>> node removed from the output the resources which were located on the
>>>> rebooted node at the time of entering maintenance mode.  I expected full
>>>> resource discovery on exiting maintenance mode,
>>
>> I experienced this with Pacemaker version 1.1.7.  I know it's old
>> and it suffers from crmd segfault on entering maintenance mode (cf.
>> http://thread.gmane.org/gmane.linux.highavailability.user/39121), but
>> works well generally so I did not get to upgrade it yet.  Now that I
>> mentioned the crmd segfault: I noted that it died on the DC when I
>> entered maintenance mode:
>> 
>> crmd: [7452]: info: te_rsc_command: Initiating action 64: cancel vm-tmvp_monitor_60000 on n01 (local)
>> crmd: [7452]: ERROR: lrm_get_rsc(666): failed to send a getrsc message to lrmd via ch_cmd channel.
>
> That looks like the lrmd died.

It did not die, at least not fully.  After entering maintenance mode
crmd asked lrmd to cancel the recurring monitor ops for all resources:

08:40:18 crmd: [7452]: info: do_te_invoke: Processing graph 20578 (ref=pe_calc-dc-1408516818-30681) derived from /var/lib/pengine/pe-input-848.bz2
08:40:18 crmd: [7452]: info: te_rsc_command: Initiating action 17: cancel dlm:0_monitor_120000 on n04
08:40:18 crmd: [7452]: info: te_rsc_command: Initiating action 84: cancel dlm:0_cancel_120000 on n01 (local)
08:40:18 lrmd: [7449]: info: cancel_op: operation monitor[194] on dlm:0 for client 7452, its parameters: [...] cancelled
08:40:18 crmd: [7452]: info: te_rsc_command: Initiating action 50: cancel dlm:2_monitor_120000 on n02

The stream of monitor op cancellation messages ended with:

08:40:18 crmd: [7452]: info: te_rsc_command: Initiating action 71: cancel vm-mdssq_monitor_60000 on n01 (local)
08:40:18 lrmd: [7449]: info: cancel_op: operation monitor[329] on vm-mdssq for client 7452, its parameters: [...] cancelled
08:40:18 crmd: [7452]: info: process_lrm_event: LRM operation vm-mdssq_monitor_60000 (call=329, status=1, cib-update=0, confirmed=true) Cancelled
08:40:18 crmd: [7452]: notice: run_graph: ==== Transition 20578 (Complete=87, Pending=0, Fired=0, Skipped=0, Incomplete=0, Source=/var/lib/pengine/pe-input-848.bz2): Complete
08:40:18 crmd: [7452]: notice: do_state_transition: State transition S_TRANSITION_ENGINE -> S_IDLE [ input=I_TE_SUCCESS cause=C_FSA_INTERNAL origin=notify_crmd ]
08:40:18 pengine: [7451]: notice: process_pe_message: Transition 20578: PEngine Input stored in: /var/lib/pengine/pe-input-848.bz2
08:41:28 crmd: [7452]: WARN: action_timer_callback: Timer popped (timeout=10000, abort_level=0, complete=true)
08:41:28 crmd: [7452]: WARN: action_timer_callback: Ignoring timeout while not in transition
[these two lines repeated several times]
08:41:28 crmd: [7452]: WARN: action_timer_callback: Timer popped (timeout=10000, abort_level=0, complete=true)
08:41:28 crmd: [7452]: WARN: action_timer_callback: Ignoring timeout while not in transition
08:41:38 crmd: [7452]: WARN: action_timer_callback: Timer popped (timeout=20000, abort_level=0, complete=true)
08:41:38 crmd: [7452]: WARN: action_timer_callback: Ignoring timeout while not in transition
08:48:05 cib: [7447]: info: cib_stats: Processed 159 operations (23207.00us average, 0% utilization) in the last 10min
08:55:18 crmd: [7452]: info: crm_timer_popped: PEngine Recheck Timer (I_PE_CALC) just popped (900000ms)
08:55:18 crmd: [7452]: notice: do_state_transition: State transition S_IDLE -> S_POLICY_ENGINE [ input=I_PE_CALC cause=C_TIMER_POPPED origin=crm_timer_popped ]
08:55:18 crmd: [7452]: info: do_state_transition: Progressed to state S_POLICY_ENGINE after C_TIMER_POPPED
08:55:19 pengine: [7451]: notice: stage6: Delaying fencing operations until there are resources to manage
08:55:19 crmd: [7452]: notice: do_state_transition: State transition S_POLICY_ENGINE -> S_TRANSITION_ENGINE [ input=I_PE_SUCCESS cause=C_IPC_MESSAGE origin=handle_response ]
08:55:19 crmd: [7452]: info: do_te_invoke: Processing graph 20579 (ref=pe_calc-dc-1408517718-30802) derived from /var/lib/pengine/pe-input-849.bz2
08:55:19 crmd: [7452]: info: te_rsc_command: Initiating action 17: cancel dlm:0_monitor_120000 on n04
08:55:19 crmd: [7452]: info: te_rsc_command: Initiating action 84: cancel dlm:0_cancel_120000 on n01 (local)
08:55:19 crmd: [7452]: info: cancel_op: No pending op found for dlm:0:194
08:55:19 lrmd: [7449]: info: on_msg_cancel_op: no operation with id 194

Interestingly, monitor[194], lastly mentioned by lrmd, was the very
first cancelled operation.

08:55:19 crmd: [7452]: info: te_rsc_command: Initiating action 50: cancel dlm:2_monitor_120000 on n02
08:55:19 crmd: [7452]: info: te_rsc_command: Initiating action 83: cancel vm-cedar_monitor_60000 on n01 (local)
08:55:19 crmd: [7452]: ERROR: lrm_get_rsc(673): failed to receive a reply message of getrsc.
08:55:19 crmd: [7452]: ERROR: lrm_get_rsc(666): failed to send a getrsc message to lrmd via ch_cmd channel.
08:55:19 crmd: [7452]: ERROR: lrm_add_rsc(870): failed to send a addrsc message to lrmd via ch_cmd channel.
08:55:19 crmd: [7452]: ERROR: lrm_get_rsc(666): failed to send a getrsc message to lrmd via ch_cmd channel.
08:55:19 crmd: [7452]: ERROR: get_lrm_resource: Could not add resource vm-cedar to LRM
08:55:19 crmd: [7452]: ERROR: do_lrm_invoke: Invalid resource definition
08:55:19 crmd: [7452]: WARN: do_lrm_invoke: bad input <create_request_adv origin="te_rsc_command" t="crmd" version="3.0.6" subt="request" reference="lrm_invoke-tengine-1408517719-30807" crm_task="lrm_invoke" crm_sys_to="lrmd" crm_sys_from="tengine" crm_host_to="n01" >
08:55:19 crmd: [7452]: WARN: do_lrm_invoke: bad input   <crm_xml >
08:55:19 crmd: [7452]: WARN: do_lrm_invoke: bad input     <rsc_op id="83" operation="cancel" operation_key="vm-cedar_monitor_60000" on_node="n01" on_node_uuid="n01" transition-key="83:20579:0:1b0a6e79-af5a-41e4-8ced-299371e7922c" >
08:55:19 crmd: [7452]: WARN: do_lrm_invoke: bad input       <primitive id="vm-cedar" long-id="vm-cedar" class="ocf" provider="niif" type="TransientDomain" />
08:55:19 crmd: [7452]: ERROR: log_data_element: Output truncated: available=727, needed=1374
08:55:19 crmd: [7452]: WARN: do_lrm_invoke: bad input       <attributes CRM_meta_call_id="195" [really very long]
08:55:19 crmd: [7452]: WARN: do_lrm_invoke: bad input     </rsc_op>
08:55:19 crmd: [7452]: WARN: do_lrm_invoke: bad input   </crm_xml>
08:55:19 crmd: [7452]: WARN: do_lrm_invoke: bad input </create_request_adv>

Blocks of messages like the above repeat a couple of times for other
resources, then crmd kicks the bucket and gets restarted:

08:55:19 corosync[6966]:   [pcmk  ] info: pcmk_ipc_exit: Client crmd (conn=0x1dc6ea0, async-conn=0x1dc6ea0) left
08:55:19 pacemakerd: [7443]: WARN: Managed crmd process 7452 killed by signal 11 [SIGSEGV - Segmentation violation].
08:55:19 pacemakerd: [7443]: notice: pcmk_child_exit: Child process crmd terminated with signal 11 (pid=7452, rc=0)
08:55:19 pacemakerd: [7443]: notice: pcmk_child_exit: Respawning failed child process: crmd
08:55:19 pacemakerd: [7443]: info: start_child: Forked child 13794 for process crmd
08:55:19 corosync[6966]:   [pcmk  ] WARN: route_ais_message: Sending message to local.crmd failed: ipc delivery failed (rc=-2)
08:55:19 crmd: [13794]: info: Invoked: /usr/lib/pacemaker/crmd 

Anyway, no further logs from lrmd after this point until hours later I
rebooted the machine:

14:37:06 pacemakerd: [7443]: notice: stop_child: Stopping lrmd: Sent -15 to process 7449
14:37:06 lrmd: [7449]: info: lrmd is shutting down
14:37:06 pacemakerd: [7443]: info: pcmk_child_exit: Child process lrmd exited (pid=7449, rc=0)

So lrmd was alive all the time.

> Which created a condition in the crmd that it couldn't handle so it
> crashed too.

Maybe their connection got severed somehow.

>> However, it got restarted seamlessly, without the node being fenced, so
>> I did not even notice this until now.  Should this have resulted in the
>> node being fenced?
>
> Depends how fast the node can respawn.

You mean how fast crmd can respawn?  How much time does it have to
respawn to avoid being fenced?

>> crmd: [13794]: ERROR: verify_stopped: Resource vm-web5 was active at shutdown.  You may ignore this error if it is unmanaged.
>
> In maintenance mode, everything is unmanaged. So that would be expected.

Is maintenance mode the same as unmanaging all resources?  I think the
latter does not cancel the monitor operations here...

>>> The discovery usually happens at the point the cluster is started on
>>> a node.
>> 
>> A local discovery did happen, but it could not find anything, as the
>> cluster was started by the init scripts, well before any resource could
>> have been moved to the freshly rebooted node (manually, to free the next
>> node for rebooting).
>
> Thats your problem then, you've started resources outside of the
> control of the cluster.

Some of them, yes, and moved the rest between the nodes.  All this
circumventing the cluster.

> Two options... recurring monitor actions with role=Stopped would have
> caught this

Even in maintenance mode?  Wouldn't they have been cancelled just like
the ordinary recurring monitor actions?

I guess adding them would run a recurring monitor operation for every
resource on every node, only with different expectations, right?

> or you can run crm_resource --cleanup after you've moved resources around.

I actually ran some crm resource cleanups for a couple of resources, and
those really were not started on exiting maintenance mode.

>>> Maintenance mode just prevents the cluster from doing anything about it.
>> 
>> Fine.  So I should have restarted Pacemaker on each node before leaving
>> maintenance mode, right?  Or is there a better way?
>
> See above

So crm_resource -r whatever -C is the way, for each resource separately.
Is there no way to do this for all resources at once?

>> You say in the above thread that resource definitions can be changed:
>> http://thread.gmane.org/gmane.linux.highavailability.user/39121/focus=39437
>> Let me quote from there (starting with the words of Ulrich Windl):
>> 
>>>>>> I think it's a common misconception that you can modify cluster
>>>>>> resources while in maintenance mode:
>>>>> 
>>>>> No, you _should_ be able to.  If that's not the case, its a bug.
>>>> 
>>>> So the end of maintenance mode starts with a "re-probe"?
>>> 
>>> No, but it doesn't need to.  
>>> The policy engine already knows if the resource definitions changed
>>> and the recurring monitor ops will find out if any are not running.
>> 
>> My experiences show that you may not *move around* resources while in
>> maintenance mode.
>
> Correct
>
>> That would indeed require a cluster-wide re-probe, which does not
>> seem to happen (unless forced some way).  Probably there was some
>> misunderstanding in the above discussion, I guess Ulrich meant moving
>> resources when he wrote "modifying cluster resources".  Does this
>> make sense?
>
> No, I've reasonably sure he meant changing their definitions in the cib.
> Or at least thats what I thought he meant at the time.

Nobody could blame you for that, because that's what it means.  But then
he inquired about a "re-probe", which fits more the problem of changing
the status of resources, not their definition.  Actually, I was so
firmly stuck in this mind set, that first I wanted to ask you to
reconsider, your response felt so much out of place.  That's all about
history for now...

After all this, I suggest to clarify this issue in the fine manual.
I've read it a couple of times, and still got the wrong impression.
-- 
Regards,
Feri.


From wferi@niif.hu Wed Aug 27 14:56:44 2014
Received: from int-mx11.intmail.prod.int.phx2.redhat.com
	(int-mx11.intmail.prod.int.phx2.redhat.com [10.5.11.24])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7RIuiT1029086 for <linux-cluster@listman.util.phx.redhat.com>;
	Wed, 27 Aug 2014 14:56:44 -0400
Received: from mx1.redhat.com (ext-mx13.extmail.prod.ext.phx2.redhat.com
	[10.5.110.18])
	by int-mx11.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7RIuiAx029161
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 14:56:44 -0400
Received: from listserv2.niif.hu (listserv2.niif.hu [193.225.14.155])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7RIueDL020190
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES128-SHA bits=128 verify=NO)
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 14:56:41 -0400
Received: from business-188-142-225-206.business.broadband.hu
	([188.142.225.206] helo=lant.ki.iif.hu)
	by listserv2.niif.hu with esmtpsa (TLS1.2:DHE_RSA_AES_128_CBC_SHA1:128)
	(Exim 4.80) (envelope-from <wferi@niif.hu>) id 1XMiOc-0002zw-Rg
	for linux-cluster@redhat.com; Wed, 27 Aug 2014 20:56:38 +0200
Received: from wferi by lant.ki.iif.hu with local (Exim 4.80)
	(envelope-from <wferi@lant.ki.iif.hu>) id 1XMiOW-0008Tl-CV
	for linux-cluster@redhat.com; Wed, 27 Aug 2014 20:56:32 +0200
From: Ferenc Wagner <wferi@niif.hu>
To: linux clustering <linux-cluster@redhat.com>
References: <871ts39e5c.fsf@lant.ki.iif.hu>
	<EBFD09CA-65DD-4759-B7F3-B73039FC6B3F@beekhof.net>
Date: Wed, 27 Aug 2014 20:56:32 +0200
In-Reply-To: <EBFD09CA-65DD-4759-B7F3-B73039FC6B3F@beekhof.net> (Andrew
	Beekhof's message of "Wed, 27 Aug 2014 09:46:18 +1000")
Message-ID: <877g1t7odb.fsf@lant.ki.iif.hu>
User-Agent: Gnus/5.13 (Gnus v5.13) Emacs/23.4 (gnu/linux)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
X-RedHat-Spam-Score: -4.199  (BAYES_00,RCVD_IN_DNSWL_MED,RP_MATCHES_RCVD)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.24
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.18
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] locating a starting resource
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Wed, 27 Aug 2014 18:56:44 -0000

Andrew Beekhof <andrew@beekhof.net> writes:

> On 27 Aug 2014, at 6:42 am, Ferenc Wagner <wferi@niif.hu> wrote:
>
>> crm_resource --locate finds the hosting node of a running (successfully
>> started) resource just fine.  Is there a way to similarly find out the
>> location of a resource *being* started, ie. whose resource agent is
>> already running the start action, but that action is not finished yet?
>
> You need to set record-pending=true in the op_defaults section.
> For some reason this is not yet documented :-/
>
> With this in place, crm_resource will find the correct location

I set it in a single start operation, and it works as advertised,
thanks!  At first I was suprised to see "Started" in the crm status
output while the resource was only starting, but the added order
constraint worked as expected, ie. the dependent resource started only
after the start action finished successfully.  This begs a bonus
question: how do I tell apart starting resources with record-pendig=true
and started resources?  crm_resource --locate does not help either.
-- 
Thanks,
Feri.


From andrew@beekhof.net Wed Aug 27 18:57:35 2014
Received: from int-mx13.intmail.prod.int.phx2.redhat.com
	(int-mx13.intmail.prod.int.phx2.redhat.com [10.5.11.26])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7RMvZsD009893 for <linux-cluster@listman.util.phx.redhat.com>;
	Wed, 27 Aug 2014 18:57:35 -0400
Received: from mx1.redhat.com (ext-mx13.extmail.prod.ext.phx2.redhat.com
	[10.5.110.18])
	by int-mx13.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7RMvZUB030063
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 18:57:35 -0400
Received: from out2-smtp.messagingengine.com (out2-smtp.messagingengine.com
	[66.111.4.26])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7RMvVf2022049
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256
	verify=NO)
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 18:57:32 -0400
Received: from compute5.internal (compute5.nyi.internal [10.202.2.45])
	by gateway2.nyi.internal (Postfix) with ESMTP id 6F8242093F
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 18:57:31 -0400 (EDT)
Received: from frontend1 ([10.202.2.160])
	by compute5.internal (MEProxy); Wed, 27 Aug 2014 18:57:31 -0400
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=beekhof.net; h=
	content-type:subject:mime-version:from:in-reply-to:date
	:message-id:references:to; s=mesmtp; bh=oOB5GrkNoFOF3MV4jfAd+/2M
	0No=; b=RyVN6NP31190oeX03PRodi9JvgHpeVAaPi9LMaZQBwaKsIZY57F7ZWjY
	aCkm4kr7BQo4QLrgCgD5ms/CFhqh7ojr9Ey8phr/NrRI688Jo5iMU4wYjRWKAywS
	DVFfEnwSpA/t/RhphDp1jbQskehqDJBTOY4o2CC3F+XwwJEk38k=
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=
	messagingengine.com; h=content-type:subject:mime-version:from
	:in-reply-to:date:message-id:references:to; s=smtpout; bh=oOB5Gr
	kNoFOF3MV4jfAd+/2M0No=; b=Pk4tE0U5Ujm0ydFeLukLz9VXrsfKZRSJDhoOXw
	zBx5RbVH7bAqhTjgtDxqPiyWaRh2bcb7oH4IJnKX3lKM1xoAgx0v8w4B3oGI8V/O
	lCGy9TcVOhGoHr38L4NaGzKUQOfCrfD521fDGyL4qI91UFvKZ7tNXQ6JI7uANRVO
	OHm10=
X-Sasl-enc: 7+fBmJrJA1qTkFvVXgHWScBdGEDX7aNAsnoz6r/ejPAu 1409180250
Received: from [172.16.1.5] (unknown [120.147.36.73])
	by mail.messagingengine.com (Postfix) with ESMTPA id 48121C00915
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 18:57:29 -0400 (EDT)
Content-Type: multipart/signed;
	boundary="Apple-Mail=_2E1D6BE2-4689-42AF-9228-6313297AE65F";
	protocol="application/pgp-signature"; micalg=pgp-sha512
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
From: Andrew Beekhof <andrew@beekhof.net>
In-Reply-To: <87iold7tba.fsf@lant.ki.iif.hu>
Date: Thu, 28 Aug 2014 08:57:26 +1000
X-Mao-Original-Outgoing-Id: 430873045.712283-a9f7dd994b632fdfeaffe5b6f3abd916
Message-Id: <749665C4-F970-4C43-9228-BCFD2EE1B442@beekhof.net>
References: <8761hlickn.fsf@lant.ki.iif.hu>
	<67506B71-8594-4C16-82C1-F94779F59826@beekhof.net>
	<87iolf9mkc.fsf@lant.ki.iif.hu>
	<F1686CC9-7920-4B72-981D-D6057EC2B754@beekhof.net>
	<87iold7tba.fsf@lant.ki.iif.hu>
To: linux clustering <linux-cluster@redhat.com>
X-RedHat-Spam-Score: -2.7  (BAYES_00, DKIM_SIGNED, DKIM_VALID, DKIM_VALID_AU,
	RCVD_IN_DNSWL_LOW)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.26
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.18
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] on exiting maintenance mode
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Wed, 27 Aug 2014 22:57:36 -0000


--Apple-Mail=_2E1D6BE2-4689-42AF-9228-6313297AE65F
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=us-ascii


On 28 Aug 2014, at 3:09 am, Ferenc Wagner <wferi@niif.hu> wrote:

> Andrew Beekhof <andrew@beekhof.net> writes:
>=20
>> On 27 Aug 2014, at 3:40 am, Ferenc Wagner <wferi@niif.hu> wrote:
>=20
>>> However, it got restarted seamlessly, without the node being fenced, =
so
>>> I did not even notice this until now.  Should this have resulted in =
the
>>> node being fenced?
>>=20
>> Depends how fast the node can respawn.
>=20
> You mean how fast crmd can respawn?  How much time does it have to
> respawn to avoid being fenced?

Until a new node can be elected DC, invoke the policy engine and start =
fencing.

>=20
>>> crmd: [13794]: ERROR: verify_stopped: Resource vm-web5 was active at =
shutdown.  You may ignore this error if it is unmanaged.
>>=20
>> In maintenance mode, everything is unmanaged. So that would be =
expected.
>=20
> Is maintenance mode the same as unmanaging all resources?  I think the
> latter does not cancel the monitor operations here...

Right. One cancels monitor operations too.

>=20
>>>> The discovery usually happens at the point the cluster is started =
on
>>>> a node.
>>>=20
>>> A local discovery did happen, but it could not find anything, as the
>>> cluster was started by the init scripts, well before any resource =
could
>>> have been moved to the freshly rebooted node (manually, to free the =
next
>>> node for rebooting).
>>=20
>> Thats your problem then, you've started resources outside of the
>> control of the cluster.
>=20
> Some of them, yes, and moved the rest between the nodes.  All this
> circumventing the cluster.
>=20
>> Two options... recurring monitor actions with role=3DStopped would =
have
>> caught this
>=20
> Even in maintenance mode?  Wouldn't they have been cancelled just like
> the ordinary recurring monitor actions?

Good point. Perhaps they wouldn't.

>=20
> I guess adding them would run a recurring monitor operation for every
> resource on every node, only with different expectations, right?
>=20
>> or you can run crm_resource --cleanup after you've moved resources =
around.
>=20
> I actually ran some crm resource cleanups for a couple of resources, =
and
> those really were not started on exiting maintenance mode.
>=20
>>>> Maintenance mode just prevents the cluster from doing anything =
about it.
>>>=20
>>> Fine.  So I should have restarted Pacemaker on each node before =
leaving
>>> maintenance mode, right?  Or is there a better way?
>>=20
>> See above
>=20
> So crm_resource -r whatever -C is the way, for each resource =
separately.
> Is there no way to do this for all resources at once?

I think you can just drop the -r

>=20
>>> You say in the above thread that resource definitions can be =
changed:
>>> =
http://thread.gmane.org/gmane.linux.highavailability.user/39121/focus=3D39=
437
>>> Let me quote from there (starting with the words of Ulrich Windl):
>>>=20
>>>>>>> I think it's a common misconception that you can modify cluster
>>>>>>> resources while in maintenance mode:
>>>>>>=20
>>>>>> No, you _should_ be able to.  If that's not the case, its a bug.
>>>>>=20
>>>>> So the end of maintenance mode starts with a "re-probe"?
>>>>=20
>>>> No, but it doesn't need to. =20
>>>> The policy engine already knows if the resource definitions changed
>>>> and the recurring monitor ops will find out if any are not running.
>>>=20
>>> My experiences show that you may not *move around* resources while =
in
>>> maintenance mode.
>>=20
>> Correct
>>=20
>>> That would indeed require a cluster-wide re-probe, which does not
>>> seem to happen (unless forced some way).  Probably there was some
>>> misunderstanding in the above discussion, I guess Ulrich meant =
moving
>>> resources when he wrote "modifying cluster resources".  Does this
>>> make sense?
>>=20
>> No, I've reasonably sure he meant changing their definitions in the =
cib.
>> Or at least thats what I thought he meant at the time.
>=20
> Nobody could blame you for that, because that's what it means.  But =
then
> he inquired about a "re-probe", which fits more the problem of =
changing
> the status of resources, not their definition.  Actually, I was so
> firmly stuck in this mind set, that first I wanted to ask you to
> reconsider, your response felt so much out of place.  That's all about
> history for now...
>=20
> After all this, I suggest to clarify this issue in the fine manual.
> I've read it a couple of times, and still got the wrong impression.

Which specific section do you suggest?

--Apple-Mail=_2E1D6BE2-4689-42AF-9228-6313297AE65F
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment;
	filename=signature.asc
Content-Type: application/pgp-signature;
	name=signature.asc
Content-Description: Message signed with OpenPGP using GPGMail

-----BEGIN PGP SIGNATURE-----
Comment: GPGTools - http://gpgtools.org

iQIcBAEBCgAGBQJT/mJVAAoJEBTzwpg4iwmN4akP/3huTSI0p73mw7DV71emNoeS
CnFgkDO56Oa+WJEkYAE5vPLyzOUaRm5+O4DL3qhH5Ytvc7LZPvQTgGOULEp+uQNM
OW36UqVviDJCAK+5DBI9CBcyX9ZuaOJQmWpFNMr8/Bv/axRmgUkweIexPLczJxtX
vQyVdVuBZVXT3FBVuF4AE40Q0gBM8mheWhSVg+nAPrxkPopy4YQQaVOQ1mnaoJn9
hOE7ddo0arwZD9Abr/Owh/mqndSSACvM6RfSjf95XLmWmHSBnnC4yBqZGHq6UFas
IyL+iLPN/MZ69DhgJvqUkxBz/VDt2/cE8mrOq0R9YrnHhIzJ0p7OJOgZQlO8d8vg
Km7uZYvDGWIZh/z6v05dhAobnIK+XK5oRwdQHFIckQG6EwQ3mr8R0Uit8UPK4IZq
vA3x0x7LO1/1jbIs0OjQMQzEfIHU78mZ/buhMjao7m3vw/noL83C2DyB6NCBQMkX
b6Bqm3jztXmYE9l+6jdG/u9eeRlz6WLaYbBjaysr1eARujE4JzRRDlbq31LGoNJr
rpNfhp2GgA9ZQ+nB16WyJW47eLqOKW9MDyemSXkgOuYe50jvkgGYFuSR/dC06316
uwgbkWb60YDCc7aPySBxnW+PnnLuv9J6X1KIxxVSXKy/Eldq1B/v3efAP+pWI7Qy
kQvD2RGE13Z2z9shSzCb
=OotU
-----END PGP SIGNATURE-----

--Apple-Mail=_2E1D6BE2-4689-42AF-9228-6313297AE65F--


From andrew@beekhof.net Wed Aug 27 18:58:27 2014
Received: from int-mx09.intmail.prod.int.phx2.redhat.com
	(int-mx09.intmail.prod.int.phx2.redhat.com [10.5.11.22])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7RMwRoY012628 for <linux-cluster@listman.util.phx.redhat.com>;
	Wed, 27 Aug 2014 18:58:27 -0400
Received: from mx1.redhat.com (ext-mx12.extmail.prod.ext.phx2.redhat.com
	[10.5.110.17])
	by int-mx09.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7RMwRu2025380
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 18:58:27 -0400
Received: from out2-smtp.messagingengine.com (out2-smtp.messagingengine.com
	[66.111.4.26])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7RMwPKX032709
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256
	verify=NO)
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 18:58:25 -0400
Received: from compute1.internal (compute1.nyi.internal [10.202.2.41])
	by gateway2.nyi.internal (Postfix) with ESMTP id 543E8202F0
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 18:58:25 -0400 (EDT)
Received: from frontend1 ([10.202.2.160])
	by compute1.internal (MEProxy); Wed, 27 Aug 2014 18:58:25 -0400
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=beekhof.net; h=
	content-type:subject:mime-version:from:in-reply-to:date
	:message-id:references:to; s=mesmtp; bh=/5fDtFciYGLWEldKgyKQkhU0
	Z5o=; b=kMmkyxqdF5OoRVdmb/pmtTPCDGE9PbPHpj4GvTEEuXo+AuahgWP2VcLE
	y16PKARDgf6ptpuxjl5H+8Fy24DoFyN21iYD3cGlPPf75AnPMrXa9LcmWRmPK/jD
	fhMuaxICHHfsoznzXEiW3vIJMiQj5hKUzFoQLV1+C0xlXJNDlF8=
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=
	messagingengine.com; h=content-type:subject:mime-version:from
	:in-reply-to:date:message-id:references:to; s=smtpout; bh=/5fDtF
	ciYGLWEldKgyKQkhU0Z5o=; b=hR0TWlwhGr1hqrc0QOVAFISFRm3CCZB55f86fF
	dvQcmY5R1zvc5pgY3l0EF5lKixn6j6jiZsdLAjyAjF3+mO2BA1AwCzWP3J4z+nUp
	OU0Psv8EdT0c9T7w1tuRocyAGchoAvCcsYSQIKihEttjItr/bEiezo8uG5ath/CK
	LzYZY=
X-Sasl-enc: VceHcSQJ9Q/D0k2+NaZq698Q1vQSKv1tEn5xI3C2qaND 1409180304
Received: from [172.16.1.5] (unknown [120.147.36.73])
	by mail.messagingengine.com (Postfix) with ESMTPA id 62986C00915
	for <linux-cluster@redhat.com>; Wed, 27 Aug 2014 18:58:23 -0400 (EDT)
Content-Type: multipart/signed;
	boundary="Apple-Mail=_C52C0F03-C750-46CF-BC8E-FEFD5875A82C";
	protocol="application/pgp-signature"; micalg=pgp-sha512
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
From: Andrew Beekhof <andrew@beekhof.net>
In-Reply-To: <877g1t7odb.fsf@lant.ki.iif.hu>
Date: Thu, 28 Aug 2014 08:58:23 +1000
X-Mao-Original-Outgoing-Id: 430873102.228596-8bbe89e4ef4c2b166e448d2fe87f661a
Message-Id: <9D0DD413-AB20-4F25-ADF5-02D8471EAA18@beekhof.net>
References: <871ts39e5c.fsf@lant.ki.iif.hu>
	<EBFD09CA-65DD-4759-B7F3-B73039FC6B3F@beekhof.net>
	<877g1t7odb.fsf@lant.ki.iif.hu>
To: linux clustering <linux-cluster@redhat.com>
X-RedHat-Spam-Score: -3.1  (BAYES_00, DCC_REPUT_00_12, DKIM_SIGNED, DKIM_VALID,
	DKIM_VALID_AU, RCVD_IN_DNSWL_LOW)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.22
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.17
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] locating a starting resource
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Wed, 27 Aug 2014 22:58:27 -0000


--Apple-Mail=_C52C0F03-C750-46CF-BC8E-FEFD5875A82C
Content-Transfer-Encoding: 7bit
Content-Type: text/plain;
	charset=us-ascii


On 28 Aug 2014, at 4:56 am, Ferenc Wagner <wferi@niif.hu> wrote:

> Andrew Beekhof <andrew@beekhof.net> writes:
> 
>> On 27 Aug 2014, at 6:42 am, Ferenc Wagner <wferi@niif.hu> wrote:
>> 
>>> crm_resource --locate finds the hosting node of a running (successfully
>>> started) resource just fine.  Is there a way to similarly find out the
>>> location of a resource *being* started, ie. whose resource agent is
>>> already running the start action, but that action is not finished yet?
>> 
>> You need to set record-pending=true in the op_defaults section.
>> For some reason this is not yet documented :-/
>> 
>> With this in place, crm_resource will find the correct location
> 
> I set it in a single start operation, and it works as advertised,
> thanks!  At first I was suprised to see "Started" in the crm status
> output while the resource was only starting, but the added order
> constraint worked as expected, ie. the dependent resource started only
> after the start action finished successfully.  This begs a bonus
> question: how do I tell apart starting resources with record-pendig=true
> and started resources?

I'm reasonably sure we don't expose that via crm_resource.
Seems like a reasonable thing to do though.

crm_mon /might/ show pending though.


>  crm_resource --locate does not help either.
> -- 
> Thanks,
> Feri.
> 
> -- 
> Linux-cluster mailing list
> Linux-cluster@redhat.com
> https://www.redhat.com/mailman/listinfo/linux-cluster


--Apple-Mail=_C52C0F03-C750-46CF-BC8E-FEFD5875A82C
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment;
	filename=signature.asc
Content-Type: application/pgp-signature;
	name=signature.asc
Content-Description: Message signed with OpenPGP using GPGMail

-----BEGIN PGP SIGNATURE-----
Comment: GPGTools - http://gpgtools.org

iQIcBAEBCgAGBQJT/mKOAAoJEBTzwpg4iwmNz14QAM3NcIrsyPW4B55FSwmz2U77
D6MCZSlz+QS1zc4IN7cw45khjqy9Vufgd2cwh9rSEKqsjvAPO1dNxtR2ENEH7/HB
/chM1IHkZxhPwnPtWjWXi4xIKlaaHovuNj16C77OGPTHd+78D/Q/ZgNSCRWquZOT
YET4TIeKKBIn8IXspyhcJ5GJpD9xqUfglXv0Lh/zwNISdJAcmJau6gw+CtriAt4a
wzeBgyRHxLbcT+MR9jkR/+UWFK84j5C0aNi8LtvAuzg/Sk0LrhbIlv//Ga/z++2f
ryaUAFWnc/zkNNkq5MUXRYQ8Xhky8s3zAEs8PUCWaOc4GNQfkvbyuwdE3j3oM6Xw
saLQj5CGB253u8lXgSb5gB9AONud5yCPeyG4sKnbYnrvOEpAJWLYxoPrA5zpkZM9
NP1lXX2esR+AN3JNYkFhQcyHVBYUTsfbeQdLY+m+VA/8Jj5Qz6tEPtnOXC+dg71+
CJB62BbUj8K8ImIwirkuJ0rEUqMr23zKvXaXwZFFwhOIupJdHXCkIST7hK8oBGaL
x+ujCLxWze1LBkkiynIIIRQH1yA4sqVPOnHY4SbrJyZYasjKoPsoZhdAMPhHJSS2
r6f+ULcuQGBzGeFu0BAdHIpbwz2NYZoNy2OZfsJvMKIdZFEHEK7OvIlz1cVBmoRU
6c2+/NOmttGmNTvhS1mc
=PONX
-----END PGP SIGNATURE-----

--Apple-Mail=_C52C0F03-C750-46CF-BC8E-FEFD5875A82C--


From neale@sinenomine.net Thu Aug 28 15:11:27 2014
Received: from int-mx13.intmail.prod.int.phx2.redhat.com
	(int-mx13.intmail.prod.int.phx2.redhat.com [10.5.11.26])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7SJBRXP025600 for <linux-cluster@listman.util.phx.redhat.com>;
	Thu, 28 Aug 2014 15:11:27 -0400
Received: from mx1.redhat.com (ext-mx11.extmail.prod.ext.phx2.redhat.com
	[10.5.110.16])
	by int-mx13.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7SJBQWY027796
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 15:11:26 -0400
Received: from smtp81.ord1c.emailsrvr.com (smtp81.ord1c.emailsrvr.com
	[108.166.43.81])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7SJBOoW025958
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=NO)
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 15:11:25 -0400
Received: from smtp11.relay.ord1c.emailsrvr.com (localhost.localdomain
	[127.0.0.1])
	by smtp11.relay.ord1c.emailsrvr.com (SMTP Server) with ESMTP id
	CAC70180532
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 15:11:24 -0400 (EDT)
X-SMTPDoctor-Processed: csmtpprox 2.7.1
Received: from localhost (localhost.localdomain [127.0.0.1])
	by smtp11.relay.ord1c.emailsrvr.com (SMTP Server) with ESMTP id
	C618A18052E
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 15:11:24 -0400 (EDT)
X-Virus-Scanned: OK
Received: from smtp192.mex05.mlsrvr.com (unknown [184.106.31.85])
	by smtp11.relay.ord1c.emailsrvr.com (SMTP Server) with ESMTPS id
	AF3DD180533
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 15:11:24 -0400 (EDT)
Received: from ORD2MBX02F.mex05.mlsrvr.com ([fe80::92e2:baff:fe11:e744]) by
	ORD2HUB33.mex05.mlsrvr.com ([::1]) with mapi id 14.03.0169.001;
	Thu, 28 Aug 2014 14:11:24 -0500
From: Neale Ferguson <neale@sinenomine.net>
To: linux clustering <linux-cluster@redhat.com>
Thread-Topic: Delaying fencing during shutdown
Thread-Index: AQHPwvPcVqNV8nY850K87bEJVRucgQ==
Date: Thu, 28 Aug 2014 19:11:24 +0000
Message-ID: <0E22A6F6-977A-4E58-A0ED-9D596D6B1A20@sinenomine.net>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
x-originating-ip: [96.247.193.74]
Content-Type: text/plain; charset="us-ascii"
Content-ID: <389D3249065C9D4FA01F724CB3611D53@mex05.mlsrvr.com>
MIME-Version: 1.0
X-RedHat-Spam-Score: -2.311  (BAYES_00, DCC_REPUT_00_12, RCVD_IN_DNSWL_NONE,
	SPF_PASS)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.26
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.16
Content-Transfer-Encoding: 8bit
X-MIME-Autoconverted: from quoted-printable to 8bit by
	lists01.pubmisc.prod.ext.phx2.redhat.com id s7SJBRXP025600
X-loop: linux-cluster@redhat.com
Subject: [Linux-cluster] Delaying fencing during shutdown
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Thu, 28 Aug 2014 19:11:27 -0000

Hi,
 In a two node cluster I shutdown one of the nodes and the other node notices the shutdown but on rare occasions that node will then fence the node that is shutting down. I assume Is this a situation where setting post_fail_delay would be useful or setting the totem timeout to something higher than its default.

Neale


From wferi@niif.hu Thu Aug 28 19:00:35 2014
Received: from int-mx14.intmail.prod.int.phx2.redhat.com
	(int-mx14.intmail.prod.int.phx2.redhat.com [10.5.11.27])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7SN0ZX1030083 for <linux-cluster@listman.util.phx.redhat.com>;
	Thu, 28 Aug 2014 19:00:35 -0400
Received: from mx1.redhat.com (ext-mx15.extmail.prod.ext.phx2.redhat.com
	[10.5.110.20])
	by int-mx14.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7SN0ZX8016714
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 19:00:35 -0400
Received: from listserv2.niif.hu (listserv2.niif.hu [193.225.14.155])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7SN0VKr018933
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES128-SHA bits=128 verify=NO)
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 19:00:33 -0400
Received: from business-188-142-225-206.business.broadband.hu
	([188.142.225.206] helo=lant.ki.iif.hu)
	by listserv2.niif.hu with esmtpsa (TLS1.2:DHE_RSA_AES_128_CBC_SHA1:128)
	(Exim 4.80) (envelope-from <wferi@niif.hu>) id 1XN8gA-0001gV-E0
	for linux-cluster@redhat.com; Fri, 29 Aug 2014 01:00:30 +0200
Received: from wferi by lant.ki.iif.hu with local (Exim 4.80)
	(envelope-from <wferi@lant.ki.iif.hu>) id 1XN8g4-000679-N0
	for linux-cluster@redhat.com; Fri, 29 Aug 2014 01:00:24 +0200
From: Ferenc Wagner <wferi@niif.hu>
To: linux clustering <linux-cluster@redhat.com>
References: <871ts39e5c.fsf@lant.ki.iif.hu>
	<EBFD09CA-65DD-4759-B7F3-B73039FC6B3F@beekhof.net>
	<877g1t7odb.fsf@lant.ki.iif.hu>
	<9D0DD413-AB20-4F25-ADF5-02D8471EAA18@beekhof.net>
Date: Fri, 29 Aug 2014 01:00:24 +0200
In-Reply-To: <9D0DD413-AB20-4F25-ADF5-02D8471EAA18@beekhof.net> (Andrew
	Beekhof's message of "Thu, 28 Aug 2014 08:58:23 +1000")
Message-ID: <87sikgb4on.fsf@lant.ki.iif.hu>
User-Agent: Gnus/5.13 (Gnus v5.13) Emacs/23.4 (gnu/linux)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
X-RedHat-Spam-Score: -4.199  (BAYES_00,RCVD_IN_DNSWL_MED,RP_MATCHES_RCVD)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.27
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.20
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] locating a starting resource
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Thu, 28 Aug 2014 23:00:35 -0000

Andrew Beekhof <andrew@beekhof.net> writes:

> On 28 Aug 2014, at 4:56 am, Ferenc Wagner <wferi@niif.hu> wrote:
>
>> Andrew Beekhof <andrew@beekhof.net> writes:
>> 
>>> On 27 Aug 2014, at 6:42 am, Ferenc Wagner <wferi@niif.hu> wrote:
>>> 
>>>> crm_resource --locate finds the hosting node of a running (successfully
>>>> started) resource just fine.  Is there a way to similarly find out the
>>>> location of a resource *being* started, ie. whose resource agent is
>>>> already running the start action, but that action is not finished yet?
>>> 
>>> You need to set record-pending=true in the op_defaults section.
>>> For some reason this is not yet documented :-/
>>> 
>>> With this in place, crm_resource will find the correct location
>> 
>> I set it in a single start operation, and it works as advertised,
>> thanks!  At first I was suprised to see "Started" in the crm status
>> output while the resource was only starting, but the added order
>> constraint worked as expected, ie. the dependent resource started only
>> after the start action finished successfully.  This begs a bonus
>> question: how do I tell apart starting resources with record-pendig=true
>> and started resources?
>
> I'm reasonably sure we don't expose that via crm_resource.
> Seems like a reasonable thing to do though.
>
> crm_mon /might/ show pending though.

Version 1.1.7 does not.  Looks like call-id="-1" signs the pending
operations of an <lrm_resource>, so pulling this info out of the CIB
is not too complicated.
-- 
Regards,
Feri.


From wferi@niif.hu Thu Aug 28 20:55:08 2014
Received: from int-mx14.intmail.prod.int.phx2.redhat.com
	(int-mx14.intmail.prod.int.phx2.redhat.com [10.5.11.27])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7T0t7vm007876 for <linux-cluster@listman.util.phx.redhat.com>;
	Thu, 28 Aug 2014 20:55:07 -0400
Received: from mx1.redhat.com (ext-mx14.extmail.prod.ext.phx2.redhat.com
	[10.5.110.19])
	by int-mx14.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7T0t7AN024149
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 20:55:07 -0400
Received: from listserv2.niif.hu (listserv2.niif.hu [193.225.14.155])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7T0t4f3016282
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES128-SHA bits=128 verify=NO)
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 20:55:05 -0400
Received: from business-188-142-225-206.business.broadband.hu
	([188.142.225.206] helo=lant.ki.iif.hu)
	by listserv2.niif.hu with esmtpsa (TLS1.2:DHE_RSA_AES_128_CBC_SHA1:128)
	(Exim 4.80) (envelope-from <wferi@niif.hu>) id 1XNAT1-0006MW-Ak
	for linux-cluster@redhat.com; Fri, 29 Aug 2014 02:55:03 +0200
Received: from wferi by lant.ki.iif.hu with local (Exim 4.80)
	(envelope-from <wferi@lant.ki.iif.hu>) id 1XNASv-0008OW-Gv
	for linux-cluster@redhat.com; Fri, 29 Aug 2014 02:54:57 +0200
From: Ferenc Wagner <wferi@niif.hu>
To: linux clustering <linux-cluster@redhat.com>
References: <8761hlickn.fsf@lant.ki.iif.hu>
	<67506B71-8594-4C16-82C1-F94779F59826@beekhof.net>
	<87iolf9mkc.fsf@lant.ki.iif.hu>
	<F1686CC9-7920-4B72-981D-D6057EC2B754@beekhof.net>
	<87iold7tba.fsf@lant.ki.iif.hu>
	<749665C4-F970-4C43-9228-BCFD2EE1B442@beekhof.net>
Date: Fri, 29 Aug 2014 02:54:57 +0200
In-Reply-To: <749665C4-F970-4C43-9228-BCFD2EE1B442@beekhof.net> (Andrew
	Beekhof's message of "Thu, 28 Aug 2014 08:57:26 +1000")
Message-ID: <87oav4azdq.fsf@lant.ki.iif.hu>
User-Agent: Gnus/5.13 (Gnus v5.13) Emacs/23.4 (gnu/linux)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
X-RedHat-Spam-Score: -4.199  (BAYES_00,RCVD_IN_DNSWL_MED,RP_MATCHES_RCVD)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.27
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.19
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] on exiting maintenance mode
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Fri, 29 Aug 2014 00:55:08 -0000

Andrew Beekhof <andrew@beekhof.net> writes:

> On 28 Aug 2014, at 3:09 am, Ferenc Wagner <wferi@niif.hu> wrote:
>
>> So crm_resource -r whatever -C is the way, for each resource separately.
>> Is there no way to do this for all resources at once?
>
> I think you can just drop the -r

Unfortunately, that does not work under version 1.1.7:

$ sudo crm_resource -C
Error performing operation: The object/attribute does not exist

>> Andrew Beekhof <andrew@beekhof.net> writes:
>> 
>>> On 27 Aug 2014, at 3:40 am, Ferenc Wagner <wferi@niif.hu> wrote:
>>> 
>>>> My experiences show that you may not *move around* resources while in
>>>> maintenance mode.
>>> 
>>> Correct
>>> 
>>>> That would indeed require a cluster-wide re-probe, which does not
>>>> seem to happen (unless forced some way).
>> 
>> After all this, I suggest to clarify this issue in the fine manual.
>> I've read it a couple of times, and still got the wrong impression.
>
> Which specific section do you suggest?

5.7.1. Monitoring Resources for Failure

Some points worth adding/emphasizing would be:
1. documentation of the role property (role=Master is mentioned later,
   but role=Stopped never)
2. In maintenance mode, monitor operations don't run
3. If management of a resource is switched off, its role=Started monitor
   operation continues running until failure, then the role=Stopped
   kicks in (I'm guessing here; also, what about the other nodes?)
4. When management is enabled again, no re-probe happens, the cluster
   expects the last state and location to be still valid
5. so don't even move unmanaged resources
6. unless you started a resource somewhere before starting the cluster
   on that node, or you cleaned up the resource
7. same is true for maintenance mode, but for all resources.

I have to agree that most of this is evident once you know it.
Unfortunately, it's also easy to get wrong while learning the ropes.
For example, hastexo has some good information online:
http://www.hastexo.com/resources/hints-and-kinks/maintenance-active-pacemaker-clusters
But from the sentence "in maintenance mode, you can stop or restart
cluster resources at will" I still miss the constraint of not moving the
resource between the nodes.  Also, setting enabled="false" works funny,
it did not get rid of the monitor operation before I set the resource to
managed, and deleting the setting or changing it to true did bring it
back.  I had to restart the resource to have monitor ops again.  Why?
-- 
Thanks,
Feri.


From wferi@niif.hu Thu Aug 28 20:57:59 2014
Received: from int-mx14.intmail.prod.int.phx2.redhat.com
	(int-mx14.intmail.prod.int.phx2.redhat.com [10.5.11.27])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7T0vxer007626 for <linux-cluster@listman.util.phx.redhat.com>;
	Thu, 28 Aug 2014 20:57:59 -0400
Received: from mx1.redhat.com (ext-mx14.extmail.prod.ext.phx2.redhat.com
	[10.5.110.19])
	by int-mx14.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7T0vxxZ025662
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 20:57:59 -0400
Received: from listserv2.niif.hu (listserv2.niif.hu [193.225.14.155])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7T0vuH2017705
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES128-SHA bits=128 verify=NO)
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 20:57:58 -0400
Received: from business-188-142-225-206.business.broadband.hu
	([188.142.225.206] helo=lant.ki.iif.hu)
	by listserv2.niif.hu with esmtpsa (TLS1.2:DHE_RSA_AES_128_CBC_SHA1:128)
	(Exim 4.80) (envelope-from <wferi@niif.hu>) id 1XNAVo-0006UT-HQ
	for linux-cluster@redhat.com; Fri, 29 Aug 2014 02:57:56 +0200
Received: from wferi by lant.ki.iif.hu with local (Exim 4.80)
	(envelope-from <wferi@lant.ki.iif.hu>) id 1XNAVi-0008S0-OZ
	for linux-cluster@redhat.com; Fri, 29 Aug 2014 02:57:50 +0200
From: Ferenc Wagner <wferi@niif.hu>
To: linux clustering <linux-cluster@redhat.com>
References: <871ts39e5c.fsf@lant.ki.iif.hu>
	<EBFD09CA-65DD-4759-B7F3-B73039FC6B3F@beekhof.net>
	<877g1t7odb.fsf@lant.ki.iif.hu>
	<9D0DD413-AB20-4F25-ADF5-02D8471EAA18@beekhof.net>
Date: Fri, 29 Aug 2014 02:57:50 +0200
In-Reply-To: <9D0DD413-AB20-4F25-ADF5-02D8471EAA18@beekhof.net> (Andrew
	Beekhof's message of "Thu, 28 Aug 2014 08:58:23 +1000")
Message-ID: <87ha0waz8x.fsf@lant.ki.iif.hu>
User-Agent: Gnus/5.13 (Gnus v5.13) Emacs/23.4 (gnu/linux)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
X-RedHat-Spam-Score: -4.199  (BAYES_00,RCVD_IN_DNSWL_MED,RP_MATCHES_RCVD)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.27
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.19
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] locating a starting resource
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Fri, 29 Aug 2014 00:57:59 -0000

Andrew Beekhof <andrew@beekhof.net> writes:

> On 28 Aug 2014, at 4:56 am, Ferenc Wagner <wferi@niif.hu> wrote:
>
>> Andrew Beekhof <andrew@beekhof.net> writes:
>> 
>>> On 27 Aug 2014, at 6:42 am, Ferenc Wagner <wferi@niif.hu> wrote:
>>> 
>>>> crm_resource --locate finds the hosting node of a running (successfully
>>>> started) resource just fine.  Is there a way to similarly find out the
>>>> location of a resource *being* started, ie. whose resource agent is
>>>> already running the start action, but that action is not finished yet?
>>> 
>>> You need to set record-pending=true in the op_defaults section.
>>> For some reason this is not yet documented :-/
>>> 
>>> With this in place, crm_resource will find the correct location
>> 
>> I set it in a single start operation, and it works as advertised,
>> thanks!  At first I was suprised to see "Started" in the crm status
>> output while the resource was only starting, but the added order
>> constraint worked as expected, ie. the dependent resource started only
>> after the start action finished successfully.  This begs a bonus
>> question: how do I tell apart starting resources with record-pendig=true
>> and started resources?
>
> I'm reasonably sure we don't expose that via crm_resource.
> Seems like a reasonable thing to do though.

crm_resource -O outputs lines like this:
[...] Started : vm-elm_start_0 (node=lant, call=-1, rc=14): pending
which seems good enough for now.
-- 
Thanks,
Feri.


From andrew@beekhof.net Thu Aug 28 22:31:43 2014
Received: from int-mx14.intmail.prod.int.phx2.redhat.com
	(int-mx14.intmail.prod.int.phx2.redhat.com [10.5.11.27])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7T2Vhsp031861 for <linux-cluster@listman.util.phx.redhat.com>;
	Thu, 28 Aug 2014 22:31:43 -0400
Received: from mx1.redhat.com (ext-mx12.extmail.prod.ext.phx2.redhat.com
	[10.5.110.17])
	by int-mx14.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7T2VhiB030865
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 22:31:43 -0400
Received: from out1-smtp.messagingengine.com (out1-smtp.messagingengine.com
	[66.111.4.25])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7T2VfZD019942
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256
	verify=NO)
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 22:31:42 -0400
Received: from compute2.internal (compute2.nyi.internal [10.202.2.42])
	by gateway2.nyi.internal (Postfix) with ESMTP id 0B26820FDC
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 22:31:41 -0400 (EDT)
Received: from frontend2 ([10.202.2.161])
	by compute2.internal (MEProxy); Thu, 28 Aug 2014 22:31:41 -0400
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=beekhof.net; h=
	content-type:subject:mime-version:from:in-reply-to:date
	:message-id:references:to; s=mesmtp; bh=AltXKMWUIJOpAeOhZ11rHg3Y
	rK4=; b=A7RB+PXYlaoweuDUvZjEkvcUEsOe0a/dsg7bi1uET+YKiHeSnQYfldrL
	iHcNSDpH94w2vjEozhSfovRbSQqSOwrzU0NRsEyrlisog+OqpHCVT0FqiUOZaTiC
	J9A7En5mtty+BaFG5lSJaoafEMXESpU+UngLy0rF7ZxG5Piidcg=
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=
	messagingengine.com; h=content-type:subject:mime-version:from
	:in-reply-to:date:message-id:references:to; s=smtpout; bh=AltXKM
	WUIJOpAeOhZ11rHg3YrK4=; b=rB8fxgCmWaBjTQA9vl2mW573MDM+NT6T6BzpET
	iIsIK1wG41LK2lcOMh89ZEf9fqvY8a4K0u/h/oQoERKsTtx6D/dZVxpK9zIQ6nMH
	a4huO8JpwichRTU8sVgmzcogve06qS9jnQN4S2T7MXAScgb1BtKYhtdthaDhkPeP
	JVlz4=
X-Sasl-enc: 5BoLpHCkbSKYPUp0//TnMEXMT45tTVK6KTdZc4zH3aDh 1409279500
Received: from [172.16.1.5] (unknown [120.147.36.73])
	by mail.messagingengine.com (Postfix) with ESMTPA id E17E76800AE
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 22:31:39 -0400 (EDT)
Content-Type: multipart/signed;
	boundary="Apple-Mail=_78114DBC-6CD7-4DA5-99DA-2BAD0B09CD04";
	protocol="application/pgp-signature"; micalg=pgp-sha512
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
From: Andrew Beekhof <andrew@beekhof.net>
In-Reply-To: <87ha0waz8x.fsf@lant.ki.iif.hu>
Date: Fri, 29 Aug 2014 12:31:36 +1000
X-Mao-Original-Outgoing-Id: 430972295.366025-ba6a479a86622422c04446c04181ddc7
Message-Id: <77CDE52A-401F-4851-ABFB-3A643F9913CD@beekhof.net>
References: <871ts39e5c.fsf@lant.ki.iif.hu>
	<EBFD09CA-65DD-4759-B7F3-B73039FC6B3F@beekhof.net>
	<877g1t7odb.fsf@lant.ki.iif.hu>
	<9D0DD413-AB20-4F25-ADF5-02D8471EAA18@beekhof.net>
	<87ha0waz8x.fsf@lant.ki.iif.hu>
To: linux clustering <linux-cluster@redhat.com>
X-RedHat-Spam-Score: -3.1  (BAYES_00, DCC_REPUT_00_12, DKIM_SIGNED, DKIM_VALID,
	DKIM_VALID_AU, RCVD_IN_DNSWL_LOW)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.27
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.17
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] locating a starting resource
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Fri, 29 Aug 2014 02:31:45 -0000


--Apple-Mail=_78114DBC-6CD7-4DA5-99DA-2BAD0B09CD04
Content-Transfer-Encoding: 7bit
Content-Type: text/plain;
	charset=us-ascii


On 29 Aug 2014, at 10:57 am, Ferenc Wagner <wferi@niif.hu> wrote:

> Andrew Beekhof <andrew@beekhof.net> writes:
> 
>> On 28 Aug 2014, at 4:56 am, Ferenc Wagner <wferi@niif.hu> wrote:
>> 
>>> Andrew Beekhof <andrew@beekhof.net> writes:
>>> 
>>>> On 27 Aug 2014, at 6:42 am, Ferenc Wagner <wferi@niif.hu> wrote:
>>>> 
>>>>> crm_resource --locate finds the hosting node of a running (successfully
>>>>> started) resource just fine.  Is there a way to similarly find out the
>>>>> location of a resource *being* started, ie. whose resource agent is
>>>>> already running the start action, but that action is not finished yet?
>>>> 
>>>> You need to set record-pending=true in the op_defaults section.
>>>> For some reason this is not yet documented :-/
>>>> 
>>>> With this in place, crm_resource will find the correct location
>>> 
>>> I set it in a single start operation, and it works as advertised,
>>> thanks!  At first I was suprised to see "Started" in the crm status
>>> output while the resource was only starting, but the added order
>>> constraint worked as expected, ie. the dependent resource started only
>>> after the start action finished successfully.  This begs a bonus
>>> question: how do I tell apart starting resources with record-pendig=true
>>> and started resources?
>> 
>> I'm reasonably sure we don't expose that via crm_resource.
>> Seems like a reasonable thing to do though.
> 
> crm_resource -O outputs lines like this:
> [...] Started : vm-elm_start_0 (node=lant, call=-1, rc=14): pending
> which seems good enough for now.
> -- 

More recent versions also have:

       -j, --pending
              Display pending state if 'record-pending' is enabled



--Apple-Mail=_78114DBC-6CD7-4DA5-99DA-2BAD0B09CD04
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment;
	filename=signature.asc
Content-Type: application/pgp-signature;
	name=signature.asc
Content-Description: Message signed with OpenPGP using GPGMail

-----BEGIN PGP SIGNATURE-----
Comment: GPGTools - http://gpgtools.org

iQIcBAEBCgAGBQJT/+YHAAoJEBTzwpg4iwmNZQEP/iI2ZJAU5b+DASpujBIJtVFf
6PRsYYLOtzFHEDckefeBS09f1c1Mcy7+JgE8YK+hZGmdpdnTl3d2dLoKQvEUF27I
a+6VqbnJLsMLF95BY7TI6Z9H5zdN7qq/H7bdZnROv3uYPjX4/6O7hHmbfgLYIi8C
/WOx7OPRNf6Zd5U3nYVpwuGHMhSJfcqjvvAOGTbpnJpXeGSO9rZ1yqgAxfpRi4N3
xlOdIu9IrefsZXFUTNCGop50wtew8hILxWRxToTBEd1TcvX9hkOU+bZWY3uaMWgr
j6KV1mU8EJuD+7+/5wZ//67q03OLIt2euHZmCNPeXvADhDzO6R4wKArFntOAqPY7
u6VErMzqNBp3geZFt/PPCHTdaeyqj/FGi3JJzEhDVrT8RUSxDISG+MMkr7iNc6cE
gqTcyZJXz4C+GKmAYqDP53X1Ru99ZoHMRxePB0ON7ksw2CU0Iv7A2OrWbKgpepSC
dCbJQDSRVU/kMkw128IM+q6U7llspQFL0TzTEdQX/kRqg9iLcT7QoVwbu2vYWY3q
9Yz9zQN9ww9HRjmE91uqaMNriWJNPLI1E4kWS6pY1BrnZKrivQxV98CqdrmMAWyM
Gfi7wT/NjNPUeY0sMAWNjq6WGnhQQ+HDESqBSfFpricwcZ2aB7iM8C5u4nY9LAhc
vkBldu+vYkMHcPhvpZQa
=kQPo
-----END PGP SIGNATURE-----

--Apple-Mail=_78114DBC-6CD7-4DA5-99DA-2BAD0B09CD04--


From andrew@beekhof.net Thu Aug 28 22:32:57 2014
Received: from int-mx13.intmail.prod.int.phx2.redhat.com
	(int-mx13.intmail.prod.int.phx2.redhat.com [10.5.11.26])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7T2Wv36014169 for <linux-cluster@listman.util.phx.redhat.com>;
	Thu, 28 Aug 2014 22:32:57 -0400
Received: from mx1.redhat.com (ext-mx11.extmail.prod.ext.phx2.redhat.com
	[10.5.110.16])
	by int-mx13.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7T2WvQ5020641
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 22:32:57 -0400
Received: from out1-smtp.messagingengine.com (out1-smtp.messagingengine.com
	[66.111.4.25])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7T2WtqF024534
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256
	verify=NO)
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 22:32:55 -0400
Received: from compute1.internal (compute1.nyi.internal [10.202.2.41])
	by gateway2.nyi.internal (Postfix) with ESMTP id E7BCE20DF0
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 22:32:54 -0400 (EDT)
Received: from frontend1 ([10.202.2.160])
	by compute1.internal (MEProxy); Thu, 28 Aug 2014 22:32:54 -0400
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=beekhof.net; h=
	content-type:subject:mime-version:from:in-reply-to:date
	:message-id:references:to; s=mesmtp; bh=KbSO10OzWnlQYmYC98xUha5R
	UHU=; b=gZBmdOQj048xaS2VBLrZhhTFRIe5YEhYHg0ePyVb8J6qRmWuKwMxUsDb
	Lw/l5WGvMBlNd3mA9gy6OTrrCcBVOu1BxZDz1uS/FBgL+FIT6ylqWsTse9LhInmh
	kFnLMapdN2rLhccpjLdlEvmvtxyGyRU+wLrhIxpPcBHgd1zLna8=
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=
	messagingengine.com; h=content-type:subject:mime-version:from
	:in-reply-to:date:message-id:references:to; s=smtpout; bh=KbSO10
	OzWnlQYmYC98xUha5RUHU=; b=iNPVV2GQJjIoQCQREZCDHW2DRL847nyK4LvOnx
	5AiCN83c4LcE8yjTgyxvKMxINEpFtjMuUv+EKBZskDEgr+k/0y53czXKDVXN6JTU
	iLuO/LjEsir4YvPzO75R0iakR1u+NWpJsA5t75TqR9127pIOnhpWtVgBBs/rut7E
	dm41I=
X-Sasl-enc: QVu0PWMzVIqumM1NvvcbWIbsYTtC92KrHcZD8lw2rznh 1409279574
Received: from [172.16.1.5] (unknown [120.147.36.73])
	by mail.messagingengine.com (Postfix) with ESMTPA id C9880C00915
	for <linux-cluster@redhat.com>; Thu, 28 Aug 2014 22:32:53 -0400 (EDT)
Content-Type: multipart/signed;
	boundary="Apple-Mail=_FA81BDE8-0B0F-45E1-9630-C6BEEE3A84BB";
	protocol="application/pgp-signature"; micalg=pgp-sha512
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
From: Andrew Beekhof <andrew@beekhof.net>
In-Reply-To: <87oav4azdq.fsf@lant.ki.iif.hu>
Date: Fri, 29 Aug 2014 12:32:50 +1000
X-Mao-Original-Outgoing-Id: 430972370.225842-2dc111f357f7b562b84b389df38c063e
Message-Id: <B9EF793F-8CCC-4AA7-82C9-CFDD64EC9537@beekhof.net>
References: <8761hlickn.fsf@lant.ki.iif.hu>
	<67506B71-8594-4C16-82C1-F94779F59826@beekhof.net>
	<87iolf9mkc.fsf@lant.ki.iif.hu>
	<F1686CC9-7920-4B72-981D-D6057EC2B754@beekhof.net>
	<87iold7tba.fsf@lant.ki.iif.hu>
	<749665C4-F970-4C43-9228-BCFD2EE1B442@beekhof.net>
	<87oav4azdq.fsf@lant.ki.iif.hu>
To: linux clustering <linux-cluster@redhat.com>
X-RedHat-Spam-Score: -3.1  (BAYES_00, DCC_REPUT_00_12, DKIM_SIGNED, DKIM_VALID,
	DKIM_VALID_AU, RCVD_IN_DNSWL_LOW)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.26
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.16
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] on exiting maintenance mode
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Fri, 29 Aug 2014 02:32:57 -0000


--Apple-Mail=_FA81BDE8-0B0F-45E1-9630-C6BEEE3A84BB
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=us-ascii


On 29 Aug 2014, at 10:54 am, Ferenc Wagner <wferi@niif.hu> wrote:

> Andrew Beekhof <andrew@beekhof.net> writes:
>=20
>> On 28 Aug 2014, at 3:09 am, Ferenc Wagner <wferi@niif.hu> wrote:
>>=20
>>> So crm_resource -r whatever -C is the way, for each resource =
separately.
>>> Is there no way to do this for all resources at once?
>>=20
>> I think you can just drop the -r
>=20
> Unfortunately, that does not work under version 1.1.7:

You know what I'm going to say here right?

>=20
> $ sudo crm_resource -C
> Error performing operation: The object/attribute does not exist
>=20
>>> Andrew Beekhof <andrew@beekhof.net> writes:
>>>=20
>>>> On 27 Aug 2014, at 3:40 am, Ferenc Wagner <wferi@niif.hu> wrote:
>>>>=20
>>>>> My experiences show that you may not *move around* resources while =
in
>>>>> maintenance mode.
>>>>=20
>>>> Correct
>>>>=20
>>>>> That would indeed require a cluster-wide re-probe, which does not
>>>>> seem to happen (unless forced some way).
>>>=20
>>> After all this, I suggest to clarify this issue in the fine manual.
>>> I've read it a couple of times, and still got the wrong impression.
>>=20
>> Which specific section do you suggest?
>=20
> 5.7.1. Monitoring Resources for Failure

Ok, I'll endeavour to improve that section :)

>=20
> Some points worth adding/emphasizing would be:
> 1. documentation of the role property (role=3DMaster is mentioned =
later,
>   but role=3DStopped never)
> 2. In maintenance mode, monitor operations don't run
> 3. If management of a resource is switched off, its role=3DStarted =
monitor
>   operation continues running until failure, then the role=3DStopped
>   kicks in (I'm guessing here; also, what about the other nodes?)
> 4. When management is enabled again, no re-probe happens, the cluster
>   expects the last state and location to be still valid
> 5. so don't even move unmanaged resources
> 6. unless you started a resource somewhere before starting the cluster
>   on that node, or you cleaned up the resource
> 7. same is true for maintenance mode, but for all resources.
>=20
> I have to agree that most of this is evident once you know it.
> Unfortunately, it's also easy to get wrong while learning the ropes.
> For example, hastexo has some good information online:
> =
http://www.hastexo.com/resources/hints-and-kinks/maintenance-active-pacema=
ker-clusters
> But from the sentence "in maintenance mode, you can stop or restart
> cluster resources at will" I still miss the constraint of not moving =
the
> resource between the nodes.  Also, setting enabled=3D"false" works =
funny,
> it did not get rid of the monitor operation before I set the resource =
to
> managed, and deleting the setting or changing it to true did bring it
> back.  I had to restart the resource to have monitor ops again.  Why?
> --=20
> Thanks,
> Feri.
>=20
> --=20
> Linux-cluster mailing list
> Linux-cluster@redhat.com
> https://www.redhat.com/mailman/listinfo/linux-cluster


--Apple-Mail=_FA81BDE8-0B0F-45E1-9630-C6BEEE3A84BB
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment;
	filename=signature.asc
Content-Type: application/pgp-signature;
	name=signature.asc
Content-Description: Message signed with OpenPGP using GPGMail

-----BEGIN PGP SIGNATURE-----
Comment: GPGTools - http://gpgtools.org

iQIcBAEBCgAGBQJT/+ZSAAoJEBTzwpg4iwmNiCQP/3v7xK9u45dpWN1bwQrsDXPb
IHilG5BVK9+SULkmttJk9ozvDhu67p3NGXwFlAKK3YXh+6ifT7LU2HO30v8nF8wn
YCCl+tWqMZCGQHjtkD3iHHYo8EEi0nWc7qgG3A0Spv91uBwYe6ahr4jwNXjvHf/i
UVqc5LVqjQMmCgGQNdDqL1bKNAFm5BjNuuou5rI3DSeihkB6SzTe666jj5ZxL5Lx
hNBEPwTT5jWETphFGY8GvKUXaqvJpyck1LUwGgjKxH0YJdUDFDmkA7ylvml2Sm99
f+k2T6yTV5/cKYIkLShjhmFlRf8fxONOLKwCtY9WP5oeLxG6RjCLxFBHXCbRUTi1
qIAcakqlkVLq8B8uLU19TriMYBZsBdRnbocIyk6tz3nsA/6QNt2rlXSRLjaplaSK
/y/v9aiobnjIV6QLiP/mFbmOMTd36WH44TFbnR8OzkxACRwCanJS+IgFWpj3SMj1
5AkAPCv4vaLP20Z71ND9mVXCUpYXPqXdoJcTRuy9gn87JnD3/aNS32Gc1aoJgtjO
oAMwhfqP/0+HmJUShaIZNFv8lQLllGvEwSuGhtl/MrYieCtVlHR5acdF9UcFPrpd
HPJOISUrR5xWeNLGwUJVgEbUQ/wfuQexwWhSqIw8Y000BoRXtu3yFpNMBYXsyZsf
VifSW8QS9xkI4MKdcgAL
=eH30
-----END PGP SIGNATURE-----

--Apple-Mail=_FA81BDE8-0B0F-45E1-9630-C6BEEE3A84BB--


From manish631@rediffmail.com Sat Aug 30 10:12:52 2014
Received: from int-mx10.intmail.prod.int.phx2.redhat.com
	(int-mx10.intmail.prod.int.phx2.redhat.com [10.5.11.23])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7UECqZB002468 for <linux-cluster@listman.util.phx.redhat.com>;
	Sat, 30 Aug 2014 10:12:52 -0400
Received: from mx1.redhat.com (ext-mx12.extmail.prod.ext.phx2.redhat.com
	[10.5.110.17])
	by int-mx10.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7UECqBG002701
	for <linux-cluster@redhat.com>; Sat, 30 Aug 2014 10:12:52 -0400
Received: from rediffmail.com (f5mail-224-126.rediffmail.com [114.31.224.126])
	by mx1.redhat.com (8.14.4/8.14.4) with SMTP id s7UECnSM031071
	for <linux-cluster@redhat.com>; Sat, 30 Aug 2014 10:12:50 -0400
Received: (qmail 4357 invoked by uid 510); 30 Aug 2014 14:12:42 -0000
Comment: DomainKeys? See http://antispam.yahoo.com/domainkeys
DomainKey-Signature: a=rsa-sha1; q=dns; c=nofws; s=redf; d=rediffmail.com;
	b=TRMRSvrvfbGzwzIbSWahvnvpyeXsT/PC9+h8IDBsThXuZ8C51wcuukWAltEo4rqHvZo5EVjMIVkYs96nfoi0ssTG13Kh+GC7Q/uR4Kq4tNfNEb9j3xB7/0qWIUx2adq2SBBA30Sh6mmz2fPsoqY6sbGw6lHYfCIEUStRvm13j38=
	; 
x-m-msg: asd54ad564ad7aa6sd5as6d5; a6da7d6asas6dasd77; 5dad65ad5sd;
X-CTCH-Spam: Unknown
X-CTCH-VOD: Unknown
X-CTCH-Flags: : 0
X-CTCH-RefID: str=0001.0A160208.5401DBDC.00AB, ss=1, re=0.000, recu=0.000,
	reip=0.000, cl=1, cld=1, fgs=0
X-REDF-OSEN: manish631@rediffmail.com
Date: 30 Aug 2014 14:12:42 -0000
Message-ID: <20140830141242.4308.qmail@f5mail-224-126.rediffmail.com>
MIME-Version: 1.0
To: <linux-cluster@redhat.com>
Received: from unknown 49.248.116.50 by rediffmail.com via HTTP;
	30 Aug 2014 14:12:41 -0000
Sender: manish631@rediffmail.com
From: "manish vaidya" <manish631@rediffmail.com>
Content-Type: multipart/alternative;
	boundary="=_5ec549d7dafe8e83843cb95689c45dbb"
X-RedHat-Spam-Score: 0.954  (BAYES_50, DKIM_SIGNED, DKIM_VALID, DKIM_VALID_AU,
	FREEMAIL_ENVFROM_END_DIGIT, FREEMAIL_FROM, HTML_IMAGE_ONLY_32,
	HTML_MESSAGE, MSGID_FROM_MTA_HEADER, RCVD_IN_DNSWL_NONE,
	RP_MATCHES_RCVD, SPF_HELO_PASS, SPF_PASS, T_REMOTE_IMAGE,
	UNPARSEABLE_RELAY)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.23
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.17
X-loop: linux-cluster@redhat.com
Subject: [Linux-cluster] =?utf-8?q?Please_help_me_on_cluster_error?=
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Sat, 30 Aug 2014 14:12:52 -0000

--=_5ec549d7dafe8e83843cb95689c45dbb
Content-Transfer-Encoding: 7bit
Content-Type: text/plain; charset="UTF-8"

i created four node cluster in kvm enviorment But i faced error when create new pv such as pvcreate /dev/sdb1
got error , lock from node 2 & lock from node3

also strange cluster logs

Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5e

    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5e
    5f
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5f
    60
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 61
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 63
    64
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 69
    6a
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 78
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 84
    85
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 9a
    9b


Please help me on this issue
--=_5ec549d7dafe8e83843cb95689c45dbb
Content-Transfer-Encoding: quoted-printable
Content-Type: text/html; charset="UTF-8"

i created four node cluster in kvm enviorment But i faced error when create=
 new pv such as pvcreate /dev/sdb1<br />
got error , lock from node 2 & lock from node3<br />
<br />
also strange cluster logs<br />
<br />
Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5e<br />
<br />
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5e<br /=
>
    5f<br />
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5f<br /=
>
    60<br />
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 61<br /=
>
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 63<br /=
>
    64<br />
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 69<br /=
>
    6a<br />
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 78<br /=
>
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 84<br /=
>
    85<br />
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 9a<br /=
>
    9b<br />
<br />
<br />
Please help me on this issue<br><Table border=3D0 Width=3D100% Height=3D57 =
cellspacing=3D0 cellpadding=3D0 style=3D"font-family:Verdana;font-size:11px=
;line-height:15px;"><TR><td><A HREF=3D"http://sigads.rediff.com/RealMedia/a=
ds/click_nx.ads/www.rediffmail.com/signatureline.htm@Middle?" target=3D"_bl=
ank"><IMG SRC=3D"http://sigads.rediff.com/RealMedia/ads/adstream_nx.ads/www=
.rediffmail.com/signatureline.htm@Middle"></A></td></TR></Table><table cell=
padding=3D"0" cellspacing=3D"0"><tbody><tr><td><div style=3D"font-family: A=
rial, Helvetica, sans-serif; font-size:14px">Get your own <span style=3D"pa=
dding-bottom: 0px; background-color: #cc0000; padding-left: 3px; padding-RI=
GHT: 3px; font-family: Arial, Helvetica, sans-serif; color: #ffffff; font-s=
ize: 12px; padding-top: 0px"><b>FREE</b></span> website,  <span style=3D"pa=
dding-bottom: 0px; background-color: #c00; padding-left: 3px; padding-RIGHT=
: 3px; font-family: Arial, Helvetica, sans-serif; color: #ffffff; font-size=
: 12px; padding-top: 0px"><b>FREE</b></span> domain &amp; <span style=3D"pa=
dding-bottom: 0px; background-color: #c00; padding-left: 3px; padding-RIGHT=
: 3px; font-family: Arial, Helvetica, sans-serif; color: #ffffff; font-size=
: 12px; padding-top: 0px"><b>FREE</b></span> mobile app with Company email.=
 &nbsp;</div></td><td><a href=3D"http://track.rediff.com/click?url=3D___htt=
p://businessemail.rediff.com/email-ids-for-companies-with-less-than-50-empl=
oyees?sc_cid=3Dsign-1-10-13___&cmp=3Dhost&lnk=3Dsign-1-10-13&nsrv1=3Dhost" =
style=3D"font-family: Arial, Helvetica, sans-serif; color: #fff; font-size:=
 14px; color:#0000cc" target=3D"_blank"><b>Know More ></b></a><!-- <in-put =
type=3D"button" cl-ass=3D"button" on-click=3D"parent.location=3D&#39;http:/=
/track.rediff.com/click?url=3D___http://businessemail.rediff.com/company-em=
ail-hosting-services?sc_cid=3Dsignature-23-9-13___&amp;cmp=3Dsignature-23-9=
-13&amp;lnk=3Dmypagelogout&amp;nsrv1=3Dhost&#39;" value=3D"Know more &gt;">=
 </input> --></td></tr></tbody></table>
--=_5ec549d7dafe8e83843cb95689c45dbb--


From emi2fast@gmail.com Sat Aug 30 10:53:10 2014
Received: from int-mx11.intmail.prod.int.phx2.redhat.com
	(int-mx11.intmail.prod.int.phx2.redhat.com [10.5.11.24])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7UErAmR002829 for <linux-cluster@listman.util.phx.redhat.com>;
	Sat, 30 Aug 2014 10:53:10 -0400
Received: from mx1.redhat.com (ext-mx15.extmail.prod.ext.phx2.redhat.com
	[10.5.110.20])
	by int-mx11.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7UErAJ2024934
	for <linux-cluster@redhat.com>; Sat, 30 Aug 2014 10:53:10 -0400
Received: from mail-oi0-f54.google.com (mail-oi0-f54.google.com
	[209.85.218.54])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7UEr87I015893
	(version=TLSv1/SSLv3 cipher=RC4-SHA bits=128 verify=FAIL)
	for <linux-cluster@redhat.com>; Sat, 30 Aug 2014 10:53:09 -0400
Received: by mail-oi0-f54.google.com with SMTP id a3so2365842oib.41
	for <linux-cluster@redhat.com>; Sat, 30 Aug 2014 07:53:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=gmail.com; s=20120113;
	h=mime-version:in-reply-to:references:date:message-id:subject:from:to
	:content-type; bh=Hbn9Gj25ryGkB5cvaUGKfJLEZPRRcTCx62wrwUuPXa8=;
	b=dVF7uKz14Ce3x/7Z93L52s7a9dVvISHXVBUDOqGSwhg8P/Ifxtl1nRNxpl1VwiUn02
	cd1txPkNMvVXYjPyWKzaNQVquwKxTgnUmiYUGfTUy7QK0mxtUtqx5m3Zq4BWgqXnOgt+
	EuQcXSpn84aun4w/GQ8m9pVC+R4ctIB+U4sZ9mZ6bBuT1eIXBa2MiqTq32t5SWQOzXqL
	bmlMBquxg6V0LLGlEts0HIjrzfX8Xt158bpk3veNo+46XYuBF6K1Jyn10QkaIHKVJhbs
	/8LLrCp8VOdIrYZAYO8/+qag9zaL/6inKtwihWGjRjJg4F9sx3QKV6TE/tjY+EcfLAMe
	bcZA==
MIME-Version: 1.0
X-Received: by 10.60.37.165 with SMTP id z5mr16829466oej.16.1409410388407;
	Sat, 30 Aug 2014 07:53:08 -0700 (PDT)
Received: by 10.76.113.211 with HTTP; Sat, 30 Aug 2014 07:53:08 -0700 (PDT)
In-Reply-To: <20140830141242.4308.qmail@f5mail-224-126.rediffmail.com>
References: <20140830141242.4308.qmail@f5mail-224-126.rediffmail.com>
Date: Sat, 30 Aug 2014 16:53:08 +0200
Message-ID: <CAE7pJ3AKUvW1ia3MAPWjE1Mrwfu10F=dCt+E48nicSJDjbJhrA@mail.gmail.com>
From: emmanuel segura <emi2fast@gmail.com>
To: linux clustering <linux-cluster@redhat.com>
Content-Type: multipart/alternative; boundary=089e013c65768b3ab50501d9ed2b
X-RedHat-Spam-Score: -1.689  (BAYES_05, DCC_REPUT_00_12, DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU, FREEMAIL_FROM, HTML_MESSAGE,
	RCVD_IN_DNSWL_LOW, SPF_PASS, T_REMOTE_IMAGE)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.24
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.20
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] Please help me on cluster error
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Sat, 30 Aug 2014 14:53:10 -0000

--089e013c65768b3ab50501d9ed2b
Content-Type: text/plain; charset=UTF-8

are you using clvmd? if your answer is = yes, you need to be sure, you pv
is visibile to your cluster nodes


2014-08-30 16:12 GMT+02:00 manish vaidya <manish631@rediffmail.com>:

> i created four node cluster in kvm enviorment But i faced error when
> create new pv such as pvcreate /dev/sdb1
> got error , lock from node 2 & lock from node3
>
> also strange cluster logs
>
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5e
>
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5e
> 5f
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5f
> 60
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 61
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 63
> 64
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 69
> 6a
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 78
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 84
> 85
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 9a
> 9b
>
>
> Please help me on this issue
>
> <http://sigads.rediff.com/RealMedia/ads/click_nx.ads/www.rediffmail.com/signatureline.htm@Middle?>
> Get your own *FREE* website, *FREE* domain & *FREE* mobile app with
> Company email.
> *Know More >*
> <http://track.rediff.com/click?url=___http://businessemail.rediff.com/email-ids-for-companies-with-less-than-50-employees?sc_cid=sign-1-10-13___&cmp=host&lnk=sign-1-10-13&nsrv1=host>
> --
> Linux-cluster mailing list
> Linux-cluster@redhat.com
> https://www.redhat.com/mailman/listinfo/linux-cluster
>



-- 
esta es mi vida e me la vivo hasta que dios quiera

--089e013c65768b3ab50501d9ed2b
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">are you using clvmd? if your answer is =3D yes, you need t=
o be sure, you pv is visibile to your cluster nodes<br></div><div class=3D"=
gmail_extra"><br><br><div class=3D"gmail_quote">2014-08-30 16:12 GMT+02:00 =
manish vaidya <span dir=3D"ltr">&lt;<a href=3D"mailto:manish631@rediffmail.=
com" target=3D"_blank">manish631@rediffmail.com</a>&gt;</span>:<br>
<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex">i created four node cluster in kvm enviormen=
t But i faced error when create new pv such as pvcreate /dev/sdb1<br>
got error , lock from node 2 &amp; lock from node3<br>
<br>
also strange cluster logs<br>
<br>
Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5e<br>
<br>
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5e<br>
    5f<br>
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5f<br>
    60<br>
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 61<br>
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 63<br>
    64<br>
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 69<br>
    6a<br>
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 78<br>
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 84<br>
    85<br>
    Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 9a<br>
    9b<br>
<br>
<br>
Please help me on this issue<br><table style=3D"font-family:Verdana;font-si=
ze:11px;line-height:15px" border=3D"0" cellpadding=3D"0" cellspacing=3D"0" =
height=3D"57" width=3D"100%"><tbody><tr><td><a href=3D"http://sigads.rediff=
.com/RealMedia/ads/click_nx.ads/www.rediffmail.com/signatureline.htm@Middle=
?" target=3D"_blank"><img src=3D"http://sigads.rediff.com/RealMedia/ads/ads=
tream_nx.ads/www.rediffmail.com/signatureline.htm@Middle"></a></td>
</tr></tbody></table><table cellpadding=3D"0" cellspacing=3D"0"><tbody><tr>=
<td><div style=3D"font-family:Arial,Helvetica,sans-serif;font-size:14px">Ge=
t your own <span style=3D"padding-bottom:0px;background-color:#cc0000;paddi=
ng-left:3px;padding-RIGHT:3px;font-family:Arial,Helvetica,sans-serif;color:=
#ffffff;font-size:12px;padding-top:0px"><b>FREE</b></span> website,  <span =
style=3D"padding-bottom:0px;background-color:#c00;padding-left:3px;padding-=
RIGHT:3px;font-family:Arial,Helvetica,sans-serif;color:#ffffff;font-size:12=
px;padding-top:0px"><b>FREE</b></span> domain &amp; <span style=3D"padding-=
bottom:0px;background-color:#c00;padding-left:3px;padding-RIGHT:3px;font-fa=
mily:Arial,Helvetica,sans-serif;color:#ffffff;font-size:12px;padding-top:0p=
x"><b>FREE</b></span> mobile app with Company email. =C2=A0</div>
</td><td><a href=3D"http://track.rediff.com/click?url=3D___http://businesse=
mail.rediff.com/email-ids-for-companies-with-less-than-50-employees?sc_cid=
=3Dsign-1-10-13___&amp;cmp=3Dhost&amp;lnk=3Dsign-1-10-13&amp;nsrv1=3Dhost" =
style=3D"font-family:Arial,Helvetica,sans-serif;color:#fff;font-size:14px;c=
olor:#0000cc" target=3D"_blank"><b>Know More &gt;</b></a></td>
</tr></tbody></table><br>--<br>
Linux-cluster mailing list<br>
<a href=3D"mailto:Linux-cluster@redhat.com">Linux-cluster@redhat.com</a><br=
>
<a href=3D"https://www.redhat.com/mailman/listinfo/linux-cluster" target=3D=
"_blank">https://www.redhat.com/mailman/listinfo/linux-cluster</a><br></blo=
ckquote></div><br><br clear=3D"all"><br>-- <br>esta es mi vida e me la vivo=
 hasta que dios quiera
</div>

--089e013c65768b3ab50501d9ed2b--


From lists@alteeve.ca Sat Aug 30 12:35:56 2014
Received: from int-mx13.intmail.prod.int.phx2.redhat.com
	(int-mx13.intmail.prod.int.phx2.redhat.com [10.5.11.26])
	by lists01.pubmisc.prod.ext.phx2.redhat.com (8.13.8/8.13.8) with ESMTP
	id s7UGZup3031201 for <linux-cluster@listman.util.phx.redhat.com>;
	Sat, 30 Aug 2014 12:35:56 -0400
Received: from mx1.redhat.com (ext-mx14.extmail.prod.ext.phx2.redhat.com
	[10.5.110.19])
	by int-mx13.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP
	id s7UGZuk3004206
	for <linux-cluster@redhat.com>; Sat, 30 Aug 2014 12:35:56 -0400
Received: from vm08-mail01.alteeve.ca (mail.alteeve.ca [65.39.153.71])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s7UGZsBj012676
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256
	verify=NO)
	for <linux-cluster@redhat.com>; Sat, 30 Aug 2014 12:35:54 -0400
Received: from lepuny.alteeve.ca (dhcp-108-168-20-201.cable.user.start.ca
	[108.168.20.201])
	by vm08-mail01.alteeve.ca (Postfix) with ESMTPSA id 3BDC7200DF
	for <linux-cluster@redhat.com>; Sat, 30 Aug 2014 12:35:50 -0400 (EDT)
Message-ID: <5401FD68.8000407@alteeve.ca>
Date: Sat, 30 Aug 2014 12:35:52 -0400
From: Digimer <lists@alteeve.ca>
User-Agent: Mozilla/5.0 (X11; Linux x86_64;
	rv:24.0) Gecko/20100101 Thunderbird/24.7.0
MIME-Version: 1.0
To: linux clustering <linux-cluster@redhat.com>
References: <20140830141242.4308.qmail@f5mail-224-126.rediffmail.com>
In-Reply-To: <20140830141242.4308.qmail@f5mail-224-126.rediffmail.com>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed
Content-Transfer-Encoding: 7bit
X-RedHat-Spam-Score: -1.899  (BAYES_00,RP_MATCHES_RCVD)
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.26
X-Scanned-By: MIMEDefang 2.68 on 10.5.110.19
X-loop: linux-cluster@redhat.com
Subject: Re: [Linux-cluster] Please help me on cluster error
X-BeenThere: linux-cluster@redhat.com
X-Mailman-Version: 2.1.12
Precedence: junk
Reply-To: linux clustering <linux-cluster@redhat.com>
List-Id: linux clustering <linux-cluster.redhat.com>
List-Unsubscribe: <https://www.redhat.com/mailman/options/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=unsubscribe>
List-Archive: <https://www.redhat.com/archives/linux-cluster>
List-Post: <mailto:linux-cluster@redhat.com>
List-Help: <mailto:linux-cluster-request@redhat.com?subject=help>
List-Subscribe: <https://www.redhat.com/mailman/listinfo/linux-cluster>,
	<mailto:linux-cluster-request@redhat.com?subject=subscribe>
X-List-Received-Date: Sat, 30 Aug 2014 16:35:56 -0000

Can you share your cluster information please?

This could be a network problem, as the messages below happen when the 
network between the nodes isn't fast enough or has too long latency and 
cluster traffic is considered lost and re-requested.

If you don't have fencing working properly, and if a network issue 
caused a node to be declared lost, clustered LVM (and anything else 
using cluster locking) will fail (by design).

If you share your configuration and more of your logs, it will help us 
understand what is happening. Please also tell us what version of the 
cluster software you're using.

digimer

On 30/08/14 10:12 AM, manish vaidya wrote:
> i created four node cluster in kvm enviorment But i faced error when
> create new pv such as pvcreate /dev/sdb1
> got error , lock from node 2 & lock from node3
>
> also strange cluster logs
>
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5e
>
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5e
> 5f
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 5f
> 60
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 61
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 63
> 64
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 69
> 6a
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 78
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 84
> 85
> Jun 10 14:46:24 node1 corosync[3266]: [TOTEM ] Retransmit List: 9a
> 9b
>
>
> Please help me on this issue
> <http://sigads.rediff.com/RealMedia/ads/click_nx.ads/www.rediffmail.com/signatureline.htm@Middle?>
>
> Get your own *FREE* website, *FREE* domain & *FREE* mobile app with
> Company email.
> 	*Know More >*
> <http://track.rediff.com/click?url=___http://businessemail.rediff.com/email-ids-for-companies-with-less-than-50-employees?sc_cid=sign-1-10-13___&cmp=host&lnk=sign-1-10-13&nsrv1=host>
>
>
>


-- 
Digimer
Papers and Projects: https://alteeve.ca/w/
What if the cure for cancer is trapped in the mind of a person without 
access to education?